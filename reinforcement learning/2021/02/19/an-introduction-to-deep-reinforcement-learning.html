<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Practicing: Deep Reinforcement Learning Course by Thomas Simonini | Guillaume’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Practicing: Deep Reinforcement Learning Course by Thomas Simonini" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Free course in Deep Reinforcement Learning from beginner to expert. My notes" />
<meta property="og:description" content="A Free course in Deep Reinforcement Learning from beginner to expert. My notes" />
<link rel="canonical" href="https://castorfou.github.io/guillaume_blog/reinforcement%20learning/2021/02/19/an-introduction-to-deep-reinforcement-learning.html" />
<meta property="og:url" content="https://castorfou.github.io/guillaume_blog/reinforcement%20learning/2021/02/19/an-introduction-to-deep-reinforcement-learning.html" />
<meta property="og:site_name" content="Guillaume’s blog" />
<meta property="og:image" content="https://castorfou.github.io/guillaume_blog/images/fastpages_posts/actions/actions_logo.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-19T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://castorfou.github.io/guillaume_blog/reinforcement%20learning/2021/02/19/an-introduction-to-deep-reinforcement-learning.html","@type":"BlogPosting","headline":"Practicing: Deep Reinforcement Learning Course by Thomas Simonini","dateModified":"2021-02-19T00:00:00-06:00","datePublished":"2021-02-19T00:00:00-06:00","image":"https://castorfou.github.io/guillaume_blog/images/fastpages_posts/actions/actions_logo.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://castorfou.github.io/guillaume_blog/reinforcement%20learning/2021/02/19/an-introduction-to-deep-reinforcement-learning.html"},"description":"A Free course in Deep Reinforcement Learning from beginner to expert. My notes","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/guillaume_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://castorfou.github.io/guillaume_blog/feed.xml" title="Guillaume's blog" /><link rel="shortcut icon" type="image/x-icon" href="/guillaume_blog/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/guillaume_blog/">Guillaume&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/guillaume_blog/about/">About Me</a><a class="page-link" href="/guillaume_blog/search/">Search</a><a class="page-link" href="/guillaume_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Practicing: Deep Reinforcement Learning Course by Thomas Simonini</h1><p class="page-description">A Free course in Deep Reinforcement Learning from beginner to expert. My notes</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-19T00:00:00-06:00" itemprop="datePublished">
        Feb 19, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#reinforcement learning">reinforcement learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#21921---step-1---an-introduction-to-deep-reinforcement-learning">(2/19/21) - Step 1 - An Introduction to Deep Reinforcement Learning?</a>
<ul>
<li class="toc-entry toc-h3"><a href="#three-approaches-to-reinforcement-learning">Three approaches to Reinforcement Learning</a>
<ul>
<li class="toc-entry toc-h4"><a href="#value-based">Value Based</a></li>
<li class="toc-entry toc-h4"><a href="#policy-based">Policy Based</a></li>
<li class="toc-entry toc-h4"><a href="#model-based">Model Based</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#deep-reinforcement-learning">Deep Reinforcement Learning</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#21921---step-2---part-1---q-learning-lets-create-an-autonomous-taxi">(2/19/21) - Step 2 - part 1 - Q-Learning, let’s create an autonomous Taxi</a>
<ul>
<li class="toc-entry toc-h4"><a href="#value-based-method">Value-based method</a></li>
<li class="toc-entry toc-h4"><a href="#bellman-equation">Bellman equation</a></li>
<li class="toc-entry toc-h4"><a href="#monte-carlo-vs-temporal-difference">Monte Carlo vs Temporal Difference</a></li>
</ul>
</li>
</ul><p>A course by <a href="https://www.simoninithomas.com/">Thomas Simonini</a></p>

<p><a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/">Syllabus (from 2018)</a></p>

<p><a href="https://medium.com/deep-reinforcement-learning-course/launching-deep-reinforcement-learning-course-v2-0-38fa3c24bcbc">kind of syllabus (from 2020)</a></p>

<p>Everything available in <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course">github</a></p>

<p>I appreciate the effort to update examples, and some 2018 implementations became obsolete. Historical Atari VC2600 games are now Starcraft 2 or minecraft, and news series on building AI for video games in Unity and Unreal Engine..</p>

<h2 id="21921---step-1---an-introduction-to-deep-reinforcement-learning">
<a class="anchor" href="#21921---step-1---an-introduction-to-deep-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>(2/19/21) - <a href="https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c">Step 1</a> - An Introduction to Deep Reinforcement Learning?</h2>

<p>Previous version from <a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419">2018: What is Deep Reinforcement Learning?</a> is quite interesting. With 3 parts:</p>

<ul>
  <li>What Reinforcement Learning is, and how rewards are the central idea</li>
  <li>The three approaches of Reinforcement Learning</li>
  <li>What the “Deep” in Deep Reinforcement Learning means</li>
</ul>

<p><img src="https://cdn-media-1.freecodecamp.org/images/1*aKYFRoEmmKkybqJOvLt2JQ.png" alt=""></p>

<p>Rewards, long-term future reward, discount rate.</p>

<p><img src="https://cdn-media-1.freecodecamp.org/images/1*zrzRTXt8rtWF5fX__kZ-yQ.png" alt=""></p>

<p>Episodic (starting and ending point) vs Continuous (e.g. stock trading) tasks.</p>

<p>Way of learning: Monte Carlo (MC: rewards collected at the end of an episode) vs Temporal Difference (TD: estimate rewards at each step)</p>

<p><img src="https://cdn-media-1.freecodecamp.org/images/1*LLfj11fivpkKZkwQ8uPi3A.png" alt=""></p>

<p>Exploration/Exploitation trade off. Will see later different ways to handle that trade-off.</p>

<p><img src="https://cdn-media-1.freecodecamp.org/images/1*APLmZ8CVgu0oY3sQBVYIuw.png" alt=""></p>

<h3 id="three-approaches-to-reinforcement-learning">
<a class="anchor" href="#three-approaches-to-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Three approaches to Reinforcement Learning</h3>

<p>These are value-based, policy-based, and model-based.</p>

<h4 id="value-based">
<a class="anchor" href="#value-based" aria-hidden="true"><span class="octicon octicon-link"></span></a>Value Based</h4>

<p>In value-based RL, the goal is to optimize the value function <em>V(s)</em>.</p>

<p>The value function is a function that tells us the maximum expected future reward the agent will get at each state.</p>

<p><img src="https://cdn-media-1.freecodecamp.org/images/1*2_JRk-4O523bcOcSy1u31g.png" alt=""></p>

<h4 id="policy-based">
<a class="anchor" href="#policy-based" aria-hidden="true"><span class="octicon octicon-link"></span></a>Policy Based</h4>

<p>In policy-based RL, we want to directly optimize the policy function <em>π(s)</em> without using a value function.</p>

<p>The policy is what defines the agent behavior at a given time.</p>

<p>We have two types of policy:</p>

<ul>
  <li>Deterministic: a policy at a given state will always return the same action.</li>
  <li>Stochastic: output a distribution probability over actions.</li>
</ul>

<p><img src="https://cdn-media-1.freecodecamp.org/images/1*fii7Z01laRGateAJDvloAQ.png" alt=""></p>

<h4 id="model-based">
<a class="anchor" href="#model-based" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Based</h4>

<p>In model-based RL, we model the environment. This means we create a model of the behavior of the environment. Not addressed in this course.</p>

<h3 id="deep-reinforcement-learning">
<a class="anchor" href="#deep-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Reinforcement Learning</h3>

<p>In Q-learning, we keep a table of actions to take for each state (based on reward). This can be huge.</p>

<p>Deep Learning allows to approximate this Q function.</p>

<p><img src="https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png" alt=""></p>

<p><a href="https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c">Updated version</a> from 2020 (and <a href="https://www.youtube.com/watch?v=q0BiUn5LiBc">video</a> version)</p>

<p>This is a good starting point, well explained.</p>

<p>Reinforcement Learning is just a <strong>computational approach of learning from action.</strong></p>

<p><strong>A formal definition</strong></p>

<blockquote>
  <p>Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that <strong>learn from the environment</strong> by <strong>interacting with it</strong> through trial and error and <strong>receiving rewards</strong> (positive or negative) <strong>as unique feedback.</strong></p>
</blockquote>

<p>Some explanations about <strong>observations</strong> (partial description) vs <strong>states</strong> (fully observed envt). Only differs in implementation, all theoretical background stays the same.</p>

<p>Action space where we can distinguish <strong>discrete</strong> (e.g. fire, up) actions from <strong>continuous</strong> (e.g. turn 23deg) ones.</p>

<p><strong>Reward</strong> part is the same as the one from 2018. With cheese, mouse, maze example.</p>

<p><strong>Episodic</strong> and <strong>continuous</strong> tasks part is the same as the one from 2018.</p>

<p><strong>Exploration/Exploitation trade-off</strong> is explained the same way + an additional example taken from <a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa18/">berkley - CS 294-112</a> - Deep Reinforcement Learning course. I want to learn more about this course!</p>

<p>About <strong>solving RL problems</strong>, it is now presented as 2 main approaches:</p>

<ul>
  <li>
<strong>policy-based</strong> methods</li>
  <li>
<strong>value-based</strong> methods</li>
</ul>

<p><img src="https://miro.medium.com/max/700/1*Vujmmyswrg2wIjmpvSUBfg.png" alt=""></p>

<p>And bedore to explain that, nice presentation of what is a <strong>policy $\pi$</strong>. Solving RL problem is to find that optimal policy: directly with policy-based method, indirectly (through value function) with value-based method.</p>

<p>There is an explanation about different types of policy: <strong>deterministic</strong> and <strong>stochastic</strong>.</p>

<p>And that we use deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based). Thomas suggests to go further with deep learning with MIT 6.S191, which is the <a href="https://castorfou.github.io/guillaume_blog/deep%20learning/mit/tensorflow/2021/02/05/learning-MIT-6.S191-2021.html">one</a> (version 2021) I follow these days.</p>

<h2 id="21921---step-2---part-1---q-learning-lets-create-an-autonomous-taxi">
<a class="anchor" href="#21921---step-2---part-1---q-learning-lets-create-an-autonomous-taxi" aria-hidden="true"><span class="octicon octicon-link"></span></a>(2/19/21) - <a href="https://thomassimonini.medium.com/q-learning-lets-create-an-autonomous-taxi-part-1-2-3e8f5e764358">Step 2 - part 1</a> - Q-Learning, let’s create an autonomous Taxi</h2>

<p>And in <a href="https://www.youtube.com/watch?v=230bR2DrbdE&amp;feature=emb_logo">video</a> (I like to read + watch the video at the same time)</p>

<p>Here in Step 2 we focus on a value-based method: Q-learning. And what is seen in part 1 and 2:</p>

<p><img src="https://miro.medium.com/max/700/1*2yYWVAXJh4FI2lpsL0ajwQ.png" alt=""></p>

<h4 id="value-based-method">
<a class="anchor" href="#value-based-method" aria-hidden="true"><span class="octicon octicon-link"></span></a>Value-based method</h4>

<p>Remember what we mean in value-based method</p>

<p><img src="https://miro.medium.com/max/700/1*jfUUaZuHUa1h61oD6O18KA.png" alt=""></p>

<p>you don’t train your policy, you define a simple function such as greedy function to select the best association State-Action, so the best action.</p>

<h4 id="bellman-equation">
<a class="anchor" href="#bellman-equation" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Bellman equation</strong>
</h4>

<p>each value as the sum of the expected return, <strong>which is a long process.</strong> This is equivalent <strong>to the sum of immediate reward + the discounted value of the state that follows.</strong></p>

<p><img src="https://miro.medium.com/max/700/1*FMjoVEELvz0oKcIfmcvGPQ.png" alt=""></p>

<h4 id="monte-carlo-vs-temporal-difference">
<a class="anchor" href="#monte-carlo-vs-temporal-difference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Monte Carlo vs Temporal Difference</h4>

<p>And then an explanation about 2 types of method to learn a policy or a value-function:</p>

<ul>
  <li>
<em>Monte Carlo</em>: learning at the end of the episode. With <em>Monte Carlo</em>, we update the value function from a complete episode and so we <strong>use the actual accurate discounted return of this episode.</strong>
</li>
  <li>
<em>TD learning</em>: learning at each step. With <em>TD learning</em>, we update the value function from a step, so we replace Gt that we don’t have with <strong>an estimated return called TD target.</strong> (chich is the immediate reward + the discounted value of the next state)</li>
</ul>

<p><img src="https://miro.medium.com/max/700/1*c8nfnXRu8n1h78bWPEK5vg.png" alt=""></p>

<p>It was not clear to me that these methods could be used for policy-based approach. It is now!</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="castorfou/guillaume_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/guillaume_blog/reinforcement%20learning/2021/02/19/an-introduction-to-deep-reinforcement-learning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/guillaume_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/guillaume_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Journey for a datascientist</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/castorfou" title="castorfou"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/GuillaumeRamel1" title="GuillaumeRamel1"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
