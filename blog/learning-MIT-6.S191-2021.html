<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Learning: MIT 6.S191 Introduction to Deep Learning - 2021 | Guillaume’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Learning: MIT 6.S191 Introduction to Deep Learning - 2021" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My notes/thoughts about the lecture" />
<meta property="og:description" content="My notes/thoughts about the lecture" />
<link rel="canonical" href="https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021.html" />
<meta property="og:url" content="https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021.html" />
<meta property="og:site_name" content="Guillaume’s blog" />
<meta property="og:image" content="https://castorfou.github.io/guillaume_blog/images/DL.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-05T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021.html","@type":"BlogPosting","headline":"Learning: MIT 6.S191 Introduction to Deep Learning - 2021","dateModified":"2021-02-05T00:00:00-06:00","datePublished":"2021-02-05T00:00:00-06:00","image":"https://castorfou.github.io/guillaume_blog/images/DL.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021.html"},"description":"My notes/thoughts about the lecture","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/guillaume_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://castorfou.github.io/guillaume_blog/feed.xml" title="Guillaume's blog" /><link rel="shortcut icon" type="image/x-icon" href="/guillaume_blog/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/guillaume_blog/">Guillaume&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/guillaume_blog/about/">About Me</a><a class="page-link" href="/guillaume_blog/search/">Search</a><a class="page-link" href="/guillaume_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Learning: MIT 6.S191 Introduction to Deep Learning - 2021</h1><p class="page-description">My notes/thoughts about the lecture</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-05T00:00:00-06:00" itemprop="datePublished">
        Feb 5, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#deep learning">deep learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#MIT">MIT</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#tensorflow">tensorflow</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#2521---intro-to-deep-learning---lecture-1">2/5/21 - Intro to Deep Learning - lecture 1</a></li>
<li class="toc-entry toc-h2"><a href="#21521---deep-sequence-modeling---lecture-2">2/15/21 - Deep Sequence Modeling - lecture 2</a></li>
<li class="toc-entry toc-h2"><a href="#21621---intro-to-tensorflow--music-generation---software-lab-1">2/16/21 - Intro to TensorFlow;  Music Generation - software lab 1</a></li>
<li class="toc-entry toc-h2"><a href="#22221---deep-computer-vision---lecture-3">2/22/21 - Deep Computer Vision - lecture 3</a></li>
<li class="toc-entry toc-h2"><a href="#3121---deep-generative-modeling---lecture-4">3/1/21 - Deep Generative Modeling - lecture 4</a></li>
<li class="toc-entry toc-h2"><a href="#1321---de-biasing-facial-recognition-systems---software-lab-2">1/3/21 - De-biasing Facial Recognition Systems - Software Lab 2</a></li>
</ul><p>From <a href="http://introtodeeplearning.com/">http://introtodeeplearning.com/</a></p>

<p>I keep all content (lectures, notebooks) in <a href="https://github.com/castorfou/mit_6s191">github</a></p>

<p>This is done with google contribution, and therefore all examples are in tensorflow. I will try to adapt notebooks in PyTorch.</p>

<h2 id="2521---intro-to-deep-learning---lecture-1">
<a class="anchor" href="#2521---intro-to-deep-learning---lecture-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>2/5/21 - Intro to Deep Learning - lecture 1</h2>

<p>Lecturer: Alexander Amini</p>

<p>Intro is just jaw-dropping!</p>

<p><a href="https://youtu.be/5tvmMX8r_OM?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;t=40">2020 intro</a> was top.</p>

<p><a href="https://youtu.be/5tvmMX8r_OM?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;t=149">2021 intro</a> is just awesome.</p>

<p>It is a standard overview of simple deep learning concepts: Perceptron, multi-perceptron, dense layers, loss, gradient-descent, backprop, SGD, regularization, dropout, early stoppping</p>

<h2 id="21521---deep-sequence-modeling---lecture-2">
<a class="anchor" href="#21521---deep-sequence-modeling---lecture-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>2/15/21 - Deep Sequence Modeling - lecture 2</h2>

<p>New lecturer: Ava Soleimany</p>

<p>Nice introduction to sequence modeling with Many-to-One, One-to-Many, Many-to-Many.</p>

<p>RNN and implementation in TensorFlow. And NLP examples: next word problem. (and NLP concepts such as Vocabulary, Indexing, Embedding)</p>

<p>And what we need for sequence modeling:</p>

<ul>
  <li>handle variable-length sequences</li>
  <li>track long-term dependencies</li>
  <li>maintain information about order</li>
  <li>share parameters across the sequence</li>
</ul>

<p>Backpropagation through time and problem of exploding/vanishing gradients.</p>

<p>Against exploding: gradient clipping. Against vanishing: 3 ways explained - activation functions, weight init, network arch.</p>

<p>Gated cell: to control what information is passed through. Ex: LSTM Long Short Term Memory. They support something closed to Forget Store Update Output. Ava explains graphically which part of LSTM cells is providing which function.</p>

<p>And then examples: Music generation (to generate 4th movement of last symphony from Schubert!), sentiment classification, machine translation (with Attention mechanisms which provide learnable memory access to solve Not long memory), trajectory prediction, environmental modeling.</p>

<h2 id="21621---intro-to-tensorflow--music-generation---software-lab-1">
<a class="anchor" href="#21621---intro-to-tensorflow--music-generation---software-lab-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>2/16/21 - Intro to TensorFlow;  Music Generation - software lab 1</h2>

<p>As an exercise I have completed labs in TensorFlow and adapted them in <a href="https://github.com/castorfou/mit_6s191/blob/main/introtodeeplearning/lab1/Part1_TensorFlow_transposed%20to%20PyTorch.ipynb">PyTorch</a>.</p>

<p>With LSTM, I ran into this error: <code class="language-plaintext highlighter-rouge">UnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]</code></p>

<p>Which is solved by calling <a href="https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth"><code class="language-plaintext highlighter-rouge">tf.config.experimental.set_memory_growth</code></a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span> 
<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="c1"># Currently, memory growth needs to be the same across GPUs
</span>    <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
      <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>
    <span class="n">logical_gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="n">list_logical_devices</span><span class="p">(</span><span class="s">'GPU'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">),</span> <span class="s">"Physical GPUs,"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">logical_gpus</span><span class="p">),</span> <span class="s">"Logical GPUs"</span><span class="p">)</span>
  <span class="k">except</span> <span class="nb">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># Memory growth must be set before GPUs have been initialized
</span>    <span class="k">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div>

<p>Music lab is nice to play with. I am not sure I would be able to convert to PyTorch. It would require time!</p>

<h2 id="22221---deep-computer-vision---lecture-3">
<a class="anchor" href="#22221---deep-computer-vision---lecture-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>2/22/21 - Deep Computer Vision - lecture 3</h2>

<p>I have never been a big fan of computer vision.</p>

<p>I like the idea developed by Alexander Amini about <strong>hierarchy of features</strong>. (low level: edges, spots; mid level: eyes, noses)</p>

<p>And how he explains limitation of FC layers for visual detection, and introduction of spatial structure (feature extraction with convolutions)</p>

<p>Some nice examples of hand-engineered convolution filters for different needs: sharpen, edge detect, strong edge detect.</p>

<p>Then classic explanations of CNN with convolution, max pooling.</p>

<p>I like the way classification problems are broken down between feature learning (convolution+relu, pooling, repeated several times) and classification (flatten, FC, softmax) which is a task learning part.</p>

<p>The second part (task learning part) can be anything: classification, object detection, segmentation, probabilistic control, …</p>

<p><img src="../images/mit_6S191_lec3_cnn_architectures.png" alt=""></p>

<p>Nice explanation of R-CNN to learn region proposals.</p>

<p>Introduction to Software lab2: de-biaising facial recognition systems.</p>

<h2 id="3121---deep-generative-modeling---lecture-4">
<a class="anchor" href="#3121---deep-generative-modeling---lecture-4" aria-hidden="true"><span class="octicon octicon-link"></span></a>3/1/21 - Deep Generative Modeling - lecture 4</h2>

<p>From pattern discovered from data (underlying structure of the data), generate examples following these patterns.</p>

<p><strong>Autoencoder</strong>: foundational generative model which builds up latent variable representation by self-encoding the input. To train such network, we create a decoder to go from latent variable to generated output, and then compare input to generated output.</p>

<p><img src="../images/mit_6S191_lec4_autoencoders.png" alt=""></p>

<p><strong>Variational autoencoder (vae)</strong>: with vae we try to encode inputs as distributions defined by mean <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span> and variance <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>. And we want to achieve continuity and completeness:</p>

<ul>
  <li>continuity: points that are close in latent space –&gt; similar content after decoding</li>
  <li>completeness: sampling from latent space –&gt; ‘meaningful’ content after decoding</li>
</ul>

<p>Regularization is pushing to get these properties.</p>

<p><img src="../images/mit_6S191_lec4_vae_regularization.png" alt=""></p>

<p>And the learning process is about minimizing reconstruction loss + a regularization term:</p>

<p><img src="../images/mit_6S191_lec4_vae_loss.png" alt=""></p>

<p>Ava is then explaining the smart trick to allow backpropagation to happen. Indeed by introducing stochastic term in the sampling layer, we are breaking the backpropagation logic.</p>

<p>We are moving z from a normal distribution to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">μ</span></span></span></span>+<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span></span>.<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> follow a normal distribution of mean 0, std 1.</p>

<p>Explanation then of space disentanglement via <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>-VAEs. It allows latent variables to be independent.</p>

<p><img src="../images/mit_6S191_lec4_vae_beta.png" alt=""></p>

<p>And then some introduction about <em>*GANs</em> (Generative Adversarial Network) which are a way to make a generative model by having 2 neural networks (generator and discriminator) compete with each other.</p>

<p>And share some recent advances on GAN such as StyleGAN(2), conditional GAN, CycleGAN. CycleGAN is famous for turning horses in zebras, but it can be used to transform speech as well (used in the synthesis of Obama’s voice)</p>

<p><img src="../images/mit_6S191_lec4_generative_summary.png" alt=""></p>

<h2 id="1321---de-biasing-facial-recognition-systems---software-lab-2">
<a class="anchor" href="#1321---de-biasing-facial-recognition-systems---software-lab-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>1/3/21 - De-biasing Facial Recognition Systems - Software Lab 2</h2>

<p><a href="https://github.com/castorfou/mit_6s191/blob/main/introtodeeplearning/lab2/Part1_MNIST.ipynb">Part 1 MNIST</a></p>

<p>starts with FC layers. With some overfitting but a good accuracy of 96%.</p>

<p>then move to a CNN architecture. I ran into <a href="https://github.com/tensorflow/tensorflow/issues/24828">gpu issues</a>.  Accuracy is now 99%.</p>

<p>I didn’t manage to make the last part working. (using tape.gradient)</p>

<p><a href="https://github.com/castorfou/mit_6s191/blob/main/introtodeeplearning/lab2/Part2_Debiasing.ipynb">Part 2 Debiasing</a></p>

<p>Fit a CNN model to classify faces based on celebA dataset. And see the bias effect by predicting on Fitzpatrick scale skin type classification system.</p>

<p>Use VAE to learn latent structure.</p>

<p><img src="https://i.ibb.co/3s4S6Gc/vae.jpg" alt="The concept of a VAE"></p>

<p>To then debias using DB-VAE model.</p>

<p><img src="https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab2/img/DB-VAE.png" alt="DB-VAE"></p>

<p>There is a lack of progressive unit tests to validate each step. Cannot go to the end.</p>

<p>Would be interested to see how to apply to non computer vision problems.</p>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="castorfou/guillaume_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/guillaume_blog/blog/learning-MIT-6.S191-2021.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/guillaume_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/guillaume_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Journey for a datascientist</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/castorfou" title="castorfou"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/GuillaumeRamel1" title="GuillaumeRamel1"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
