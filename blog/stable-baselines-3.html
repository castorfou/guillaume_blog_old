<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Stable baselines 3 - 1st steps | Guillaume’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Stable baselines 3 - 1st steps" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="installation, 1st experimentations" />
<meta property="og:description" content="installation, 1st experimentations" />
<link rel="canonical" href="https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html" />
<meta property="og:url" content="https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html" />
<meta property="og:site_name" content="Guillaume’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-24T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html","@type":"BlogPosting","headline":"Stable baselines 3 - 1st steps","dateModified":"2021-03-24T00:00:00-05:00","datePublished":"2021-03-24T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html"},"description":"installation, 1st experimentations","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/guillaume_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://castorfou.github.io/guillaume_blog/feed.xml" title="Guillaume's blog" /><link rel="shortcut icon" type="image/x-icon" href="/guillaume_blog/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/guillaume_blog/">Guillaume&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/guillaume_blog/about/">About Me</a><a class="page-link" href="/guillaume_blog/search/">Search</a><a class="page-link" href="/guillaume_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Stable baselines 3 - 1st steps</h1><p class="page-description">installation, 1st experimentations</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-24T00:00:00-05:00" itemprop="datePublished">
        Mar 24, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      25 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#reinforcement learning">reinforcement learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#sb3">sb3</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#What-is-stable-baselines-3-(sb3)">What is stable baselines 3 (sb3) </a></li>
<li class="toc-entry toc-h1"><a href="#My-installation">My installation </a></li>
<li class="toc-entry toc-h1"><a href="#SB3-tutorials">SB3 tutorials </a></li>
<li class="toc-entry toc-h1"><a href="#Issues">Issues </a>
<ul>
<li class="toc-entry toc-h2"><a href="#CUDA-error:-CUBLAS_STATUS_INTERNAL_ERROR">CUDA error: CUBLAS_STATUS_INTERNAL_ERROR </a></li>
<li class="toc-entry toc-h2"><a href="#RuntimeError:-CUDA-error:-invalid-device-function">RuntimeError: CUDA error: invalid device function </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Stable-baselines-3-user-guide">Stable baselines 3 user guide </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Tips-and-tricks">Tips and tricks </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Discrete-Actions¶>->-Note">Discrete Actions¶&gt; &gt; Note </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Discrete-Actions---Single-Process¶>->-DQN-with-extensions-(double-DQN,-prioritized-replay,-…)-are-the-recommended-algorithms.-We-notably-provide-QR-DQN-in-our-contrib-repo.-DQN-is-usually-slower-to-train-(regarding-wall-clock-time)-but-is-the-most-sample-efficient-(because-of-its-replay-buffer).>->-####-Discrete-Actions---Multiprocessed¶>->-You-should-give-a-try-to-PPO-or-A2C.">Discrete Actions - Single Process¶&gt; &gt; DQN with extensions (double DQN, prioritized replay, …) are the recommended algorithms. We notably provide QR-DQN in our contrib repo. DQN is usually slower to train (regarding wall clock time) but is the most sample efficient (because of its replay buffer).&gt; &gt; #### Discrete Actions - Multiprocessed¶&gt; &gt; You should give a try to PPO or A2C. </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Continuous-Actions¶>->-####-Continuous-Actions---Single-Process¶>->-Current-State-Of-The-Art-(SOTA)-algorithms-are-SAC,-TD3-and-TQC-(available-in-our-contrib-repo).-Please-use-the-hyperparameters-in-the-RL-zoo-for-best-results.>->-####-Continuous-Actions---Multiprocessed¶>->-Take-a-look-at-PPO-or-A2C.-Again,-don’t-forget-to-take-the-hyperparameters-from-the-RL-zoo-for-continuous-actions-problems-(cf-Bullet-envs).">Continuous Actions¶&gt; &gt; #### Continuous Actions - Single Process¶&gt; &gt; Current State Of The Art (SOTA) algorithms are SAC, TD3 and TQC (available in our contrib repo). Please use the hyperparameters in the RL zoo for best results.&gt; &gt; #### Continuous Actions - Multiprocessed¶&gt; &gt; Take a look at PPO or A2C. Again, don’t forget to take the hyperparameters from the RL zoo for continuous actions problems (cf Bullet envs). </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Resource-page">Resource page </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-03-24-stable-baselines-3.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="What-is-stable-baselines-3-(sb3)">
<a class="anchor" href="#What-is-stable-baselines-3-(sb3)" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is stable baselines 3 (sb3)<a class="anchor-link" href="#What-is-stable-baselines-3-(sb3)"> </a>
</h1>
<p>I have just read about this new release. This is a complete rewrite of stable baselines 2, without any reference to tensorflow, and based on pytorch (&gt;1.4+).</p>
<p>There is a lot of running implementations of RL algorithms, based on gym.
A very good introduction in this <a href="https://araffin.github.io/post/sb3/">blog entry</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://araffin.github.io/post/sb3/">Stable-Baselines3: Reliable Reinforcement Learning Implementations | Antonin Raffin | Homepage</a></p>
<p><strong>Links</strong></p>
<ul>
<li>
<p>GitHub repository: <a href="https://github.com/DLR-RM/stable-baselines3">https://github.com/DLR-RM/stable-baselines3</a></p>
</li>
<li>
<p>Documentation: <a href="https://stable-baselines3.readthedocs.io/">https://stable-baselines3.readthedocs.io/</a></p>
</li>
<li>
<p>RL Baselines3 Zoo: <a href="https://github.com/DLR-RM/rl-baselines3-zoo">https://github.com/DLR-RM/rl-baselines3-zoo</a></p>
</li>
<li>
<p>Contrib: <a href="https://github.com/Stable-Baselines-Team/stable-baselines3-contrib">https://github.com/Stable-Baselines-Team/stable-baselines3-contrib</a></p>
</li>
<li>
<p>RL Tutorial: <a href="https://github.com/araffin/rl-tutorial-jnrr19">https://github.com/araffin/rl-tutorial-jnrr19</a></p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="My-installation">
<a class="anchor" href="#My-installation" aria-hidden="true"><span class="octicon octicon-link"></span></a>My installation<a class="anchor-link" href="#My-installation"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Standard installation</p>
<div class="highlight"><pre><span></span>conda create --name stablebaselines3 <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.7
conda activate stablebaselines3
pip install stable-baselines3<span class="o">[</span>extra<span class="o">]</span>
conda install -c conda-forge jupyter_contrib_nbextensions
conda install nb_conda
</pre></div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>conda list
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre># packages in environment at /home/explore/miniconda3/envs/stablebaselines3:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main  
absl-py                   0.12.0                   pypi_0    pypi
atari-py                  0.2.6                    pypi_0    pypi
attrs                     20.3.0             pyhd3deb0d_0    conda-forge
backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
backports                 1.0                        py_2    conda-forge
backports.functools_lru_cache 1.6.1                      py_0    conda-forge
bleach                    3.3.0              pyh44b312d_0    conda-forge
box2d                     2.3.10                   pypi_0    pypi
box2d-py                  2.3.8                    pypi_0    pypi
ca-certificates           2021.1.19            h06a4308_1  
cachetools                4.2.1                    pypi_0    pypi
certifi                   2020.12.5        py37h06a4308_0  
chardet                   4.0.0                    pypi_0    pypi
cloudpickle               1.6.0                    pypi_0    pypi
cycler                    0.10.0                   pypi_0    pypi
decorator                 4.4.2                      py_0    conda-forge
defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
future                    0.18.2                   pypi_0    pypi
google-auth               1.28.0                   pypi_0    pypi
google-auth-oauthlib      0.4.3                    pypi_0    pypi
grpcio                    1.36.1                   pypi_0    pypi
gym                       0.18.0                   pypi_0    pypi
icu                       58.2              hf484d3e_1000    conda-forge
idna                      2.10                     pypi_0    pypi
importlib-metadata        3.7.3            py37h89c1867_0    conda-forge
ipykernel                 5.5.0            py37h888b3d9_1    conda-forge
ipython                   7.21.0           py37h888b3d9_0    conda-forge
ipython_genutils          0.2.0                      py_1    conda-forge
jedi                      0.18.0           py37h89c1867_2    conda-forge
jinja2                    2.11.3             pyh44b312d_0    conda-forge
jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
jupyter_client            6.1.12             pyhd8ed1ab_0    conda-forge
jupyter_contrib_core      0.3.3                      py_2    conda-forge
jupyter_contrib_nbextensions 0.5.1              pyhd8ed1ab_2    conda-forge
jupyter_core              4.7.1            py37h89c1867_0    conda-forge
jupyter_highlight_selected_word 0.2.0           py37h89c1867_1002    conda-forge
jupyter_latex_envs        1.4.6           pyhd8ed1ab_1002    conda-forge
jupyter_nbextensions_configurator 0.4.1            py37h89c1867_2    conda-forge
kiwisolver                1.3.1                    pypi_0    pypi
ld_impl_linux-64          2.33.1               h53a641e_7  
libffi                    3.3                  he6710b0_2  
libgcc-ng                 9.1.0                hdf63c60_0  
libsodium                 1.0.18               h36c2ea0_1    conda-forge
libstdcxx-ng              9.1.0                hdf63c60_0  
libxml2                   2.9.10               hb55368b_3  
libxslt                   1.1.34               hc22bd24_0  
lxml                      4.6.3            py37h9120a33_0  
markdown                  3.3.4                    pypi_0    pypi
markupsafe                1.1.1            py37hb5d75c8_2    conda-forge
matplotlib                3.3.4                    pypi_0    pypi
mistune                   0.8.4           py37h4abf009_1002    conda-forge
nb_conda                  2.2.1                    py37_0  
nb_conda_kernels          2.3.1            py37h06a4308_0  
nbconvert                 5.6.1            py37hc8dfbb8_1    conda-forge
nbformat                  5.1.2              pyhd8ed1ab_1    conda-forge
ncurses                   6.2                  he6710b0_1  
notebook                  5.7.10           py37hc8dfbb8_0    conda-forge
numpy                     1.20.1                   pypi_0    pypi
oauthlib                  3.1.0                    pypi_0    pypi
opencv-python             4.5.1.48                 pypi_0    pypi
openssl                   1.1.1j               h27cfd23_0  
packaging                 20.9               pyh44b312d_0    conda-forge
pandas                    1.2.3                    pypi_0    pypi
pandoc                    2.12                 h7f98852_0    conda-forge
pandocfilters             1.4.2                      py_1    conda-forge
parso                     0.8.1              pyhd8ed1ab_0    conda-forge
pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
pickleshare               0.7.5                   py_1003    conda-forge
pillow                    7.2.0                    pypi_0    pypi
pip                       21.0.1           py37h06a4308_0  
prometheus_client         0.9.0              pyhd3deb0d_0    conda-forge
prompt-toolkit            3.0.18             pyha770c72_0    conda-forge
protobuf                  3.15.6                   pypi_0    pypi
psutil                    5.8.0                    pypi_0    pypi
ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
pyasn1                    0.4.8                    pypi_0    pypi
pyasn1-modules            0.2.8                    pypi_0    pypi
pyglet                    1.5.0                    pypi_0    pypi
pygments                  2.8.1              pyhd8ed1ab_0    conda-forge
pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
pyrsistent                0.17.3           py37h4abf009_1    conda-forge
python                    3.7.10               hdb3f193_0  
python-dateutil           2.8.1                      py_0    conda-forge
python_abi                3.7                     1_cp37m    conda-forge
pytz                      2021.1                   pypi_0    pypi
pyyaml                    5.3.1            py37hb5d75c8_1    conda-forge
pyzmq                     19.0.2           py37hac76be4_2    conda-forge
readline                  8.1                  h27cfd23_0  
requests                  2.25.1                   pypi_0    pypi
requests-oauthlib         1.3.0                    pypi_0    pypi
rsa                       4.7.2                    pypi_0    pypi
scipy                     1.6.1                    pypi_0    pypi
send2trash                1.5.0                      py_0    conda-forge
setuptools                52.0.0           py37h06a4308_0  
six                       1.15.0             pyh9f0ad1d_0    conda-forge
sqlite                    3.35.2               hdfb4753_0  
stable-baselines3         1.0                      pypi_0    pypi
tensorboard               2.4.1                    pypi_0    pypi
tensorboard-plugin-wit    1.8.0                    pypi_0    pypi
terminado                 0.9.3            py37h89c1867_0    conda-forge
testpath                  0.4.4                      py_0    conda-forge
tk                        8.6.10               hbc83047_0  
torch                     1.8.0                    pypi_0    pypi
tornado                   6.1              py37h4abf009_0    conda-forge
traitlets                 5.0.5                      py_0    conda-forge
typing_extensions         3.7.4.3                    py_0    conda-forge
urllib3                   1.26.4                   pypi_0    pypi
wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
webencodings              0.5.1                      py_1    conda-forge
werkzeug                  1.0.1                    pypi_0    pypi
wheel                     0.36.2             pyhd3eb1b0_0  
xz                        5.2.5                h7b6447c_0  
yaml                      0.2.5                h516909a_0    conda-forge
zeromq                    4.3.4                h2531618_0  
zipp                      3.4.1              pyhd8ed1ab_0    conda-forge
zlib                      1.2.11               h7b6447c_3  
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="SB3-tutorials">
<a class="anchor" href="#SB3-tutorials" aria-hidden="true"><span class="octicon octicon-link"></span></a>SB3 tutorials<a class="anchor-link" href="#SB3-tutorials"> </a>
</h1>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">A2C</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.monitor</span> <span class="kn">import</span> <span class="n">Monitor</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.callbacks</span> <span class="kn">import</span> <span class="n">CheckpointCallback</span><span class="p">,</span> <span class="n">EvalCallback</span>

<span class="c1"># Save a checkpoint every 1000 steps</span>
<span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">CheckpointCallback</span><span class="p">(</span><span class="n">save_freq</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="s2">"/home/explore/git/guillaume/stable_baselines_3/logs/"</span><span class="p">,</span>
                                         <span class="n">name_prefix</span><span class="o">=</span><span class="s2">"rl_model"</span><span class="p">)</span>

<span class="c1"># Evaluate the model periodically</span>
<span class="c1"># and auto-save the best model and evaluations</span>
<span class="c1"># Use a monitor wrapper to properly report episode stats</span>
<span class="n">eval_env</span> <span class="o">=</span> <span class="n">Monitor</span><span class="p">(</span><span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">"LunarLander-v2"</span><span class="p">))</span>
<span class="c1"># Use deterministic actions for evaluation</span>
<span class="n">eval_callback</span> <span class="o">=</span> <span class="n">EvalCallback</span><span class="p">(</span><span class="n">eval_env</span><span class="p">,</span> <span class="n">best_model_save_path</span><span class="o">=</span><span class="s2">"/home/explore/git/guillaume/stable_baselines_3/logs/"</span><span class="p">,</span>
                             <span class="n">log_path</span><span class="o">=</span><span class="s2">"/home/explore/git/guillaume/stable_baselines_3/logs/"</span><span class="p">,</span> <span class="n">eval_freq</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                             <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">render</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Train an agent using A2C on LunarLander-v2</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="s2">"MlpPolicy"</span><span class="p">,</span> <span class="s2">"LunarLander-v2"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint_callback</span><span class="p">,</span> <span class="n">eval_callback</span><span class="p">])</span>

<span class="c1"># Retrieve and reset the environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_env</span><span class="p">()</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="c1"># Query the agent (stochastic action here)</span>
<span class="n">action</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Using cuda device
Creating environment from the given name 'LunarLander-v2'
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 97.6     |
|    ep_rew_mean        | -265     |
| time/                 |          |
|    fps                | 484      |
|    iterations         | 100      |
|    time_elapsed       | 1        |
|    total_timesteps    | 500      |
| train/                |          |
|    entropy_loss       | -1.28    |
|    explained_variance | -0.0391  |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | -5.3     |
|    value_loss         | 17.3     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 107      |
|    ep_rew_mean        | -249     |
| time/                 |          |
|    fps                | 499      |
|    iterations         | 200      |
|    time_elapsed       | 2        |
|    total_timesteps    | 1000     |
| train/                |          |
|    entropy_loss       | -1.19    |
|    explained_variance | 0.00593  |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | 2.79     |
|    value_loss         | 16.6     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 108      |
|    ep_rew_mean        | -277     |
| time/                 |          |
|    fps                | 502      |
|    iterations         | 300      |
|    time_elapsed       | 2        |
|    total_timesteps    | 1500     |
| train/                |          |
|    entropy_loss       | -1.22    |
|    explained_variance | -0.00474 |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | -2.79    |
|    value_loss         | 9.45     |
------------------------------------
Eval num_timesteps=2000, episode_reward=-1534.45 +/- 783.96
Episode length: 184.80 +/- 74.13
New best mean reward!
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 185       |
|    mean_reward        | -1.53e+03 |
| rollout/              |           |
|    ep_len_mean        | 117       |
|    ep_rew_mean        | -296      |
| time/                 |           |
|    fps                | 414       |
|    iterations         | 400       |
|    time_elapsed       | 4         |
|    total_timesteps    | 2000      |
| train/                |           |
|    entropy_loss       | -1.1      |
|    explained_variance | 0.0118    |
|    learning_rate      | 0.0007    |
|    n_updates          | 399       |
|    policy_loss        | -1.07     |
|    value_loss         | 5.93      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 131      |
|    ep_rew_mean        | -299     |
| time/                 |          |
|    fps                | 426      |
|    iterations         | 500      |
|    time_elapsed       | 5        |
|    total_timesteps    | 2500     |
| train/                |          |
|    entropy_loss       | -1.17    |
|    explained_variance | 0.00582  |
|    learning_rate      | 0.0007   |
|    n_updates          | 499      |
|    policy_loss        | 2.68     |
|    value_loss         | 14       |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 133       |
|    ep_rew_mean        | -309      |
| time/                 |           |
|    fps                | 428       |
|    iterations         | 600       |
|    time_elapsed       | 6         |
|    total_timesteps    | 3000      |
| train/                |           |
|    entropy_loss       | -0.902    |
|    explained_variance | -1.94e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 599       |
|    policy_loss        | -5.9      |
|    value_loss         | 38.7      |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 133       |
|    ep_rew_mean        | -286      |
| time/                 |           |
|    fps                | 437       |
|    iterations         | 700       |
|    time_elapsed       | 7         |
|    total_timesteps    | 3500      |
| train/                |           |
|    entropy_loss       | -1.13     |
|    explained_variance | -0.000943 |
|    learning_rate      | 0.0007    |
|    n_updates          | 699       |
|    policy_loss        | 1.34      |
|    value_loss         | 5.43      |
-------------------------------------
Eval num_timesteps=4000, episode_reward=-2461.14 +/- 815.61
Episode length: 450.40 +/- 58.92
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 450       |
|    mean_reward        | -2.46e+03 |
| rollout/              |           |
|    ep_len_mean        | 137       |
|    ep_rew_mean        | -276      |
| time/                 |           |
|    fps                | 343       |
|    iterations         | 800       |
|    time_elapsed       | 11        |
|    total_timesteps    | 4000      |
| train/                |           |
|    entropy_loss       | -0.763    |
|    explained_variance | -0.00105  |
|    learning_rate      | 0.0007    |
|    n_updates          | 799       |
|    policy_loss        | 1.13      |
|    value_loss         | 24.1      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 140      |
|    ep_rew_mean        | -274     |
| time/                 |          |
|    fps                | 354      |
|    iterations         | 900      |
|    time_elapsed       | 12       |
|    total_timesteps    | 4500     |
| train/                |          |
|    entropy_loss       | -0.87    |
|    explained_variance | -0.00024 |
|    learning_rate      | 0.0007   |
|    n_updates          | 899      |
|    policy_loss        | 11.7     |
|    value_loss         | 349      |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 151       |
|    ep_rew_mean        | -270      |
| time/                 |           |
|    fps                | 363       |
|    iterations         | 1000      |
|    time_elapsed       | 13        |
|    total_timesteps    | 5000      |
| train/                |           |
|    entropy_loss       | -0.949    |
|    explained_variance | -0.000288 |
|    learning_rate      | 0.0007    |
|    n_updates          | 999       |
|    policy_loss        | -3.85     |
|    value_loss         | 6.77      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 152      |
|    ep_rew_mean        | -262     |
| time/                 |          |
|    fps                | 371      |
|    iterations         | 1100     |
|    time_elapsed       | 14       |
|    total_timesteps    | 5500     |
| train/                |          |
|    entropy_loss       | -0.836   |
|    explained_variance | 0.000152 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1099     |
|    policy_loss        | -2.77    |
|    value_loss         | 4.58     |
------------------------------------
Eval num_timesteps=6000, episode_reward=-1441.51 +/- 344.51
Episode length: 470.00 +/- 216.72
New best mean reward!
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 470       |
|    mean_reward        | -1.44e+03 |
| rollout/              |           |
|    ep_len_mean        | 154       |
|    ep_rew_mean        | -246      |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 1200      |
|    time_elapsed       | 19        |
|    total_timesteps    | 6000      |
| train/                |           |
|    entropy_loss       | -0.655    |
|    explained_variance | 4.6e-05   |
|    learning_rate      | 0.0007    |
|    n_updates          | 1199      |
|    policy_loss        | -3.96     |
|    value_loss         | 10        |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 158       |
|    ep_rew_mean        | -242      |
| time/                 |           |
|    fps                | 323       |
|    iterations         | 1300      |
|    time_elapsed       | 20        |
|    total_timesteps    | 6500      |
| train/                |           |
|    entropy_loss       | -0.799    |
|    explained_variance | -0.000516 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1299      |
|    policy_loss        | -4.78     |
|    value_loss         | 91.6      |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 162       |
|    ep_rew_mean        | -238      |
| time/                 |           |
|    fps                | 330       |
|    iterations         | 1400      |
|    time_elapsed       | 21        |
|    total_timesteps    | 7000      |
| train/                |           |
|    entropy_loss       | -0.623    |
|    explained_variance | -0.000611 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1399      |
|    policy_loss        | 8.3       |
|    value_loss         | 88.8      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 171      |
|    ep_rew_mean        | -227     |
| time/                 |          |
|    fps                | 333      |
|    iterations         | 1500     |
|    time_elapsed       | 22       |
|    total_timesteps    | 7500     |
| train/                |          |
|    entropy_loss       | -0.956   |
|    explained_variance | 1.63e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1499     |
|    policy_loss        | -3.67    |
|    value_loss         | 41.2     |
------------------------------------
Eval num_timesteps=8000, episode_reward=-684.75 +/- 100.33
Episode length: 490.60 +/- 85.08
New best mean reward!
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 491      |
|    mean_reward        | -685     |
| rollout/              |          |
|    ep_len_mean        | 174      |
|    ep_rew_mean        | -223     |
| time/                 |          |
|    fps                | 302      |
|    iterations         | 1600     |
|    time_elapsed       | 26       |
|    total_timesteps    | 8000     |
| train/                |          |
|    entropy_loss       | -0.144   |
|    explained_variance | 0.00724  |
|    learning_rate      | 0.0007   |
|    n_updates          | 1599     |
|    policy_loss        | -0.162   |
|    value_loss         | 61.2     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 176      |
|    ep_rew_mean        | -220     |
| time/                 |          |
|    fps                | 308      |
|    iterations         | 1700     |
|    time_elapsed       | 27       |
|    total_timesteps    | 8500     |
| train/                |          |
|    entropy_loss       | -0.528   |
|    explained_variance | 9.72e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1699     |
|    policy_loss        | 2        |
|    value_loss         | 7.58     |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 175       |
|    ep_rew_mean        | -219      |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 1800      |
|    time_elapsed       | 28        |
|    total_timesteps    | 9000      |
| train/                |           |
|    entropy_loss       | -0.726    |
|    explained_variance | -0.000773 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1799      |
|    policy_loss        | 9.47      |
|    value_loss         | 243       |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 179       |
|    ep_rew_mean        | -218      |
| time/                 |           |
|    fps                | 320       |
|    iterations         | 1900      |
|    time_elapsed       | 29        |
|    total_timesteps    | 9500      |
| train/                |           |
|    entropy_loss       | -0.609    |
|    explained_variance | -0.000227 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1899      |
|    policy_loss        | 2.69      |
|    value_loss         | 145       |
-------------------------------------
Eval num_timesteps=10000, episode_reward=-1105.99 +/- 697.41
Episode length: 859.40 +/- 72.91
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 859       |
|    mean_reward        | -1.11e+03 |
| rollout/              |           |
|    ep_len_mean        | 181       |
|    ep_rew_mean        | -218      |
| time/                 |           |
|    fps                | 270       |
|    iterations         | 2000      |
|    time_elapsed       | 37        |
|    total_timesteps    | 10000     |
| train/                |           |
|    entropy_loss       | -1.03     |
|    explained_variance | -0.11     |
|    learning_rate      | 0.0007    |
|    n_updates          | 1999      |
|    policy_loss        | -3.99     |
|    value_loss         | 29.9      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 181      |
|    ep_rew_mean        | -218     |
| time/                 |          |
|    fps                | 273      |
|    iterations         | 2100     |
|    time_elapsed       | 38       |
|    total_timesteps    | 10500    |
| train/                |          |
|    entropy_loss       | -0.895   |
|    explained_variance | 0.191    |
|    learning_rate      | 0.0007   |
|    n_updates          | 2099     |
|    policy_loss        | 0.0962   |
|    value_loss         | 0.102    |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 198       |
|    ep_rew_mean        | -212      |
| time/                 |           |
|    fps                | 278       |
|    iterations         | 2200      |
|    time_elapsed       | 39        |
|    total_timesteps    | 11000     |
| train/                |           |
|    entropy_loss       | -0.706    |
|    explained_variance | -5.46e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2199      |
|    policy_loss        | 7.79      |
|    value_loss         | 206       |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 200       |
|    ep_rew_mean        | -213      |
| time/                 |           |
|    fps                | 283       |
|    iterations         | 2300      |
|    time_elapsed       | 40        |
|    total_timesteps    | 11500     |
| train/                |           |
|    entropy_loss       | -0.797    |
|    explained_variance | -0.000233 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2299      |
|    policy_loss        | -5.37     |
|    value_loss         | 22.2      |
-------------------------------------
Eval num_timesteps=12000, episode_reward=-175.07 +/- 257.78
Episode length: 767.20 +/- 287.16
New best mean reward!
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 767       |
|    mean_reward        | -175      |
| rollout/              |           |
|    ep_len_mean        | 202       |
|    ep_rew_mean        | -210      |
| time/                 |           |
|    fps                | 253       |
|    iterations         | 2400      |
|    time_elapsed       | 47        |
|    total_timesteps    | 12000     |
| train/                |           |
|    entropy_loss       | -0.999    |
|    explained_variance | -0.000878 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2399      |
|    policy_loss        | -0.454    |
|    value_loss         | 3.24      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 208      |
|    ep_rew_mean        | -210     |
| time/                 |          |
|    fps                | 256      |
|    iterations         | 2500     |
|    time_elapsed       | 48       |
|    total_timesteps    | 12500    |
| train/                |          |
|    entropy_loss       | -0.0409  |
|    explained_variance | -0.00201 |
|    learning_rate      | 0.0007   |
|    n_updates          | 2499     |
|    policy_loss        | -0.00709 |
|    value_loss         | 1.45     |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 206       |
|    ep_rew_mean        | -206      |
| time/                 |           |
|    fps                | 262       |
|    iterations         | 2600      |
|    time_elapsed       | 49        |
|    total_timesteps    | 13000     |
| train/                |           |
|    entropy_loss       | -0.394    |
|    explained_variance | -0.000404 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2599      |
|    policy_loss        | -3.02     |
|    value_loss         | 27.3      |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 209       |
|    ep_rew_mean        | -202      |
| time/                 |           |
|    fps                | 266       |
|    iterations         | 2700      |
|    time_elapsed       | 50        |
|    total_timesteps    | 13500     |
| train/                |           |
|    entropy_loss       | -0.727    |
|    explained_variance | -0.000365 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2699      |
|    policy_loss        | 4.28      |
|    value_loss         | 52.3      |
-------------------------------------
Eval num_timesteps=14000, episode_reward=-45.13 +/- 159.96
Episode length: 595.40 +/- 209.59
New best mean reward!
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 595       |
|    mean_reward        | -45.1     |
| rollout/              |           |
|    ep_len_mean        | 210       |
|    ep_rew_mean        | -200      |
| time/                 |           |
|    fps                | 251       |
|    iterations         | 2800      |
|    time_elapsed       | 55        |
|    total_timesteps    | 14000     |
| train/                |           |
|    entropy_loss       | -0.482    |
|    explained_variance | -3.78e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2799      |
|    policy_loss        | 8.53      |
|    value_loss         | 126       |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 213       |
|    ep_rew_mean        | -199      |
| time/                 |           |
|    fps                | 256       |
|    iterations         | 2900      |
|    time_elapsed       | 56        |
|    total_timesteps    | 14500     |
| train/                |           |
|    entropy_loss       | -0.692    |
|    explained_variance | -0.000127 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2899      |
|    policy_loss        | -0.475    |
|    value_loss         | 3.51      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 213      |
|    ep_rew_mean        | -197     |
| time/                 |          |
|    fps                | 260      |
|    iterations         | 3000     |
|    time_elapsed       | 57       |
|    total_timesteps    | 15000    |
| train/                |          |
|    entropy_loss       | -0.373   |
|    explained_variance | 0.0614   |
|    learning_rate      | 0.0007   |
|    n_updates          | 2999     |
|    policy_loss        | 0.592    |
|    value_loss         | 61.1     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 217      |
|    ep_rew_mean        | -192     |
| time/                 |          |
|    fps                | 263      |
|    iterations         | 3100     |
|    time_elapsed       | 58       |
|    total_timesteps    | 15500    |
| train/                |          |
|    entropy_loss       | -0.827   |
|    explained_variance | 0.000454 |
|    learning_rate      | 0.0007   |
|    n_updates          | 3099     |
|    policy_loss        | -4.96    |
|    value_loss         | 167      |
------------------------------------
Eval num_timesteps=16000, episode_reward=-212.05 +/- 90.82
Episode length: 606.80 +/- 136.16
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 607      |
|    mean_reward        | -212     |
| rollout/              |          |
|    ep_len_mean        | 219      |
|    ep_rew_mean        | -192     |
| time/                 |          |
|    fps                | 250      |
|    iterations         | 3200     |
|    time_elapsed       | 63       |
|    total_timesteps    | 16000    |
| train/                |          |
|    entropy_loss       | -0.397   |
|    explained_variance | -0.0068  |
|    learning_rate      | 0.0007   |
|    n_updates          | 3199     |
|    policy_loss        | 0.713    |
|    value_loss         | 4.8      |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 222       |
|    ep_rew_mean        | -187      |
| time/                 |           |
|    fps                | 253       |
|    iterations         | 3300      |
|    time_elapsed       | 65        |
|    total_timesteps    | 16500     |
| train/                |           |
|    entropy_loss       | -0.691    |
|    explained_variance | -0.000298 |
|    learning_rate      | 0.0007    |
|    n_updates          | 3299      |
|    policy_loss        | -3.58     |
|    value_loss         | 29.9      |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 222       |
|    ep_rew_mean        | -185      |
| time/                 |           |
|    fps                | 257       |
|    iterations         | 3400      |
|    time_elapsed       | 66        |
|    total_timesteps    | 17000     |
| train/                |           |
|    entropy_loss       | -0.678    |
|    explained_variance | -6.87e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 3399      |
|    policy_loss        | 4.88      |
|    value_loss         | 58.4      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 222      |
|    ep_rew_mean        | -183     |
| time/                 |          |
|    fps                | 260      |
|    iterations         | 3500     |
|    time_elapsed       | 67       |
|    total_timesteps    | 17500    |
| train/                |          |
|    entropy_loss       | -0.973   |
|    explained_variance | -0.46    |
|    learning_rate      | 0.0007   |
|    n_updates          | 3499     |
|    policy_loss        | -1.88    |
|    value_loss         | 100      |
------------------------------------
Eval num_timesteps=18000, episode_reward=1.73 +/- 344.63
Episode length: 609.20 +/- 210.81
New best mean reward!
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 609      |
|    mean_reward        | 1.73     |
| rollout/              |          |
|    ep_len_mean        | 228      |
|    ep_rew_mean        | -177     |
| time/                 |          |
|    fps                | 245      |
|    iterations         | 3600     |
|    time_elapsed       | 73       |
|    total_timesteps    | 18000    |
| train/                |          |
|    entropy_loss       | -0.149   |
|    explained_variance | 0.0186   |
|    learning_rate      | 0.0007   |
|    n_updates          | 3599     |
|    policy_loss        | 0.198    |
|    value_loss         | 31.9     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 228      |
|    ep_rew_mean        | -173     |
| time/                 |          |
|    fps                | 248      |
|    iterations         | 3700     |
|    time_elapsed       | 74       |
|    total_timesteps    | 18500    |
| train/                |          |
|    entropy_loss       | -0.369   |
|    explained_variance | 0.0153   |
|    learning_rate      | 0.0007   |
|    n_updates          | 3699     |
|    policy_loss        | 0.175    |
|    value_loss         | 2.75     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 228      |
|    ep_rew_mean        | -171     |
| time/                 |          |
|    fps                | 252      |
|    iterations         | 3800     |
|    time_elapsed       | 75       |
|    total_timesteps    | 19000    |
| train/                |          |
|    entropy_loss       | -1.09    |
|    explained_variance | 0.00332  |
|    learning_rate      | 0.0007   |
|    n_updates          | 3799     |
|    policy_loss        | -5.61    |
|    value_loss         | 94.5     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 228      |
|    ep_rew_mean        | -172     |
| time/                 |          |
|    fps                | 255      |
|    iterations         | 3900     |
|    time_elapsed       | 76       |
|    total_timesteps    | 19500    |
| train/                |          |
|    entropy_loss       | -0.441   |
|    explained_variance | -17.2    |
|    learning_rate      | 0.0007   |
|    n_updates          | 3899     |
|    policy_loss        | 0.701    |
|    value_loss         | 23.8     |
------------------------------------
Eval num_timesteps=20000, episode_reward=2.77 +/- 147.98
Episode length: 394.00 +/- 204.93
New best mean reward!
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 394      |
|    mean_reward        | 2.77     |
| rollout/              |          |
|    ep_len_mean        | 227      |
|    ep_rew_mean        | -170     |
| time/                 |          |
|    fps                | 250      |
|    iterations         | 4000     |
|    time_elapsed       | 79       |
|    total_timesteps    | 20000    |
| train/                |          |
|    entropy_loss       | -0.353   |
|    explained_variance | 0.221    |
|    learning_rate      | 0.0007   |
|    n_updates          | 3999     |
|    policy_loss        | 0.00796  |
|    value_loss         | 0.0138   |
------------------------------------
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Issues">
<a class="anchor" href="#Issues" aria-hidden="true"><span class="octicon octicon-link"></span></a>Issues<a class="anchor-link" href="#Issues"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="CUDA-error:-CUBLAS_STATUS_INTERNAL_ERROR">
<a class="anchor" href="#CUDA-error:-CUBLAS_STATUS_INTERNAL_ERROR" aria-hidden="true"><span class="octicon octicon-link"></span></a>CUDA error: CUBLAS_STATUS_INTERNAL_ERROR<a class="anchor-link" href="#CUDA-error:-CUBLAS_STATUS_INTERNAL_ERROR"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Downgrade pytorch to 1.7.1</p>
<p>to avoid <code>RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasCreate(handle)</code></p>
<div class="highlight"><pre><span></span>pip install <span class="nv">torch</span><span class="o">==</span><span class="m">1</span>.7.1
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="RuntimeError:-CUDA-error:-invalid-device-function">
<a class="anchor" href="#RuntimeError:-CUDA-error:-invalid-device-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>RuntimeError: CUDA error: invalid device function<a class="anchor-link" href="#RuntimeError:-CUDA-error:-invalid-device-function"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Thu Mar 25 09:13:49 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 4000     Off  | 00000000:01:00.0  On |                  N/A |
| N/A   41C    P5    18W /  N/A |   2104MiB /  7982MiB |     32%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1153      G   /usr/lib/xorg/Xorg                162MiB |
|    0   N/A  N/A      1904      G   /usr/lib/xorg/Xorg                268MiB |
|    0   N/A  N/A      2076      G   /usr/bin/gnome-shell              403MiB |
|    0   N/A  N/A      2697      G   ...gAAAAAAAAA --shared-files       54MiB |
|    0   N/A  N/A      7220      G   ...AAAAAAAAA= --shared-files       84MiB |
|    0   N/A  N/A     57454      G   /usr/lib/firefox/firefox            2MiB |
|    0   N/A  N/A     59274      C   ...ablebaselines3/bin/python     1051MiB |
+-----------------------------------------------------------------------------+
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>CUDA version is 11.0 on my workstation.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvcc --version
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>conda install <span class="nv">pytorch</span><span class="o">==</span><span class="m">1</span>.7.1 <span class="nv">torchvision</span><span class="o">==</span><span class="m">0</span>.8.2 <span class="nv">torchaudio</span><span class="o">==</span><span class="m">0</span>.7.2 <span class="nv">cudatoolkit</span><span class="o">=</span><span class="m">11</span>.0 -c pytorch
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting package metadata (current_repodata.json): done
Solving environment: done

# All requested packages already installed.

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Everything seems fine after these updates.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Stable-baselines-3-user-guide">
<a class="anchor" href="#Stable-baselines-3-user-guide" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stable baselines 3 user guide<a class="anchor-link" href="#Stable-baselines-3-user-guide"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is an impressive documentation associated with stable baselines 3.
<a href="https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html">Quickstart</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tips-and-tricks">
<a class="anchor" href="#Tips-and-tricks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tips and tricks<a class="anchor-link" href="#Tips-and-tricks"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This <a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html">page</a> covers general advice about RL (where to start, which algorithm to choose, how to evaluate an algorithm, …), as well as tips and tricks when using a custom environment or implementing an RL algorithm.</p>
<ul>
<li>Be familiar with RL, see <a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl.html">resource</a> page</li>
<li>read SB3 documentation</li>
<li>do the <a href="https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3">tutorial</a>
</li>
</ul>
<p><strong>Tune hyperparameters</strong>
RL zoo is introduced. It contains some hyperparameter optimization.</p>
<p><strong>RL evaluation</strong>
We suggest you reading <a href="https://arxiv.org/abs/1709.06560">Deep Reinforcement Learning that Matters</a> for a good discussion about RL evaluation.</p>
<p><strong>which algorithm to choose</strong>
1st criteria is discrete vs continuous actions. And 2nd is capacity to parallelize training.</p>
<p><a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html">Reinforcement Learning Tips and Tricks — Stable Baselines3 1.1.0a1 documentation</a></p>
<blockquote>
<h3 id="Discrete-Actions¶&gt;-&gt;-Note">
<a class="anchor" href="#Discrete-Actions%C2%B6&gt;-&gt;-Note" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discrete Actions<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions" title="Permalink to this headline">¶</a>&gt; &gt; Note<a class="anchor-link" href="#Discrete-Actions%C2%B6&gt;-&gt;-Note"> </a>
</h3>
<p>This covers <code>Discrete</code>, <code>MultiDiscrete</code>, <code>Binary</code> and <code>MultiBinary</code> spaces</p>
<h4 id="Discrete-Actions---Single-Process¶&gt;-&gt;-DQN-with-extensions-(double-DQN,-prioritized-replay,-…)-are-the-recommended-algorithms.-We-notably-provide-QR-DQN-in-our-contrib-repo.-DQN-is-usually-slower-to-train-(regarding-wall-clock-time)-but-is-the-most-sample-efficient-(because-of-its-replay-buffer).&gt;-&gt;-####-Discrete-Actions---Multiprocessed¶&gt;-&gt;-You-should-give-a-try-to-PPO-or-A2C.">
<a class="anchor" href="#Discrete-Actions---Single-Process%C2%B6&gt;-&gt;-DQN-with-extensions-(double-DQN,-prioritized-replay,-%E2%80%A6)-are-the-recommended-algorithms.-We-notably-provide-QR-DQN-in-our-contrib-repo.-DQN-is-usually-slower-to-train-(regarding-wall-clock-time)-but-is-the-most-sample-efficient-(because-of-its-replay-buffer).&gt;-&gt;-####-Discrete-Actions---Multiprocessed%C2%B6&gt;-&gt;-You-should-give-a-try-to-PPO-or-A2C." aria-hidden="true"><span class="octicon octicon-link"></span></a>Discrete Actions - Single Process<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions-single-process" title="Permalink to this headline">¶</a>&gt; &gt; <code>DQN</code> with extensions (double DQN, prioritized replay, …) are the recommended algorithms. We notably provide <code>QR-DQN</code> in our <a href="https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html#sb3-contrib">contrib repo</a>. <code>DQN</code> is usually slower to train (regarding wall clock time) but is the most sample efficient (because of its replay buffer).&gt; &gt; #### Discrete Actions - Multiprocessed<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions-multiprocessed" title="Permalink to this headline">¶</a>&gt; &gt; You should give a try to <code>PPO</code> or <code>A2C</code>.<a class="anchor-link" href="#Discrete-Actions---Single-Process%C2%B6&gt;-&gt;-DQN-with-extensions-(double-DQN,-prioritized-replay,-%E2%80%A6)-are-the-recommended-algorithms.-We-notably-provide-QR-DQN-in-our-contrib-repo.-DQN-is-usually-slower-to-train-(regarding-wall-clock-time)-but-is-the-most-sample-efficient-(because-of-its-replay-buffer).&gt;-&gt;-####-Discrete-Actions---Multiprocessed%C2%B6&gt;-&gt;-You-should-give-a-try-to-PPO-or-A2C."> </a>
</h4>
<h3 id="Continuous-Actions¶&gt;-&gt;-####-Continuous-Actions---Single-Process¶&gt;-&gt;-Current-State-Of-The-Art-(SOTA)-algorithms-are-SAC,-TD3-and-TQC-(available-in-our-contrib-repo).-Please-use-the-hyperparameters-in-the-RL-zoo-for-best-results.&gt;-&gt;-####-Continuous-Actions---Multiprocessed¶&gt;-&gt;-Take-a-look-at-PPO-or-A2C.-Again,-don’t-forget-to-take-the-hyperparameters-from-the-RL-zoo-for-continuous-actions-problems-(cf-Bullet-envs).">
<a class="anchor" href="#Continuous-Actions%C2%B6&gt;-&gt;-####-Continuous-Actions---Single-Process%C2%B6&gt;-&gt;-Current-State-Of-The-Art-(SOTA)-algorithms-are-SAC,-TD3-and-TQC-(available-in-our-contrib-repo).-Please-use-the-hyperparameters-in-the-RL-zoo-for-best-results.&gt;-&gt;-####-Continuous-Actions---Multiprocessed%C2%B6&gt;-&gt;-Take-a-look-at-PPO-or-A2C.-Again,-don%E2%80%99t-forget-to-take-the-hyperparameters-from-the-RL-zoo-for-continuous-actions-problems-(cf-Bullet-envs)." aria-hidden="true"><span class="octicon octicon-link"></span></a>Continuous Actions<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions" title="Permalink to this headline">¶</a>&gt; &gt; #### Continuous Actions - Single Process<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions-single-process" title="Permalink to this headline">¶</a>&gt; &gt; Current State Of The Art (SOTA) algorithms are <code>SAC</code>, <code>TD3</code> and <code>TQC</code> (available in our <a href="https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html#sb3-contrib">contrib repo</a>). Please use the hyperparameters in the <a href="https://github.com/DLR-RM/rl-baselines3-zoo">RL zoo</a> for best results.&gt; &gt; #### Continuous Actions - Multiprocessed<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions-multiprocessed" title="Permalink to this headline">¶</a>&gt; &gt; Take a look at <code>PPO</code> or <code>A2C</code>. Again, don’t forget to take the hyperparameters from the <a href="https://github.com/DLR-RM/rl-baselines3-zoo">RL zoo</a> for continuous actions problems (cf <em>Bullet</em> envs).<a class="anchor-link" href="#Continuous-Actions%C2%B6&gt;-&gt;-####-Continuous-Actions---Single-Process%C2%B6&gt;-&gt;-Current-State-Of-The-Art-(SOTA)-algorithms-are-SAC,-TD3-and-TQC-(available-in-our-contrib-repo).-Please-use-the-hyperparameters-in-the-RL-zoo-for-best-results.&gt;-&gt;-####-Continuous-Actions---Multiprocessed%C2%B6&gt;-&gt;-Take-a-look-at-PPO-or-A2C.-Again,-don%E2%80%99t-forget-to-take-the-hyperparameters-from-the-RL-zoo-for-continuous-actions-problems-(cf-Bullet-envs)."> </a>
</h3>
</blockquote>
<p><strong>Creating a custom env</strong></p>
<p>multiple times there are advices about normalizing: observation and action space. A good practice is to rescale your actions to lie in [-1, 1]. This does not limit you as you can easily rescale the action inside the environment</p>
<p><strong>tips and tricks to reproduce a RL paper</strong></p>
<p><a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html">Reinforcement Learning Tips and Tricks — Stable Baselines3 1.1.0a1 documentation</a></p>
<blockquote>
<p>A personal pick (by @araffin) for environments with gradual difficulty in RL with continuous actions:&gt; &gt; 1.  Pendulum (easy to solve)
</p>
<ol>
<li>
<p>HalfCheetahBullet (medium difficulty with local minima and shaped reward)</p>
</li>
<li>
<p>BipedalWalkerHardcore (if it works on that one, then you can have a cookie)</p>
</li>
</ol>
<p>in RL with discrete actions:&gt; &gt; 1.  CartPole-v1 (easy to be better than random agent, harder to achieve maximal performance)
</p>
<ol>
<li>
<p>LunarLander</p>
</li>
<li>
<p>Pong (one of the easiest Atari game)</p>
</li>
<li>
<p>other Atari games (e.g. Breakout)</p>
</li>
</ol>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Resource-page">
<a class="anchor" href="#Resource-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resource page<a class="anchor-link" href="#Resource-page"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl.html">Reinforcement Learning Resources — Stable Baselines3 1.1.0a1 documentation</a></p>
<p>Stable-Baselines3 assumes that you already understand the basic concepts of Reinforcement Learning (RL).</p>
<p>However, if you want to learn about RL, there are several good resources to get started:</p>
<ul>
<li>
<p><a href="https://spinningup.openai.com/en/latest/">OpenAI Spinning Up</a></p>
</li>
<li>
<p><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver’s course</a></p>
</li>
<li>
<p><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">Lilian Weng’s blog</a></p>
</li>
<li>
<p><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Berkeley’s Deep RL Bootcamp</a></p>
</li>
<li>
<p><a href="http://rail.eecs.berkeley.edu/deeprlcourse/">Berkeley’s Deep Reinforcement Learning course</a></p>
</li>
<li>
<p><a href="https://github.com/dennybritz/reinforcement-learning">More resources</a></p>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="castorfou/guillaume_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/guillaume_blog/blog/stable-baselines-3.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/guillaume_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/guillaume_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Journey for a datascientist</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/castorfou" title="castorfou"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/GuillaumeRamel1" title="GuillaumeRamel1"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
