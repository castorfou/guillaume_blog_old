<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introduction to Reinforcement Learning with David Silver | Guillaume’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Introduction to Reinforcement Learning with David Silver" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From deepmind. My notes" />
<meta property="og:description" content="From deepmind. My notes" />
<link rel="canonical" href="https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html" />
<meta property="og:url" content="https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html" />
<meta property="og:site_name" content="Guillaume’s blog" />
<meta property="og:image" content="https://castorfou.github.io/guillaume_blog/images/RL.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-09T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html","@type":"BlogPosting","headline":"Introduction to Reinforcement Learning with David Silver","dateModified":"2021-03-09T00:00:00-06:00","datePublished":"2021-03-09T00:00:00-06:00","image":"https://castorfou.github.io/guillaume_blog/images/RL.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html"},"description":"From deepmind. My notes","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/guillaume_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://castorfou.github.io/guillaume_blog/feed.xml" title="Guillaume's blog" /><link rel="shortcut icon" type="image/x-icon" href="/guillaume_blog/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/guillaume_blog/">Guillaume&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/guillaume_blog/about/">About Me</a><a class="page-link" href="/guillaume_blog/search/">Search</a><a class="page-link" href="/guillaume_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to Reinforcement Learning with David Silver</h1><p class="page-description">From deepmind. My notes</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-03-09T00:00:00-06:00" itemprop="datePublished">
        Mar 9, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#reinforcement learning">reinforcement learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#deepmind">deepmind</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#3921---lecture-1-introduction-to-reinforcement-learning">3/9/21 - Lecture 1: Introduction to Reinforcement Learning</a></li>
<li class="toc-entry toc-h2"><a href="#31021---lecture-2-markov-decision-processes">3/10/21 - Lecture 2: Markov Decision Processes</a></li>
<li class="toc-entry toc-h2"><a href="#31221---lecture-3-planning-by-dynamic-programming">3/12/21 - Lecture 3: Planning by Dynamic Programming</a></li>
<li class="toc-entry toc-h2"><a href="#31521---lecture-4-model-free-prediction">3/15/21 - Lecture 4: Model-Free Prediction</a></li>
</ul><p>This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.</p>

<p><a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver">Website with 10 lectures: videos and slides</a></p>

<p><a href="https://github.com/castorfou/introduction-reinforcement-learning-david-silver">My repo with slides</a></p>

<p><img src="../images/deepmind_sylabus.png" alt=""></p>

<h2 id="3921---lecture-1-introduction-to-reinforcement-learning">
<a class="anchor" href="#3921---lecture-1-introduction-to-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>3/9/21 - Lecture 1: Introduction to Reinforcement Learning</h2>

<p>This introduction is essentially about giving examples of RL to have a good intuition about this field and to provide definitions or context:</p>

<ul>
  <li>Definitions: rewards, actions, agent, environment, state (and history)</li>
  <li>Major components: policy, value function, model</li>
  <li>Categorizing RL agents (taxonomy): value based, policy based, actor critic, model free, model based</li>
  <li>Learning and planning</li>
  <li>Prediction and control</li>
</ul>

<p>And David gives 2 references:</p>

<ul>
  <li>
<a href="/guillaume_blog/blog/reinforcement-learning-readings.html">well known</a> Introduction to Reinforcement Learning, Sutton and Barto, 1998</li>
  <li>Algorithms for Reinforcement Learning, Szepesvari. Available <a href="http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">online</a>.</li>
</ul>

<p>Policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>(s): essentially a map from state to action. Can be deterministic <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>(s) or stochastic <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>(a|s).</p>

<p>Value function v<sub>$\pi$</sub>(s): is a prediction of expected future reward.</p>

<p>Model: it is not the environment itself but useful to predict what the environment will do next. 2 types of models: transitions model and rewards model. Transition model predicts the next state (e.g. based on dynamics). Reward model predicts the next immediate reward.</p>

<p>A lot of algorithms are model-free and doesn’t require these models. It is a fundamental distinctions in RL.</p>

<p><img src="../images/deepmind_lec1_taxonomy.png" alt=""></p>

<p>And then David explains 2 fundamental different problems with Learning vs Planning.</p>

<p>With Learning, environment is unknown, agent interacts directly with the environment and improves its policy.</p>

<p>With Planning, a model of environment is known, and agent “plays” with this model and improves its policy.</p>

<p>These 2 problems may be linked where you start to learn from the environment and apply planning then.</p>

<p>2 examples based on atari games.</p>

<p>Another topic is exploration vs exploitation then prediction and control.</p>

<h2 id="31021---lecture-2-markov-decision-processes">
<a class="anchor" href="#31021---lecture-2-markov-decision-processes" aria-hidden="true"><span class="octicon octicon-link"></span></a>3/10/21 - Lecture 2: Markov Decision Processes</h2>

<p><strong><em>Markov decision processes</em></strong> formally describe an environment for reinforcement learning.</p>

<p><strong><em>Markov property</em></strong>: the future is independent of the past given the present.</p>

<p><strong><em>Markov Process</em></strong> (or <strong><em>Markov Chain</em></strong>) is the tuple (S, P)</p>

<p><img src="../images/deepmind_lec2_markovchain.png" alt=""></p>

<p>We can take sample episodes from this chain. (e.g. C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep)</p>

<p>We can formalize the transition matrix from s to s’.</p>

<p>When you add reward you get <strong><em>Markov reward process</em></strong> (S, P, R, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span>)</p>

<p>Reward here is a function to map for each state the immediate reward.</p>

<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span> is the discounted factor, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> [0,1]. David explains why we could need such discount.</p>

<p><strong><em>Return</em></strong> Gt is the total discounted reward at time-step t for a given sample.</p>

<p><img src="../images/deepmind_lec2_return.png" alt=""></p>

<p><strong><em>Value function</em></strong> v(s) is really what we care about, it is the long-term value of state s.</p>

<p><img src="../images/deepmind_lec2_valuefunction.png" alt=""></p>

<p><strong><em>Bellman Equation for MRPs</em></strong></p>

<p>The value function can be decomposed into two parts:</p>
<ul>
  <li>immediate reward R<sub>t+1</sub>
</li>
  <li>discounted value of next state <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span>.v (S<sub>t+1</sub>)</li>
</ul>

<p><img src="../images/deepmind_lec2_bellman.png" alt=""></p>

<p>We use that to calculate value function with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span> $\neq$ 0.</p>

<p>And calculating value function can be seen as the resolution of this linear equation:</p>

<p><img src="../images/deepmind_lec2_bellman_solving.png" alt=""></p>

<p>And now we introduce actions and it gives <strong><em>Markov Decision Process</em></strong></p>

<p><img src="../images/deepmind_lec2_mdp.png" alt=""></p>

<p>And we introduce policy</p>

<p><img src="../images/deepmind_lec2_policy.png" alt=""></p>

<p>Then we can define the <em>state-value function</em> v<sub>$\pi$</sub>(s,a) for a given policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span></p>

<p><img src="../images/deepmind_lec2_statevaluefunction.png" alt=""></p>

<p>and <em>action-value function</em> q<sub>$\pi$</sub>(s,a) for a given policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span></p>

<p><img src="../images/deepmind_lec2_actionvaluefunction.png" alt=""></p>

<p>And impact on Bellman Equation ends like that:</p>

<p><img src="../images/deepmind_lec2_bellman_mdp.png" alt=""></p>

<p>v is giving us how good it is to be in a state. q is giving us how good is it to take an action.</p>

<p>And then we have the Bellman equation expressed with v and q.</p>

<p>We don’t care much about a given v<sub>$\pi$</sub>, we want to get the best policy. And ultimately to get q<sub>*</sub> which is the <strong>optimal action value function</strong>.</p>

<p><img src="/home/explore/git/guillaume/blog/images/deepmind_lec2_optimal_value_function.png" alt=""></p>

<p>The optimal value function specifies the best possible performance in the MDP.
A MDP is “solved” when we know the optimal value function q<sub>*</sub>.</p>

<p>What we really care about is <strong>optimal policy</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span><sub>*</sub>. There is a partial ordering about policies. And a theorem saying that for any MDP, there exists at least one optimal policy.</p>

<p>So the optimal value function calculation is similar to what we did earlier when we averaged the value of the next state but now we take the max instead of average.</p>

<p>So no we can write the <strong>Bellman Optimality Equation</strong>. Unfortunately this is non-linear.</p>

<p>There are many approaches such as iterative ones.</p>

<ul>
  <li>Value Iteration</li>
  <li>Policy Iteration</li>
  <li>Q-learning</li>
  <li>Sarsa</li>
</ul>

<h2 id="31221---lecture-3-planning-by-dynamic-programming">
<a class="anchor" href="#31221---lecture-3-planning-by-dynamic-programming" aria-hidden="true"><span class="octicon octicon-link"></span></a>3/12/21 - Lecture 3: Planning by Dynamic Programming</h2>

<p>Will discuss from the agent side: how to solve these MDP problems.</p>

<p>David starts with general ideas on dynamic programming. (programming in a sense of policy)</p>

<p>Value function is an important idea for RL because it sotres valuable information that you can later reuse (it embeds solutions). And Bellman equation gives the recursive decomposition.</p>

<p><strong>Planning by Dynamic Programming</strong></p>

<p>We assume full knowledge of the MDP. Dynamic programming is used for planning in an MDP. With 2 usages:</p>

<ul>
  <li>prediction: given MDP and policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">π</span></span></span></span>, we predict the value of this policy v<sub>$\pi$</sub>.</li>
  <li>control: given MDP, we get optimal value function v<sub>&amp;ast;</sub> and optimal policy $\pi$<sub>&amp;ast;</sub>.</li>
</ul>

<p>And by full MDP it would mean for an atari game to have access to internal code to calculate everything.</p>

<p>We need the 2 aspects to solve MDP: prediction to value policy, and control to get the best one.</p>

<p><strong>Policy Evaluation</strong></p>

<p>Problem: evaluate a given policy π
Solution: iterative application of Bellman expectation backup</p>

<p>(Bellman expectation is used in prediction, Bellman optimality is used in control)</p>

<p>David takes an example with a small grid-world and calculates iteratively (k=0, 1, 2, …) v(s) for a uniform random policy (north, south, east, west with prob 0.25) (left column). And then we follow policy greedily using v function. (right column)</p>

<p><strong>Policy Iteration</strong></p>

<p>In small grid-world example, just by evaluating the policy and act greedily were sufficient to get the optimal policy. This is not generally the case. In general, need more iterations of  evaluation (iterative policy evaluation) / improvement (greedy policy).
But this process of policy iteration always converges to π∗</p>

<p>David uses Jack’s Car Rental where it needs 4 steps to get the optimal policy.  And explains why acting greedy improves the policy.       And if improvement stops, Bellman optimality equation is satisfied, we have our optimal policy.</p>

<p>Some question then about convergence of v<sub>$\pi$</sub> . Why not update policy at each step of evaluation -&gt; this is value iteration.</p>

<p><strong>Value Iteration</strong></p>

<p>Problem: find optimal policy π
Solution: iterative application of Bellman optimality backup</p>

<p><strong>Extensions to dynamic programming</strong></p>

<p>DP uses full-width backups. It is effective for medium-sized problems. Curse of dimensionality for large problems. Even one backup can be too expensive.</p>

<p>One solution is to <strong>sample backups</strong>.</p>

<p>Advantages:
Model-free: no advance knowledge of MDP required
Breaks the curse of dimensionality through sampling
Cost of backup is constant, independent of n = |S|</p>

<h2 id="31521---lecture-4-model-free-prediction">
<a class="anchor" href="#31521---lecture-4-model-free-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>3/15/21 - Lecture 4: Model-Free Prediction</h2>

<p>Model-Free: no-one gives us the MDP. And we still want to solve it.</p>

<ul>
  <li>
    <p><strong>Monte-Carlo learning</strong>: basically methods which goes all the way to the end of trajectory and estimates value by looking at sample returns.</p>
  </li>
  <li>
<strong>Temporal-Difference learning</strong>: goes one step ahead and estimates after one step</li>
  <li>
<strong>TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>)</strong>: unify both approaches</li>
</ul>

<p>We give up the assumption giving how the environment works (which is highly unrealistic for interesting problems). We break it down in 2 pieces (as with previous lecture with planning):</p>

<ul>
  <li>policy evaluation case (this lecture) - how much reward we get from that policy (in model-free envt)</li>
  <li>control (next lecture) - find the optimum value function and then optimum policy</li>
</ul>

<p><strong>Monte-Carlo Reinforcement Learning</strong></p>

<p>We go all the way through the episodes and we take sample returns. So the estimated value function can be the average of all returns. You have to terminate to perform this mean.</p>

<p>It means we use the <em>empirical mean return</em> in place of <em>expected return</em>. (by <em>law of large numbers</em>, this average returns will converge to value function as the number of episodes for that state tends to infinity)</p>

<p><strong>Temporal-Difference Reinforcement Learning</strong></p>

<p>TD learns from incomplete episodes, by bootstrapping</p>

<p>David takes an example from Sutton about predicting time to commute home, comparing MC and TD.</p>

<p>TD target (R<sub>t+1</sub>+<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span>V<sub>t+1</sub>) is biased estimate of v<sub>$\pi$</sub>(S<sub>t</sub>), but has lower variance than the return G<sub>t</sub>.</p>

<p>David compares perf of MC, TD(0), … using Random Walk example and different values of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>.</p>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="castorfou/guillaume_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/guillaume_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/guillaume_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Journey for a datascientist</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/castorfou" title="castorfou"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/GuillaumeRamel1" title="GuillaumeRamel1"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
