---
title: "ANITI's first Reinforcement Learning Virtual School"
description: My notes
toc: true
comments: true
layout: post
categories: [reinforcement learning]
image: images/RL.png
---



![](https://d1keuthy5s86c8.cloudfront.net/static/ems/upload/img/72947a097165dcd24a6f700e2f28d690.png)

https://rlvs.aniti.fr/

Schedule is 

## RLVS schedule

This condensed schedule does not include class breaks and social events. Times are Central European Summer Time (UTC+2).

| Schedule   |             |                                                              |                                                              |
| ---------- | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| March 25th | 9:00-9:10   | [Opening remarks](https://rl-vs.github.io/rlvs2021/opening.html) | [S. Gerchinovitz](https://rl-vs.github.io/rlvs2021/sebastien-gerchinovitz.html) |
|            | 9:10-9:30   | [RLVS Overview](https://rl-vs.github.io/rlvs2021/rlvs-overview.html) | [E. Rachelson](https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html) |
|            | 9:30-13:00  | [RL fundamentals](https://rl-vs.github.io/rlvs2021/rl-fundamentals.html) | [E. Rachelson](https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html) |
|            | 14:00-16:00 | [Introduction to Deep Learning](https://rl-vs.github.io/rlvs2021/deep-learning.html) | [D. Wilson](https://rl-vs.github.io/rlvs2021/dennis-wilson.html) |
|            | 16:30-17:30 | [Reward Processing Biases in Humans and RL Agents](https://rl-vs.github.io/rlvs2021/human-behavioral-agents.html) | [I. Rish](https://rl-vs.github.io/rlvs2021/irina-rish.html)  |
|            | 17:45-18:45 | [Introduction to Hierarchical Reinforcement Learning](https://rl-vs.github.io/rlvs2021/hierarchical.html) | [D. Precup](https://rl-vs.github.io/rlvs2021/doina-precup.html) |
| March 26th | 10:00-12:00 | [Stochastic bandits](https://rl-vs.github.io/rlvs2021/stochastic-bandits.html) | [T. Lattimore](https://rl-vs.github.io/rlvs2021/tor-lattimore.html) |
|            | 14:00-16:00 | [Monte Carlo Tree Search](https://rl-vs.github.io/rlvs2021/mcts.html) | [T. Lattimore](https://rl-vs.github.io/rlvs2021/tor-lattimore.html) |
|            | 16:30-17:30 | [Multi-armed bandits in clinical trials](https://rl-vs.github.io/rlvs2021/clinical.html) | [D. A. Berry](https://rl-vs.github.io/rlvs2021/donald-berry.html) |
| April 1st  | 9:00-15:00  | [Deep Q-Networks and its variants](https://rl-vs.github.io/rlvs2021/dqn.html) | [B. Piot](https://rl-vs.github.io/rlvs2021/bilal-piot.html), [C. Tallec](https://rl-vs.github.io/rlvs2021/corentin-tallec.html) |
|            | 15:15-16:15 | [Regularized MDPs](https://rl-vs.github.io/rlvs2021/regularized-mdps.html) | [M. Geist](https://rl-vs.github.io/rlvs2021/matthieu-geist.html) |
|            | 16:30-17:30 | [Regret bounds of model-based reinforcement learning](https://rl-vs.github.io/rlvs2021/regret-bound.html) | [M. Wang](https://rl-vs.github.io/rlvs2021/mengdi-wang.html) |
| April 2nd  | 9:00-12:30  | [Policy Gradients and Actor Critic methods](https://rl-vs.github.io/rlvs2021/pg.html) | [O. Sigaud](https://rl-vs.github.io/rlvs2021/olivier-sigaud.html) |
|            | 14:00-15:00 | [Pitfalls in Policy Gradient methods](https://rl-vs.github.io/rlvs2021/pg-pitfalls.html) | [O. Sigaud](https://rl-vs.github.io/rlvs2021/olivier-sigaud.html) |
|            | 15:30-17:30 | [Exploration in Deep RL](https://rl-vs.github.io/rlvs2021/exploration.html) | [M. Pirotta](https://rl-vs.github.io/rlvs2021/matteo-pirotta.html) |
| April 8th  | 9:00-11:00  | [Evolutionary Reinforcement Learning](https://rl-vs.github.io/rlvs2021/evo-rl.html) | [D. Wilson](https://rl-vs.github.io/rlvs2021/dennis-wilson.html), [J.-B. Mouret](https://rl-vs.github.io/rlvs2021/jean-baptiste-mouret.html) |
|            | 11:30-12:30 | [Evolving Agents that Learn More Like Animals](https://rl-vs.github.io/rlvs2021/evolving-agents.html) | [S. Risi](https://rl-vs.github.io/rlvs2021/sebastian-risi.html) |
|            | 14:00-16:00 | [Micro-data Policy Search](https://rl-vs.github.io/rlvs2021/micro-data.html) | [K. Chatzilygeroudis](https://rl-vs.github.io/rlvs2021/konstantinos-chatzilygeroudis.html), [J.-B. Mouret](https://rl-vs.github.io/rlvs2021/jean-baptiste-mouret.html) |
|            | 16:30-17:30 | [Efficient Motor Skills Learning in Robotics](https://rl-vs.github.io/rlvs2021/efficient-motor.html) | [D. Lee](https://rl-vs.github.io/rlvs2021/dongheui-lee.html) |
| April 9th  | 9:00-13:00  | [RL tips and tricks](https://rl-vs.github.io/rlvs2021/tips-and-tricks.html) | [A. Raffin](https://rl-vs.github.io/rlvs2021/antonin-raffin.html) |
|            | 14:30-15:30 | [Symbolic representations and reinforcement learning](https://rl-vs.github.io/rlvs2021/symbolic.html) | [M. Garnelo](https://rl-vs.github.io/rlvs2021/marta-garnelo.html) |
|            | 15:45-16:45 | [Leveraging model-learning for extreme generalization](https://rl-vs.github.io/rlvs2021/model-learning.html) | [L. P. Kaelbling](https://rl-vs.github.io/rlvs2021/leslie-kaelbling.html) |
|            | 17:00-18:00 | RLVS wrap-up                                                 | [E. Rachelson](https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html) |

## (4/1/21) - [Deep Q-Networks and its variants](https://whova.com/embedded/session/rlstc_202011/1416824/?view=)



Speaker is Bilal Piot.

**Deep Q network** as a solution for a practicable control theory.

Introduction of ALE (Atari Learning Environment)

DQN is (almost) end-to-end: from raw observations to actions. Bilal explains the preprocessing part (from 160x210x3 to 84x84 + stacking 4 frames + downsampling to 15 Hz)

Value Iteration (VI) algorithm: Recurrent algorithm to get Q. $Q_{k+1}=T^*Q$

But it is not practical in a real-world case. What we can do is use interactions with real world. And estimate $Q^*$ using a regression.

Would be interesting to have slides. I like the link between regression notations and VI notation.

From neural Fitted-$Q$ to DQN. Main difference is data collection (in DQN you have updated interactions and it allows exploration, and size of architecture)

With DQN we have acting part and learning part. Acting is the data collection. (using $\epsilon$-greedy policy)



**hands-on based on DQN tutorial notebook.**

had to  `export LD_LIBRARY_PATH=/home/explore/miniconda3/envs/aniti/lib/`

Nice introduction to JAX and haiku. Haiku is similar modules in pytorch and can turn NN into pure version. Which is useful for Jax.



**overview of the literature**

![](https://kstatic.googleusercontent.com/files/f6b5f285173d4449285a8e812b8385f45c03f7104e1c41370a73e0c8558ff82d6a69e60962dd91c4972c444fd73bc4f98a06b5487eff5a037a37bc42f97cef3b)



## (4/2/21) - [From Policy Gradients to Actor Critic methods](https://whova.com/embedded/session/rlstc_202011/1416833/?view=)

