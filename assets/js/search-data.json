{
  
    
        "post0": {
            "title": "Hello nbdev",
            "content": "Resources . Everything is under nbdev website. . 3 resources worth to be mentioning: . nbdev tutorial video on youtube; 1 year old but seems still valid | nbdev tutorial page | nbdev github repo | . What I plan to do is to watch the video part, and keep note of my progress in this blog entry. . Walkthrough tutorial . repo creation . As suggested by Jeremy, I start by creating a github repo named hello_nbdev from a nbdev template. . It is just about clicking this link: https://github.com/fastai/nbdev_template/generate. If I am logged in github it will show the proper page. . github pages . Documentation will be hosted at github (can be hosted anywhere but github seems a straightforward option) and to do that we have to setup github pages: . Settings &gt; Options &gt; Github pages &gt; Source &gt; Master (branch) &gt; /docs (folder) &gt; Save . And when done . Now we can insert this doc url as our repo website setting: . repo home &gt; &lt;&gt; code &gt; about (edit repo details) &gt; Website . Edit settings.ini . Everything is in this file. . Just edit directly from github. . lib_name = nbdev_template # For Enterprise Git add variable repo_name and company name # repo_name = analytics # company_name = nike user = fastai # description = A description of your project # keywords = some keywords # author = Your Name # author_email = email@example.com # copyright = Your Name or Company Name . to . lib_name = hello_nbdev user = castorfou description = A tutorial walkthrough with nbdev keywords = fastai nbdev tutorial author = Guillaume Ramelet author_email = guillaume.ramelet@gmail.com copyright = Guillaume R. . and commit changes . Clone repo . ~/git/guillaume$ git clone git@github.com:castorfou/hello_nbdev.git Cloning into &#39;hello_nbdev&#39;... remote: Enumerating objects: 106, done. remote: Counting objects: 100% (106/106), done. remote: Compressing objects: 100% (94/94), done. remote: Total 106 (delta 7), reused 81 (delta 4), pack-reused 0 Receiving objects: 100% (106/106), 1.02 MiB | 2.45 MiB/s, done. Resolving deltas: 100% (7/7), done. . Setup nbdev python environment . It is not specifically mentionned in the video. For this walkthrough I will use my existing fastai environment. . ~/git/guillaume$ conda activate fastai ~/git/guillaume$ nbdev_ nbdev_build_docs nbdev_diff_nbs nbdev_test_nbs nbdev_build_lib nbdev_fix_merge nbdev_trust_nbs nbdev_bump_version nbdev_install_git_hooks nbdev_update_lib nbdev_clean_nbs nbdev_nb2md nbdev_upgrade nbdev_conda_package nbdev_new nbdev_detach nbdev_read_nbs . As expected nbdev is already integrated in it. . Otherwise my guess is that I have to run conda install -c fastai nbdev under my python env. . Install git hooks . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_install_git_hooks Executing: git config --local include.path ../.gitconfig Success: hooks are installed and repo&#39;s .gitconfig is now trusted . deal with conflicts . If needed in case of conflict, Jeremy explains one can call nbdev_fix_merge filename.ipynb and it will use the standard conflict marker to help you identify and fix the conflict. . Open 00_core.ipynb . create lib (we start with a core module) . Just following Jeremy&#39;s instructions. . Create say_hello function | Use it (example) | Test it (assert) | . build_lib . We can call nbdev_build_lib from anywhere in the repo. . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_build_lib Converted 00_core.ipynb. Converted index.ipynb. . and it creates files, under hello_nbdev . hello_nbdev$ ls hello_nbdev/ core.py __init__.py _nbdev.py __pycache__ . Module Documentation . There are 2 levels of documentation. Documentation for your library that will be in index.ipynb and documentation for your modules that will be directly created from your code/notebooks 00_core.ipynb, etc . And to generate this documentation it will be just a matter of calling nbdev_build_docs. . Library documentation into index.ipynb . create doc . Documentation (what will be puclished) is in index.ipynb. . This is an actual documentation. Documentation won&#39;t be written in markdown. It will be executed as code and rendered as such. How great is that. . To make it happen we have to import our lib just freshly generated. . And now we can use all the part of our lib to explain how it works and why it is great. . say_hello(&quot;Guillaume&quot;) . &#39;Hello Guillaume!&#39; . build_docs . We have to call nbdev_build_docs from our repo root. . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_build_docs converting: /home/explore/git/guillaume/hello_nbdev/00_core.ipynb converting /home/explore/git/guillaume/hello_nbdev/index.ipynb to README.md . commit to publish docs . Here is the list of files to be pushed: . git status Changes to be committed: modified: 00_core.ipynb new file: 00_core.py new file: Makefile modified: README.md new file: docs/_config.yml modified: docs/_data/sidebars/home_sidebar.yml new file: docs/_data/topnav.yml new file: docs/core.html new file: docs/index.html modified: docs/sidebar.json new file: hello_nbdev/__init__.py new file: hello_nbdev/_nbdev.py new file: hello_nbdev/core.py modified: index.ipynb new file: index.py . init.py . Just add from .core import * to __init__.py . !cat /home/explore/git/guillaume/hello_nbdev/hello_nbdev/__init__.py . __version__ = &#34;0.0.1&#34; from .core import * . So that we can easily use hello_nbdev without mentioning core module . commit and push . (fastai) ~/git/guillaume/hello_nbdev$ git commit -m &#39;initial commit&#39; [master 3484db7] initial commit 15 files changed, 520 insertions(+), 31 deletions(-) create mode 100644 00_core.py create mode 100644 Makefile create mode 100644 docs/_config.yml create mode 100644 docs/_data/topnav.yml create mode 100644 docs/core.html create mode 100644 docs/index.html create mode 100644 hello_nbdev/__init__.py create mode 100644 hello_nbdev/_nbdev.py create mode 100644 hello_nbdev/core.py create mode 100644 index.py (fastai) ~/git/guillaume/hello_nbdev$ git push Enumerating objects: 30, done. Counting objects: 100% (30/30), done. Delta compression using up to 12 threads Compressing objects: 100% (19/19), done. Writing objects: 100% (21/21), 4.87 KiB | 2.44 MiB/s, done. Total 21 (delta 7), reused 0 (delta 0) remote: Resolving deltas: 100% (7/7), completed with 5 local objects. remote: remote: GitHub found 1 vulnerability on castorfou/hello_nbdev&#39;s default branch (1 low). To find out more, visit: remote: https://github.com/castorfou/hello_nbdev/security/dependabot/docs/Gemfile.lock/nokogiri/open remote: To github.com:castorfou/hello_nbdev.git 3aec9f4..3484db7 master -&gt; master . And documentation is ready . https://castorfou.github.io/hello_nbdev/ . . Going further . Now that we have a 1st simple example up and running, we can go further with: . classes | autoreload tip | launch nbdev_build_lib from jupyter | run tests in parallel | . Classes . Following tutorial, we can create class HelloSayer and document our methods by calling show_doc(HelloSayer.say). . We can decide to add entries into index.ipynb if this is something worth having at the library level. . autoreload . By adding these lines . %load_ext autoreload %autoreload 2 . your notebook automatically reads in the new modules as soon as the python file changes . launch nbdev scripts directly from jupyter . Make it your last cell . from nbdev.export import notebook2script; notebook2script() . Converted 00_core.ipynb. Converted index.ipynb. . run tests in parallel . Just run nbdev_test_nbs . If your notebook starts with _, it will be excluded from the test list. . Jekyll to view documentation locally . Jekyll is the web server to properly render documentation. This is what is used at github pages. . Installation and setup . From https://jekyllrb.com/docs/installation/ubuntu/, . sudo apt-get install ruby-full build-essential zlib1g-dev . Install variables to use gem: . echo &#39;# Install Ruby Gems to ~/gems&#39; &gt;&gt; ~/.bashrc echo &#39;export GEM_HOME=&quot;$HOME/gems&quot;&#39; &gt;&gt; ~/.bashrc echo &#39;export PATH=&quot;$HOME/gems/bin:$PATH&quot;&#39; &gt;&gt; ~/.bashrc source ~/.bashrc . Install Jekyll and Builder: . gem install jekyll bundler . Setup our lib to use Jekyll . From our docs folder, launch bundle install . (fastai) ~/git/guillaume/hello_nbdev/docs$ bundle install Fetching gem metadata from https://rubygems.org/......... Using concurrent-ruby 1.1.7 .... Bundle complete! 4 Gemfile dependencies, 90 gems now installed. Use `bundle info [gemname]` to see where a bundled gem is installed. . Use it . From repo root, launch make docs_serve . (fastai) ~/git/guillaume/hello_nbdev$ make docs_serve cd docs &amp;&amp; bundle exec jekyll serve Configuration file: /home/explore/git/guillaume/hello_nbdev/docs/_config.yml Source: /home/explore/git/guillaume/hello_nbdev/docs Destination: /home/explore/git/guillaume/hello_nbdev/docs/_site Incremental build: disabled. Enable with --incremental Generating... GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data. done in 0.098 seconds. /home/explore/gems/gems/pathutil-0.16.2/lib/pathutil.rb:502: warning: Using the last argument as keyword parameters is deprecated Auto-regeneration: enabled for &#39;/home/explore/git/guillaume/hello_nbdev/docs&#39; Server address: http://127.0.0.1:4000/hello_nbdev// Server running... press ctrl-c to stop. . It is available locally at http://127.0.0.1:4000/hello_nbdev/ . . Skipped . I have not gone through pypi publication and console_scripts. . I don&#39;t have the need for the moment, if I need that I will add an entry here. .",
            "url": "https://castorfou.github.io/guillaume_blog/fastai/nbdev/jupyter/2021/01/12/nbdev_tutorial.html",
            "relUrl": "/fastai/nbdev/jupyter/2021/01/12/nbdev_tutorial.html",
            "date": " • Jan 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "About my datacamp learning process",
            "content": "Datacamp . I started learning with Datacamp in March 2019. This is a great resource and I recommend all datascience newcomers to give it a shot. . What I like are the consistent courses content. There is an overall logic between all courses. And content is just incredible: more than 300 interactive courses. OK maybe you won&#39;t find all of them super useful but at least you can pick what is of interest for you. Following my learning process it takes me about 8 hours to complete a course. . . Career tracks are a smart way to help you build a 1st tour in your datascience journey. I followed python programmer (old version), data scientist with python (old version) and machine learning scientist with python tracks. Mileage may vary but it is about 20 courses per track. Updated versions of tracks are now online and this is a mix between courses, projects and skills assessments. I have tested one project but it is a little bit too basic for me. . . There is a nice and smooth progress tracking system, and as in a game you earn XP for each achivement. . . Selecting courses . A natural way to select courses is to browse through courses from career tracks. And I will complete courses from new version of career tracks. Or when I need to learn on a new domain, I just search for relevant courses (search engine is very good). . I have 2 ways to track these courses: . bookmarks in Datacamp | . entries in ITP (individual training plan, a big excel list of learning items I plan to follow) | . Learning process . Starting a project . As an example I will use . . which is a project from the new Data Scientist career track and which is in my ITP: . . Git repo - data-scientist-skills . In my data-scientist-skills github repo, I have 2 folders: . Other datacamp courses - where I keep lectures (pdf slides) from datacamp courses | python-sandbox - where I keep notebooks and data from datacamp exercises | . creation of Data Manipulation with pandas folder under Other datacamp courses | creation of data-manipulation-with-pandas folder under python-sandbox | copy of python-sandbox/_1project-template/ into python-sandbox/data-manipulation-with-pandas | . Datacamp project template . In this project template, . data_from_datacamp will store all data needed to launch datacamp exercises | exports_py will contain exports of notebooks in txt/py format (usefull to search on code patterns) | start_env.sh start_env.bat to launch jupyter notebook from the right conda env | downloadfromFileIO.py to download data files from my local notebooks (using in the background file.io) | uploadfromdatacamp.py to upload data files from datacamp | uploadfromdatacamp_examples.py some examples to transfer dataframes, dataseries, lists, ... | . Projects structure . After initialisation, I have the following structure and content: . . On your left lectures (one per chapter) and final certificate. . On your right notebooks. . Notebooks for exercises . Just run the jupyter notebook environment by calling start_env.sh. . Get the chapter title: . . And name the notebook accordingly: . . Then enter interactive instructions. I copy paste instructions using copy selection as markdown firefox add-on. . . Here in this example, if I want to follow instructions locally I need to have homelessness dataframe. . I can use the following code from uploadfromdatacamp_examples.py . ##### Dataframe ################### #upload and download from downloadfromFileIO import saveFromFileIO &quot;&quot;&quot; à executer sur datacamp: (apres copie du code uploadfromdatacamp.py) uploadToFileIO(homelessness) &quot;&quot;&quot; tobedownloaded=&quot;&quot;&quot; {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} &quot;&quot;&quot; prefixToc=&#39;1.1&#39; prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=&quot;&quot;) #initialisation import pandas as pd homelessness = pd.read_csv(prefix+&#39;homelessness.csv&#39;,index_col=0) . Before executing this cell, I have to copy/paste/execute uploadfromdatacamp.py content on datacamp server. And call . uploadToFileIO(homelessness) . Then get the results last line . In [2]: uploadToFileIO(homelessness) {&quot;success&quot;:true,&quot;key&quot;:&quot;vTM1t2ehXds4&quot;,&quot;link&quot;:&quot;https://file.io/vTM1t2ehXds4&quot;,&quot;expiry&quot;:&quot;14 days&quot;} {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} . and copy it in tobedownloaded variable. . Update prefixTOC to the good value (exercise 1.1 is the 1st one in first chapter) which is used as a prefix in data files. And update local variable name and csv file. . Run the cell . Here is the result . Téléchargements à lancer {&#39;pandas.core.frame.DataFrame&#39;: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2528 0 2528 0 0 4870 0 --:--:-- --:--:-- --:--:-- 4870 . And homelessness is available to be used. . Files downloaded are in data_from_datacamp folder. . . And running again the cell won&#39;t download file from file.io, but will read the cached file. (delete file to force download) . Full content of this notebook example at the bottom . keep content in git . ~/git/guillaume/data-scientist-skills$ git add . ~/git/guillaume/data-scientist-skills$ git commit -m &#39;start of data manipulation in pandas course&#39; [master c8696ce] start of data manipulation in pandas course 45 files changed, 9010 insertions(+) create mode 100644 Other datacamp courses/Data Manipulation with pandas/chapter1.pdf create mode 100644 python-sandbox/data-manipulation-with-pandas/.ipynb_checkpoints/chapter1 - Transforming Data-checkpoint.ipynb create mode 100644 python-sandbox/data-manipulation-with-pandas/__pycache__/downloadfromFileIO.cpython-37.pyc create mode 100644 python-sandbox/data-manipulation-with-pandas/chapter1 - Transforming Data.ipynb create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/.empty_dir.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/chapter1 - Transforming Data-Exercise1.1_3277903540843719836.lock create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/chapter1 - Transforming Data-Exercise1.1_homelessness.csv create mode 100644 python-sandbox/data-manipulation-with-pandas/downloadfromFileIO.py create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/.empty_dir.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/Untitled.py create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/Untitled.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/chapter1 - Transforming Data.py create mode 100644 python-sandbox/data-manipulation-with-pandas/start_env.bat create mode 100755 python-sandbox/data-manipulation-with-pandas/start_env.sh create mode 100644 python-sandbox/data-manipulation-with-pandas/uploadfromdatacamp.py create mode 100644 python-sandbox/data-manipulation-with-pandas/uploadfromdatacamp_examples.py ~/git/guillaume/data-scientist-skills$ git push Enumerating objects: 43, done. Counting objects: 100% (43/43), done. Delta compression using up to 12 threads Compressing objects: 100% (38/38), done. Writing objects: 100% (40/40), 5.75 MiB | 3.85 MiB/s, done. Total 40 (delta 8), reused 1 (delta 0) remote: Resolving deltas: 100% (8/8), completed with 3 local objects. To github.com:castorfou/data-scientist-skills.git 89f60e5..c8696ce master -&gt; master . Update progress in ITP . Datacamp is giving instant progress . . So I regularly report this progress (here 0.18/4=5%) in ITP. . keep certificates . I download and keep certificates with lectures. . . Notebook example : Introducing DataFrames . Inspecting a DataFrame | Python . Inspecting a DataFrame . When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this. . .head() returns the first few rows (the “head” of the DataFrame). | .info() shows information on each of the columns, such as the data type and number of missing values. | .shape returns the number of rows and columns of the DataFrame. | .describe() calculates a few summary statistics for each column. | . homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state&#39;s total population. . pandas is imported for you. . init . ##### Dataframe ################### #upload and download from downloadfromFileIO import saveFromFileIO &quot;&quot;&quot; à executer sur datacamp: (apres copie du code uploadfromdatacamp.py) uploadToFileIO(homelessness) &quot;&quot;&quot; tobedownloaded=&quot;&quot;&quot; {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} &quot;&quot;&quot; prefixToc=&#39;1.1&#39; prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=&quot;&quot;) #initialisation import pandas as pd homelessness = pd.read_csv(prefix+&#39;homelessness.csv&#39;,index_col=0) . Téléchargements à lancer {&#39;pandas.core.frame.DataFrame&#39;: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2528 0 2528 0 0 4870 0 --:--:-- --:--:-- --:--:-- 4870 . code . Print the head of the homelessness DataFrame. . print(homelessness.head()) . region state individuals family_members state_pop 0 East South Central Alabama 2570.0 864.0 4887681 1 Pacific Alaska 1434.0 582.0 735139 2 Mountain Arizona 7259.0 2606.0 7158024 3 West South Central Arkansas 2280.0 432.0 3009733 4 Pacific California 109008.0 20964.0 39461588 . Print information about the column types and missing values in homelessness. . print(homelessness.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 51 entries, 0 to 50 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 region 51 non-null object 1 state 51 non-null object 2 individuals 51 non-null float64 3 family_members 51 non-null float64 4 state_pop 51 non-null int64 dtypes: float64(2), int64(1), object(2) memory usage: 2.4+ KB None . Print the number of rows and columns in homelessness. . print(homelessness.shape) . (51, 5) . Print some summary statistics that describe the homelessness DataFrame. . print(homelessness.describe()) . individuals family_members state_pop count 51.000000 51.000000 5.100000e+01 mean 7225.784314 3504.882353 6.405637e+06 std 15991.025083 7805.411811 7.327258e+06 min 434.000000 75.000000 5.776010e+05 25% 1446.500000 592.000000 1.777414e+06 50% 3082.000000 1482.000000 4.461153e+06 75% 6781.500000 3196.000000 7.340946e+06 max 109008.000000 52070.000000 3.946159e+07 . .",
            "url": "https://castorfou.github.io/guillaume_blog/datacamp/data%20science/2021/01/07/Datacamp.html",
            "relUrl": "/datacamp/data%20science/2021/01/07/Datacamp.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Auto export python code from jupyter notebooks",
            "content": "This hack comes from https://github.com/jupyter/notebook/blob/master/docs/source/extending/savehooks.rst. . jupyter_notebook_config.py . Here is the code: . # Based off of https://github.com/jupyter/notebook/blob/master/docs/source/extending/savehooks.rst import io import os from notebook.utils import to_api_path _script_exporter = None _html_exporter = None def script_post_save(model, os_path, contents_manager, **kwargs): &quot;&quot;&quot;convert notebooks to Python script after save with nbconvert replaces `ipython notebook --script` &quot;&quot;&quot; from nbconvert.exporters.script import ScriptExporter from nbconvert.exporters.html import HTMLExporter if model[&#39;type&#39;] != &#39;notebook&#39;: return global _script_exporter if _script_exporter is None: _script_exporter = ScriptExporter(parent=contents_manager) log = contents_manager.log global _html_exporter if _html_exporter is None: _html_exporter = HTMLExporter(parent=contents_manager) log = contents_manager.log # save .py file base, ext = os.path.splitext(os_path) script, resources = _script_exporter.from_filename(os_path) # si le sous rep eports_py existe, on ecrit dedans, sinon on ecrit à la racine sous_rep=&#39;&#39; repertoire=os.path.dirname(base) if os.path.exists(repertoire+&#39;/exports_py&#39;): sous_rep=&#39;/exports_py&#39; basename = os.path.basename(base) script_fname = repertoire+ sous_rep+&#39;/&#39;+basename+resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;base: {}, basename: {}, sous_rep: {}, repertoire: {}&quot;.format(base, basename, sous_rep, repertoire)) log.info(&quot;script_fname: {}&quot;.format(script_fname)) #script_fname = base + resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;Saving script /%s&quot;, to_api_path(script_fname, contents_manager.root_dir)) with io.open(script_fname, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(script) &quot;&quot;&quot; # save html base, ext = os.path.splitext(os_path) script, resources = _html_exporter.from_filename(os_path) script_fname = base + resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;Saving html /%s&quot;, to_api_path(script_fname, contents_manager.root_dir)) with io.open(script_fname, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(script) &quot;&quot;&quot; c.FileContentsManager.post_save_hook = script_post_save . In this version, if a subfolder exports_py exists, .py version will be exported in it. Oherwise it will be exported in the notebook folder. . Maybe in a later version it would be good to export only of this subfolder exists. (for example I don&#39;t need these py files when creating such a blog entry, even if my .gitignore won&#39;t publish .py files) . And to remove the creation of Untitled.txt files when notebooks are just being created (and not yet named). . deployment . Just save/merge this jupyter_notebook_config.py file (download) to your jupyter home directory. . According to Config file and command line options in jupyter documentation, it is located at ~/.jupyter . And in windows it is at C: Users &lt;yourID&gt; .jupyter . This will be valid for all your conda environments. . test . ~/.jupyter$ cp ~/git/guillaume/blog/files/jupyter_notebook_config.py . . Restart Jupyter notebook server and click save on any notebook: . .",
            "url": "https://castorfou.github.io/guillaume_blog/jupyter/2021/01/05/jupyter-export-notebook-as-py.html",
            "relUrl": "/jupyter/2021/01/05/jupyter-export-notebook-as-py.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "From cron to anacron",
            "content": "Current config . I run generate_plots.sh daily at 9:30 AM. However what happens if my PC is off at that time, will have to wait another uptime at 9:30 AM. . Solution is to move from cron to anacron. . From https://www.putorius.net/cron-vs-anacron.html: . . Anacron as user . Anacron is already setup on ubuntu. Actually cron.daily is managed by anacron therefore everything under /etc/cron.daily is run even if your system was off at the time by crontab. . But it is true for root, and has to be setup for users. . I will follow these recommandations: https://askubuntu.com/a/235090 . .anacron folders . Create a .anacron folder in your home directory and in it two subfolders, etc and spool . !mkdir -p ~/.anacron/{etc,spool} . anacrontab . Create a new file ~/.anacron/etc/anacrontab with the following content: . # ~/.anacron/etc/anacrontab: configuration file for anacron # See anacron(8) and anacrontab(5) for details. SHELL=/bin/bash PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/home/explore/miniconda3/bin:/home/explore/miniconda3/condabin:/home/explore/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin # period delay job-identifier command 1 10 squeezebox ~/git/guillaume/squeezebox/generate_plots.sh . start anacron . Add the following line to your crontab using crontab -e: . @hourly /usr/sbin/anacron -s -t $HOME/.anacron/etc/anacrontab -S $HOME/.anacron/spool . And remove squeezebox entry from crontab. . !crontab -l . # NVIDIA SDK Manager updater # NVIDIA SDK Manager updater 0 12 */7 * * /bin/bash /home/explore/.nvsdkm/.updater/updater.sh #30 9 * * * ~/git/guillaume/squeezebox/generate_plots.sh @hourly /usr/sbin/anacron -s -t $HOME/.anacron/etc/anacrontab -S $HOME/.anacron/spool .",
            "url": "https://castorfou.github.io/guillaume_blog/ubuntu/cron/2020/12/13/from-cron-to-anacron.html",
            "relUrl": "/ubuntu/cron/2020/12/13/from-cron-to-anacron.html",
            "date": " • Dec 13, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Repo with 2 remote-urls",
            "content": "from github . I have just created this empty repo: https://github.com/castorfou/data-scientist-skills . from existing local repo connected to gitlab . Add the new remote url to github, name it github . (base) guillaume@LL11LPC0PQARQ:~/git/data-scientist-skills$ git remote add github https://github.com/castorfou/data-scientist-skills.git . Or this is possible to use ssh protocol: git remote add origin git@github.com:castorfou/data-scientist-skills.git . Push repo to this new remote url: git push -u github . (base) guillaume@LL11LPC0PQARQ:~/git/data-scientist-skills$ git push -u github Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Counting objects: 4736, done. Delta compression using up to 8 threads. Compressing objects: 100% (3171/3171), done. Writing objects: 100% (4736/4736), 630.97 MiB | 9.08 MiB/s, done. Total 4736 (delta 1549), reused 4538 (delta 1458) remote: Resolving deltas: 100% (1549/1549), done. To https://github.com/castorfou/data-scientist-skills.git * [new branch] master -&gt; master Branch &#39;master&#39; set up to track remote branch &#39;master&#39; from &#39;github&#39;. . for new local repo . Clone the new repo: git clone git@github.com:castorfou/data-scientist-skills.git . And I want to have same names for same remotes: git remote rename origin github . So now it is quite easy to update from different remote repo: . cat refresh_from_github.sh #!/bin/bash git fetch github git pull .",
            "url": "https://castorfou.github.io/guillaume_blog/git/2020/12/08/share-github-repo-on-2-pc.html",
            "relUrl": "/git/2020/12/08/share-github-repo-on-2-pc.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Push large files to github: git-lfs",
            "content": "Where is the problem . I am currently following deep learning specialization from Andrew Ng on coursera. . In the course 4 about CNNs, there are some pre-trained yolo models that we use to do object detection. And these models come as large .h5 files. . Because I run all programming assignments locally and keep everything (lectures + codes) on my local repo, when I pushed to github I got this error: . (base) explore@explore-ThinkPad-P53:~/git/guillaume/deeplearning_specialization$ git push Enumerating objects: 247, done. Counting objects: 100% (247/247), done. Delta compression using up to 12 threads Compressing objects: 100% (239/239), done. Writing objects: 100% (242/242), 707.06 MiB | 4.74 MiB/s, done. Total 242 (delta 6), reused 0 (delta 0) remote: Resolving deltas: 100% (6/6), completed with 3 local objects. remote: warning: File notebooks/C4W3/nb_images/pred_video.mp4 is 85.44 MB; this is larger than GitHub&#39;s recommended maximum file size of 50.00 MB remote: warning: File notebooks/C4W3/nb_images/road_video.mp4 is 81.71 MB; this is larger than GitHub&#39;s recommended maximum file size of 50.00 MB remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com. remote: error: Trace: 2d1944991c30279b831124b51e4aac57a17a860f2ef789b4e32801fb65282244 remote: error: See http://git.io/iEPt8g for more information. remote: error: File notebooks/C4W2/ResNet50.h5 is 270.32 MB; this exceeds GitHub&#39;s file size limit of 100.00 MB remote: error: File notebooks/C4W3/model_data/yolo.h5 is 194.69 MB; this exceeds GitHub&#39;s file size limit of 100.00 MB To github.com:castorfou/deeplearning_specialization.git ! [remote rejected] master -&gt; master (pre-receive hook declined) . Solution: git-lfs . As explained in https://github.com/git-lfs/git-lfs/wiki/Tutorial, there is (always) a way to do it properly. . First it is a matter of installing git-lfs: . sudo apt-get install git-lfs . Then to setup git lfs . git lfs install . And then to &quot;migrate&quot; big files to lfs: . git lfs migrate import --include=&quot;*.mp4&quot; . git lfs migrate import --include=&quot;*.h5&quot; . And now to git push . (base) explore@explore-ThinkPad-P53:~/git/guillaume/deeplearning_specialization$ git push Uploading LFS objects: 100% (25/25), 954 MB | 37 MB/s, done. Enumerating objects: 311, done. Counting objects: 100% (311/311), done. Delta compression using up to 12 threads Compressing objects: 100% (273/273), done. Writing objects: 100% (276/276), 60.49 MiB | 5.59 MiB/s, done. Total 276 (delta 18), reused 0 (delta 0) remote: Resolving deltas: 100% (18/18), completed with 16 local objects. To github.com:castorfou/deeplearning_specialization.git d0d2dc2..004fa09 master -&gt; master .",
            "url": "https://castorfou.github.io/guillaume_blog/git/2020/12/02/push-big-files-to-github.html",
            "relUrl": "/git/2020/12/02/push-big-files-to-github.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Open Jupyter Notebook with http launch instead of redirect file",
            "content": "Where is the problem? . Default configuration when launching jupyter notebook is to create a redirect file. . Here is the explanation from config file ~/.jupyter/jupyter_notebook_config.py. . ## Disable launching browser by redirect file # # For versions of notebook &gt; 5.7.2, a security feature measure was added that # prevented the authentication token used to launch the browser from being # visible. This feature makes it difficult for other users on a multi-user # system from running code in your Jupyter session as you. # # However, some environments (like Windows Subsystem for Linux (WSL) and # Chromebooks), launching a browser using a redirect file can lead the browser # failing to load. This is because of the difference in file structures/paths # between the runtime and the browser. # # Disabling this setting to False will disable this behavior, allowing the # browser to launch by using a URL and visible token (as before). #c.NotebookApp.use_redirect_file = True . And when launching jupyter notebook from WSL . (xgboost) guillaume@LL11LPC0PQARQ:~/git/d059-vld-ic$ jupyter notebook [I 13:09:57.346 NotebookApp] The port 8888 is already in use, trying another port. [I 13:09:57.370 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1 [I 13:09:57.371 NotebookApp] Serving notebooks from local directory: /mnt/d/git/d059-vld-ic [I 13:09:57.372 NotebookApp] The Jupyter Notebook is running at: [I 13:09:57.373 NotebookApp] http://localhost:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 [I 13:09:57.373 NotebookApp] or http://127.0.0.1:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 [I 13:09:57.374 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 13:10:00.384 NotebookApp] To access the notebook, open this file in a browser: file:///home/guillaume/.local/share/jupyter/runtime/nbserver-828-open.html Or copy and paste one of these URLs: http://localhost:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 or http://127.0.0.1:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 . . Solution . As given in https://stackoverflow.com/questions/57679894/how-to-change-jupyter-launch-from-file-to-url, . update jupyter config file to change #c.NotebookApp.use_redirect_file = True to c.NotebookApp.use_redirect_file = False . .",
            "url": "https://castorfou.github.io/guillaume_blog/jupyter/wsl/2020/10/21/open-jupyter-from-http-link.html",
            "relUrl": "/jupyter/wsl/2020/10/21/open-jupyter-from-http-link.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Use git with github (ssh) behind corporate proxy",
            "content": "Configuration . . I use 2 kinds of repo. gitlab for internal/corporate projects, hosted inside my company. github for public/pet projects and as a blogging platform. 3 days a week I am inside company, 4 days a week outside. . Green lines are the natural path to collaborate. . When outside I don&#39;t have proxy configuration or firewall, and I can directly access github. I cannot access to gitlab but I don&#39;t want to address it now, this is why it is set as a black line. (if this is really needed I have a vpn access and this is as being inside) . When inside, I use internal proxy. I can directly access gitlab. But I want to access github in a transparent way. And yes from both Windows and Linux (WSL). This is the red line. . Setup the red line . Here is the https://stackoverflow.com/questions/21318535/how-to-setup-corkscrew-to-connect-to-github-through-draconian-proxy discussion. . 1st step is to install workscrew: sudo apt install corkscrew . Then I create 2 ssh config files: . (base) guillaume@LL11LPC0PQARQ:~/proxy_files$ cat ssh_config_noproxy Host github.com IdentityFile ~/.ssh/id_rsa_gmail Host gitlab.&lt;mycompany&gt;.com IdentityFile ~/.ssh/id_rsa (base) guillaume@LL11LPC0PQARQ:~/proxy_files$ cat ssh_config_proxy Host github.com ProxyCommand /usr/bin/corkscrew &lt;my_proxy_hostname&gt; &lt;my_proxy_port&gt; %h %p IdentityFile ~/.ssh/id_rsa_gmail Host gitlab.&lt;mycompany&gt;.com IdentityFile ~/.ssh/id_rsa . It is just a matter of linking appropriate files when I am in or out of corporate network. . As in my_ip.sh: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf #proxy for conda ln -sf ~/proxy_files/.condarc_proxy ~/.condarc #proxy for git git config --global http.proxy http://proxy_ip:80 ln -sf ~/proxy_files/ssh_config_proxy ~/.ssh/config } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf #no proxy for conda ln -sf ~/proxy_files/.condarc_noproxy ~/.condarc #no proxy for git git config --global --unset http.proxy ln -sf ~/proxy_files/ssh_config_noproxy ~/.ssh/config } .",
            "url": "https://castorfou.github.io/guillaume_blog/git/wsl/2020/10/21/github-ssh-behind-proxy.html",
            "relUrl": "/git/wsl/2020/10/21/github-ssh-behind-proxy.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "edX MIT 6.00.2x Introduction to Computational Thinking and Data Science",
            "content": "MIT courses at edX . Last summer I have been following a 1st MIT course on python programming. Not that I would need this knowledge but as for Polytechnique courses, I like their way to explain knowledge foundations. Teachers from these schools tend to go back to deep roots, and provide clear and somtimes illuminating examples to help us understand concepts. . About 6 years ago I have completed a Probability introduction from Ecole Polythechnique. That was great. I had always been hermeticly closed to probability and statistics. For a reason I don&#39;t understand, it is not being teached in CPGE (which is a two-or-three-year intensive full-time course preparing top high school graduates for the entrance examination of French engineering and business schools, this is just after high school). It means last time I was exposed to probability was in high school, and probably in engineering school as well but on a light way. . That would be great to give back a look to these courses. . MIT 6.00.1x - Introduction to Computer Science and Programming Using Python . Unfortunately I registered in September when only a couple of weeks were left to complete this 9-week course. And because I didn&#39;t upgrade to a Verified Certificate, I lost access to materials and progress. It cut when my progress was about 38%. . I like Eric Grimson&#39;s style. He is calm and has his own way to explain some advanced subjects. . Next session is planned on Jan 27, 2021. That would be a good idea to complete this course. . MIT 6.00.2x - Introduction to Computational Thinking and Data Science . For this one I have registered on time. And I have purchased the Verified Certificate. . Here is the full course program and dates: . . 1st lectures are interesting. As said before I like to be back to roots of problems. And on that matter I expect to get a full overview. . Lecture 4 should be released in the coming at the end of October. cannot wait to resume these sessions. . As a matter of comparaison with gan specialization from coursera+openAI, I like better the interactions with students offered by openAI. They use slack as a platform to support these interactions and I think it is a smart move. .",
            "url": "https://castorfou.github.io/guillaume_blog/edx/mit/data%20science/python/2020/10/20/MIT-edx-Introduction-Computational-Thinking-and-Data-Science.html",
            "relUrl": "/edx/mit/data%20science/python/2020/10/20/MIT-edx-Introduction-Computational-Thinking-and-Data-Science.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "GAN Specialization course 2 week 3 - Apply and certificate",
            "content": "Notes . tbd later . Certificate . . About next steps . For the moment I am hesitating to follow last course for gans. This is a deep dive into gan, with lots of theory (papers) associated to it. My 1st goal was to know better about it and this is met. My 2nd one was to figure out how to use this kind of generative networks for other area such as tabular data + prescription issues. I am not sure this is applicable (or not yet). . Maybe it is better for now to resume my learning sessions with Jeremy Howard on fastai v2. .",
            "url": "https://castorfou.github.io/guillaume_blog/gan/pytorch/2020/10/15/gan-course2-week3-certificate.html",
            "relUrl": "/gan/pytorch/2020/10/15/gan-course2-week3-certificate.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "GAN Specialization course 2 week 2 - Disadvantages and Bias",
            "content": "Notes . .",
            "url": "https://castorfou.github.io/guillaume_blog/gan/pytorch/2020/10/09/gan-course2-week2-disadventages-bias.html",
            "relUrl": "/gan/pytorch/2020/10/09/gan-course2-week2-disadventages-bias.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "GAN Specialization course 2 week 1 - evaluations on GANs",
            "content": "Features extraction, inception v3, embeddings, FID (Fr&#233;chet Inception Distance), Sampling and Truncation, Precision and Recall . .",
            "url": "https://castorfou.github.io/guillaume_blog/gan/pytorch/2020/10/09/gan-course2-week1-evaluations-on-gans.html",
            "relUrl": "/gan/pytorch/2020/10/09/gan-course2-week1-evaluations-on-gans.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "GAN Specialization course 1 week 4 - conditional generation, controllable generation",
            "content": "Conditional generation, controllable generation, disentanglement of Z-space . . End of course . Next one is a 3 week course named: Build Better Generative Adversarial Networks (GANs) . Here is my certificate . .",
            "url": "https://castorfou.github.io/guillaume_blog/gan/pytorch/2020/10/07/gan-specialization-week4-conditional_generation-controllable-generation.html",
            "relUrl": "/gan/pytorch/2020/10/07/gan-specialization-week4-conditional_generation-controllable-generation.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "GAN Specialization course 1 week 3 - mode collapse, vanishing gradient, wasserstein loss",
            "content": "Mode collapse, Vanishing gradient . Very good explanation about why it happens. Flat region when discriminator is learning faster (it has an easier job) than generator. . . Earth mover&#39;s distance. Wasserstein loss. 1-L continuous condition . .",
            "url": "https://castorfou.github.io/guillaume_blog/gan/pytorch/2020/10/07/gan-specialization-week3-mode_collapse-W_loss.html",
            "relUrl": "/gan/pytorch/2020/10/07/gan-specialization-week3-mode_collapse-W_loss.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Variables traces using show_guts decorator",
            "content": "show_guts decorator . Adaptaton from https://stackoverflow.com/questions/24165374/printing-a-functions-local-variable-names-and-values . Update to python 3. . import sys import threading def show_guts(f): sentinel = object() gutsdata = threading.local() gutsdata.captured_locals = None gutsdata.tracing = False def trace_locals(frame, event, arg): if event.startswith(&#39;c_&#39;): # C code traces, no new hook return if event == &#39;call&#39;: # start tracing only the first call if gutsdata.tracing: return None gutsdata.tracing = True return trace_locals if event == &#39;line&#39;: # continue tracing return trace_locals # event is either exception or return, capture locals, end tracing gutsdata.captured_locals = frame.f_locals.copy() return None def wrapper(*args, **kw): # preserve existing tracer, start our trace old_trace = sys.gettrace() sys.settrace(trace_locals) retval = sentinel try: retval = f(*args, **kw) finally: # reinstate existing tracer, report, clean up sys.settrace(old_trace) for key, val in gutsdata.captured_locals.items(): print(&#39;{}: {!r}&#39;.format(key, val)) if retval is not sentinel: print(&#39;Returned: {!r}&#39;.format(retval)) gutsdata.captured_locals = None gutsdata.tracing = False return retval return wrapper . use example . import torch from torch import nn from tqdm.auto import tqdm from torchvision import transforms from torchvision.utils import make_grid from torchvision.datasets import CelebA from torch.utils.data import DataLoader import matplotlib.pyplot as plt . @show_guts def get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight): &#39;&#39;&#39; Function to return the score of the current classifications, penalizing changes to other classes with an L2 norm. Parameters: current_classifications: the classifications associated with the current noise original_classifications: the classifications associated with the original noise target_indices: the index of the target class other_indices: the indices of the other classes penalty_weight: the amount that the penalty should be weighted in the overall score &#39;&#39;&#39; # Steps: 1) Calculate the change between the original and current classifications (as a tensor) # by indexing into the other_indices you&#39;re trying to preserve, like in x[:, features]. # 2) Calculate the norm (magnitude) of changes per example. # 3) Multiply the mean of the example norms by the penalty weight. # This will be your other_class_penalty. # Make sure to negate the value since it&#39;s a penalty! # 4) Take the mean of the current classifications for the target feature over all the examples. # This mean will be your target_score. #### START CODE HERE #### change_original_classification = (current_classifications[:,other_indices] - original_classifications[:,other_indices]) # Calculate the norm (magnitude) of changes per example and multiply by penalty weight other_class_penalty = - torch.mean(torch.norm(change_original_classification, dim=1) * penalty_weight) # Take the mean of the current classifications for the target feature target_score = torch.mean(current_classifications) #### END CODE HERE #### return target_score + other_class_penalty . rows = 10 current_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float() original_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float() # Must be 3 assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3 . current_classifications: tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.]]) original_classifications: tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.]]) target_indices: [1, 3] other_indices: [0, 2] penalty_weight: 0.2 change_original_classification: tensor([[0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.]]) other_class_penalty: tensor(-0.) target_score: tensor(2.5000) Returned: tensor(2.5000) . AssertionError Traceback (most recent call last) &lt;ipython-input-5-c7e77f2e4ae1&gt; in &lt;module&gt; 4 5 # Must be 3 -&gt; 6 assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3 AssertionError: .",
            "url": "https://castorfou.github.io/guillaume_blog/python/2020/10/07/decorator-trace-variables.html",
            "relUrl": "/python/2020/10/07/decorator-trace-variables.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Conda activate from bash scripts",
            "content": "Can&#39;t execute conda activate from bash script . Good description of the problem in conda github. . Calling conda activate from a bash script will raise some errors: . CommandNotFoundError: Your shell has not been properly configured to use &#39;conda activate&#39;. To initialize your shell, run $ conda init &lt;SHELL_NAME&gt; Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See &#39;conda init --help&#39; for more information and options. . source ~/your_conda/etc/profile.d/conda.sh . It is just a matter of sourcing the conda bash settings before calling conda activate. . In m case I have installed conda in ~/miniconda3, I just have to call source ~/miniconda3/etc/profile.d/conda.sh . Example to run my blogging environment . #!/bin/bash source ~/miniconda3/etc/profile.d/conda.sh cd ~/git/guillaume/guillaume_blog/_notebooks conda activate fastai jupyter notebook .",
            "url": "https://castorfou.github.io/guillaume_blog/conda/bash/2020/10/07/conda-activate-from-bash-script.html",
            "relUrl": "/conda/bash/2020/10/07/conda-activate-from-bash-script.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "GAN Specialization course 1 week 2 - Deep Convolutional GAN",
            "content": "My notes . .",
            "url": "https://castorfou.github.io/guillaume_blog/gan/pytorch/2020/10/06/gan-specialization-course1-week2-Deep_convolutional_GAN.html",
            "relUrl": "/gan/pytorch/2020/10/06/gan-specialization-course1-week2-Deep_convolutional_GAN.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "GAN Specialization course 1 week 1 - intro to GAN",
            "content": "My notes . . .",
            "url": "https://castorfou.github.io/guillaume_blog/gan/pytorch/2020/10/06/gan-specialization-course1-week1-intro_to_gan.html",
            "relUrl": "/gan/pytorch/2020/10/06/gan-specialization-course1-week1-intro_to_gan.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Generative Adversarial Networks (GANs) Specialization from Coursera",
            "content": "Coursera . This specialization comes in 3 courses. . . Build Basic Generative Adversarial Networks . Build Better Generative Adversarial Networks . Apply Generative Adversarial Networks . env installation . I am just getting the version from coursera to be sure I have the same behaviour. . import torch . print(torch.__version__) . 1.4.0 . So I can now create a gan environment with appropriate lib versions. . conda create -n gan python=3.7 conda activate gan conda install -c pytorch pytorch=1.4.0 conda install jupyter matplotlib conda install -c conda-forge tqdm conda install -c pytorch torchvision . git settings . echo &quot;# gan_specialization from coursera&quot; &gt;&gt; README.md git init git add README.md git commit -m &quot;first commit&quot; git branch -M master git remote add origin git@github.com:castorfou/gan_specialization.git git push -u origin master . I am pushing notebooks to github . Intro to PyTorch . I have exported Intro to Pytorch notebook from coursera lab. . To run it on my machine. . Intro to GAN using tensorflow . Versions of python seems incompatible between each other. (3.7 for pytorch, 3.6 for tensorflow=1.10) . I create a new python environment: . conda create -n gan_tensorflow python=3.6 conda activate gan_tensorflow conda install jupyter conda install -c conda-forge requests pip install tensorflow-gpu==1.15 .",
            "url": "https://castorfou.github.io/guillaume_blog/gan/pytorch/2020/10/01/gan-pytorch-coursera.html",
            "relUrl": "/gan/pytorch/2020/10/01/gan-pytorch-coursera.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Use fingerprint to authenticate on Ubuntu, and passwordless on some apps",
            "content": "Fingerprint authentication . . Just by activating Fingerprint login, quite surprisingly it has been working directly. . Passwordless commands . Because I have changed my password for a quite complex one, I am interested to launch some sudo commands without prompt of password. . How to run sudo commands without password . Use visudo to update /etc/sudoers. I understand there is some syntax check to avoid mistake when editing this file. You don&#39;t want to be left with a defective sudo system. . I have just added this line. explore is my username. I can add additional commands after a comma (e.g. /bin/systemctl restart httpd.service, /bin/kill) . explore ALL = NOPASSWD: /usr/bin/apt .",
            "url": "https://castorfou.github.io/guillaume_blog/ubuntu/2020/10/01/fingerprint-authentication-sudoers.html",
            "relUrl": "/ubuntu/2020/10/01/fingerprint-authentication-sudoers.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Upgrade ubuntu LTS 18.04 to 20.04",
            "content": "Standard upgrade process . As a LTS user, I want to keep using these long term support version. . !cat /etc/issue . Ubuntu 18.04.5 LTS n l . A good way to do it is by using do-release-upgrade tool. Full explanation at: 18.04 to 20.04. . sudo do-release-upgrade Checking for a new Ubuntu release There is no development version of an LTS available. To upgrade to the latest non-LTS develoment release set Prompt=normal in /etc/update-manager/release-upgrades. . Waiting for blockers to be fixed . There is a last blocker before releasing Ubuntu 20.04.1 LTS. . . Expected around 1st of October 2020. . (2020-09-28) blockers are fixed, upgrade in progress . Unfortunately the upgrade process went uneventful. Nothing broke, nothing to learn ;) . It took minutes to do the upgrade. . . Ubuntu releases-code names .",
            "url": "https://castorfou.github.io/guillaume_blog/ubuntu/2020/09/28/upgrade-ubuntu-18.04-to-20.04.html",
            "relUrl": "/ubuntu/2020/09/28/upgrade-ubuntu-18.04-to-20.04.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Multiple subplots and animations with matplotlib",
            "content": "Subplots . What I want it to display multiple plots, with a given max rows. And to display my plots depending only on these parameters. . from fastai.tabular.all import * %matplotlib inline # fastai v1 backward compatibility import matplotlib.pyplot as plt import torch import torch.nn as nn import numpy as np . def my_hidden_f(x): return 4*x**3+2*x**2-12*x+5+10*torch.rand(x.shape) n=100 time = torch.ones(n,1) time[:,0].uniform_(-3.14,3.14) speed=my_hidden_f(time) plt.scatter(time[:,0], speed) plt.scatter(tensor(-1.5), my_hidden_f(tensor([-1.5])), color=&#39;red&#39;) def f(t, params): a,b,c,d = params return a*(t**3) + (b*t**2) + c*t + d def mse(preds, targets): return ((preds-targets)**2).mean() def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-50,150) lr = 1e-4 def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . params = torch.randn(4).requires_grad_() #nbr of iterations max_iter = 1000 #nbr of curves visible nbr_graph = 4 #max number of curves on one row max_columns = 5 #nbr of rows max_rows = (nbr_graph-1) // max_columns + 1 #nbr of iter per plot graph_iteration = max_iter //(nbr_graph-1) _,axs = plt.subplots(nrows=max_rows,ncols=max_columns,figsize=(3*max_columns,3*max_rows)) i=-1 ax_index= ((i+1) // graph_iteration ) // (max_columns), ((i+1) // graph_iteration ) % (max_columns) if (max_rows ==1): ax_index= ((i+1) // graph_iteration ) % (max_columns) show_preds(apply_step(params, prn=False), axs[ax_index]) axs[ax_index].set_title(&#39;iter 0&#39;) for i in range(max_iter): preds=apply_step(params, prn=False) if ((i+1) % graph_iteration == 0): ax_index= ((i+1) // graph_iteration ) // (max_columns), ((i+1) // graph_iteration ) % (max_columns) if (max_rows ==1): ax_index= ((i+1) // graph_iteration ) % (max_columns) show_preds(preds, axs[ax_index]) axs[ax_index].set_title(&#39;iter &#39;+str(i+1)) plt.tight_layout() . Animation . import . %matplotlib inline # fastai v1 backward compatibility import matplotlib.pyplot as plt import torch import torch.nn as nn import numpy as np def tensor(*argv): return torch.tensor(argv) # TEST assert torch.all(tensor(1,2) == torch.tensor([1,2])), &#39;Backward compatibility with fastai v1&#39; . function and plot . n=100 x = torch.ones(n,1) x.uniform_(-3.14,3.14) def my_function(x, a): return ((torch.cat((x**3, x**2, x, torch.ones(n,1) ), 1))@a).reshape((n)) a=tensor(4., 2., -12., 5.) y = my_function(x, a) a = tensor(-1.,-2., 6., -8) y_hat = my_function(x, a) plt.scatter(x[:,0], y) plt.scatter(x[:,0],y_hat); def mse(y_hat, y): return ((y_hat-y)**2).mean() . gradient descent . a = nn.Parameter(a); a def update(): y_hat = my_function(x, a) loss = mse(y, y_hat) if t % 10 == 0: print(loss) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() lr = 1e-3 for t in range(100): update() . tensor(1967.0251, grad_fn=&lt;MeanBackward0&gt;) tensor(559.2718, grad_fn=&lt;MeanBackward0&gt;) tensor(365.7207, grad_fn=&lt;MeanBackward0&gt;) tensor(282.6393, grad_fn=&lt;MeanBackward0&gt;) tensor(245.4054, grad_fn=&lt;MeanBackward0&gt;) tensor(227.3450, grad_fn=&lt;MeanBackward0&gt;) tensor(217.3324, grad_fn=&lt;MeanBackward0&gt;) tensor(210.7267, grad_fn=&lt;MeanBackward0&gt;) tensor(205.5912, grad_fn=&lt;MeanBackward0&gt;) tensor(201.1171, grad_fn=&lt;MeanBackward0&gt;) . animation . from matplotlib import animation, rc rc(&#39;animation&#39;, html=&#39;jshtml&#39;) a = nn.Parameter(tensor(-1.,1)) a=tensor(4., 2., -12., 5.) y = my_function(x, a) a = tensor(-1.,-2., 6., -8) y_hat = my_function(x, a) a = nn.Parameter(a); a fig = plt.figure() plt.scatter(x[:,0], y, c=&#39;orange&#39;) line = plt.scatter(x[:,0], y_hat.detach()) plt.close() def animate(i): line.set_offsets(np.c_[x[:,0], (my_function(x,a)).detach()]) update() return line, animation.FuncAnimation(fig, animate, np.arange(0, 300), interval=5) . &lt;/input&gt; Once Loop Reflect",
            "url": "https://castorfou.github.io/guillaume_blog/matplotlib/2020/09/26/Matplotlib-multiple-subplots-and-animations.html",
            "relUrl": "/matplotlib/2020/09/26/Matplotlib-multiple-subplots-and-animations.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Fastai book Deep Learning for Coders with fastai and Pytorch",
            "content": "Paper version of fastai book . Of course the 1st tep is to purchase this great book: . . I have liked what Jeremy Howard said about why this is important to purchase it (in video 1). Fastai is offering full access to the book as notebooks. So that we can run all codes from them. . Get notebook version of fastai book . ~/git/guillaume$ git clone https://github.com/fastai/fastbook.git . . I have now a perfect combo between paper book and notebooks. . Video courses based on fastai book . Rachel Thomas and Jeremy Howard have done some great videos about learning fastai (and pytorch). . They tend to do it every year. But this year is quite special due to fastai book. . Here are all the 1st 7 videos: https://course.fast.ai/videos/?lesson=1 . Fastai forums . This is the natural source of information and interactions with other students. . There is a category Part 1 (2020) which seems perfect: https://forums.fast.ai/c/part1-v4/46 . Personal git organization . ~/git/guillaume$ ll fastai/ &lt;-- from https://github.com/fastai/fastai fastai_experiments/ &lt;-- I will keep all experiments I will do here fastbook/ &lt;-- from https://github.com/fastai/fastbook.git guillaume_blog/ &lt;-- from git@github.com:castorfou/guillaume_blog.git . fastai_experiments likely to have 1 notebook per chapter or video. . update jupyter to include extensions (toc, ...) . conda install -c conda-forge jupyter_contrib_nbextensions . I like table of content, others are quite usefull as well (scratchpad, ExecuteTime...). . Install some libraries to run book examples . conda install -c fastai fastbook . It will install graphviz, nbdev, and other libraries. . And most of them can be loaded by calling from utils import * . Launch jupyter notebook and start expermenting . cd ~/git/guillaume conda activate fastai jupyter notebook . And launch several tabs at: blog entries, fastai experiments, fastai courses and fastai videos. . And I keep track of progress with git. .",
            "url": "https://castorfou.github.io/guillaume_blog/fastai/jupyter/fastbook/2020/09/24/fastai-book.html",
            "relUrl": "/fastai/jupyter/fastbook/2020/09/24/fastai-book.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Setup ubuntu box with fastai",
            "content": "Install miniconda . Get miniconda Linux installer. . Check sha256sum: sha256sum Miniconda3-latest-Linux-x86_64.sh . Run install: ./Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3 . Install fastai . conda create -n fastai python=3.8 conda activate fastai conda install -c fastai -c pytorch fastai . Install jupyter within fastai environment . conda activate fastai conda install jupyter . Test fastai installation (valid for v1) . With fastai v1, there was an easy way to check installation: . conda activate fastai python -m fastai.utils.show_install . get git repo to learn from fastai . From git folder, . git clone https://github.com/fastai/fastai . Test fastai v2 installation . From python environment: . from fastai.vision.all import * . From jupyter notebook . from fastai.vision.all import * . Install nvidia drivers for ubuntu . I tried by downloading a driver from nvidia website. But I was unable to install it (nvidia-drm-drv.c:662:44: error: &#39;DRIVER_PRIME&#39; undeclared here (not in a function); did you mean &#39;DRIVER_PCI_DMA&#39;?) . sudo ubuntu-drivers autoinstall . then rebooting fixed the issue. . Run courses from fastai github repo . just run fastai/dev_nbs/course/lesson1-pets.ipynb . And everything is just fined ;) . install nbdev . This is for rendering reasons: To get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev . !pip install nbdev . Collecting nbdev Downloading nbdev-1.0.18-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 1.5 MB/s eta 0:00:01 Requirement already satisfied: fastcore&gt;=1.0.5 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (1.0.13) Requirement already satisfied: packaging in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (20.4) Requirement already satisfied: jupyter-client in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (6.1.6) Requirement already satisfied: nbconvert&lt;6 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.6.1) Requirement already satisfied: nbformat&gt;=4.4.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.0.7) Collecting fastscript&gt;=1.0.0 Downloading fastscript-1.0.0-py3-none-any.whl (11 kB) Requirement already satisfied: pyyaml in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.3.1) Requirement already satisfied: pip in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (20.2.2) Requirement already satisfied: ipykernel in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.3.4) Requirement already satisfied: pyparsing&gt;=2.0.2 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;nbdev) (2.4.7) Requirement already satisfied: six in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;nbdev) (1.15.0) Requirement already satisfied: pyzmq&gt;=13 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (19.0.2) Requirement already satisfied: tornado&gt;=4.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (6.0.4) Requirement already satisfied: python-dateutil&gt;=2.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (2.8.1) Requirement already satisfied: traitlets in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (4.3.3) Requirement already satisfied: jupyter-core&gt;=4.6.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (4.6.3) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (1.4.2) Requirement already satisfied: pygments in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (2.7.1) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.8.4) Requirement already satisfied: defusedxml in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.6.0) Requirement already satisfied: bleach in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (3.2.1) Requirement already satisfied: jinja2&gt;=2.4 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (2.11.2) Requirement already satisfied: testpath in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.4.4) Requirement already satisfied: entrypoints&gt;=0.2.2 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.3) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbformat&gt;=4.4.0-&gt;nbdev) (3.0.2) Requirement already satisfied: ipython-genutils in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbformat&gt;=4.4.0-&gt;nbdev) (0.2.0) Requirement already satisfied: ipython&gt;=5.0.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipykernel-&gt;nbdev) (7.18.1) Requirement already satisfied: decorator in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from traitlets-&gt;jupyter-client-&gt;nbdev) (4.4.2) Requirement already satisfied: webencodings in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from bleach-&gt;nbconvert&lt;6-&gt;nbdev) (0.5.1) Requirement already satisfied: MarkupSafe&gt;=0.23 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jinja2&gt;=2.4-&gt;nbconvert&lt;6-&gt;nbdev) (1.1.1) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (0.17.3) Requirement already satisfied: attrs&gt;=17.4.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (20.2.0) Requirement already satisfied: setuptools in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (49.6.0.post20200814) Requirement already satisfied: backcall in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.2.0) Requirement already satisfied: pexpect&gt;4.3; sys_platform != &#34;win32&#34; in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (4.8.0) Requirement already satisfied: pickleshare in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.7.5) Requirement already satisfied: jedi&gt;=0.10 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.17.2) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (3.0.7) Requirement already satisfied: ptyprocess&gt;=0.5 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from pexpect&gt;4.3; sys_platform != &#34;win32&#34;-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.6.0) Requirement already satisfied: parso&lt;0.8.0,&gt;=0.7.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jedi&gt;=0.10-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.7.0) Requirement already satisfied: wcwidth in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.2.5) Installing collected packages: fastscript, nbdev Successfully installed fastscript-1.0.0 nbdev-1.0.18 .",
            "url": "https://castorfou.github.io/guillaume_blog/fastai/cuda/linux/2020/09/23/Setup-ubuntu-box-with-fastai.html",
            "relUrl": "/fastai/cuda/linux/2020/09/23/Setup-ubuntu-box-with-fastai.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Fastai on WSL 2 with Cuda",
            "content": "This is based on what is explained in https://forums.fast.ai/t/fastai-on-wsl-2-ubuntu-0-7-0-or-any-version/76651 . install update of nvidia drivers . Based on Deep Learning Course Forums Platform: Windows 10 using WSL2 w/GPU fastai users . create nvidia account | download quadro driver from https://developer.nvidia.com/cuda/wsl/download (460.15) | install | . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ nvidia-smi.exe Mon Sep 21 16:00:46 2020 +--+ | NVIDIA-SMI 460.15 Driver Version: 460.15 CUDA Version: 11.1 | |-+-+-+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Quadro M1000M WDDM | 00000000:01:00.0 On | N/A | | N/A 59C P0 N/A / N/A | 905MiB / 4096MiB | 0% Default | +-+-+-+ . install of WSL2 and convert existing images . Open a PowerShell window as an Administrator . Run wsl --set-default-version 2 . update KB . . --set-default-version 2 is not a valid option. KB4566116 should be installed . This can be downloaded from Catalog Microsoft Update . update kernel version . If you see this message after running the command: WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel. You still need to install the MSI Linux kernel update package. . Download from https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4download-the-linux-kernel-update-package . set default WSL to be version 2 . PS C: WINDOWS system32&gt; wsl --set-default-version 2 . convert existing images . PS C: WINDOWS system32&gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Running 1 PS C: WINDOWS system32&gt; wsl --set-version Ubuntu-18.04 2 La conversion est en cours. Cette opération peut prendre quelques minutes... Pour plus d’informations sur les différences de clés avec WSL 2, visitez https://aka.ms/wsl2 La conversion est terminée. . It took a while (~1 hour) for my unique ubuntu image. . And at the end it has worked. . PS C: WINDOWS system32&gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Stopped 2 . install of nvidia drivers under ubuntu . [Installation instructions)(https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal) . wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda . !cat /usr/local/cuda/version.txt . CUDA Version 11.0.228 . !/usr/local/cuda/samples/4_Finance/BlackScholes/BlackScholes . [/usr/local/cuda/samples/4_Finance/BlackScholes/BlackScholes] - Starting... CUDA error at ../../common/inc/helper_cuda.h:777 code=35(cudaErrorInsufficientDriver) &#34;cudaGetDeviceCount(&amp;device_count)&#34; . There is an error when launching CUDA samples. Googling that error maybe my video card is running on low driver version? . I have posted on nvidia (cuda+wsl) forum: https://forums.developer.nvidia.com/t/cuda-sample-throwing-error/142537/18 . (update 09-22: it is not possible to have cuda on wsl2 if not in Windows Insider build from Dev Channel. (20145 or higher)) . . because I am in version 1909 (18363.1049), it won&#39;t work for me. ;( . cuda for WSL . Here is a link that could be interesting: https://docs.nvidia.com/cuda/wsl-user-guide/index.html . According to this, I should not have installed cuda but cuda-toolkit. Do not choose the cuda, cuda-11-0, or cuda-drivers meta-packages under WSL 2 since these packages will result in an attempt to install the Linux NVIDIA driver under WSL 2. . Is it causing my issue? . apt-get install -y cuda-toolkit-11-0 . !/usr/local/cuda/bin/nvcc --version . nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Wed_Jul_22_19:09:09_PDT_2020 Cuda compilation tools, release 11.0, V11.0.221 Build cuda_11.0_bu.TC445_37.28845127_0 . install a new distro (ubuntu 20.04) . Because I cannot use windows store, I have to manually install https://docs.microsoft.com/fr-fr/windows/wsl/install-manual . Installation by just launching Ubuntu_2004.2020.424.0_x64.appx. . I have now 2 distros, . PS C: WINDOWS system32&gt; wsl --list -v NAME STATE VERSION * Ubuntu-18.04 Running 2 Ubuntu-20.04 Running 2 . WSL2 and network . There is a change of network architecture between WSL 1 and WSL 2. In WSL 2, a new network interface is available: . Carte Ethernet vEthernet (WSL) : Suffixe DNS propre à la connexion. . . : Adresse IPv6 de liaison locale. . . . .: Adresse IPv4. . . . . . . . . . . . . .: 192.168.81.193 Masque de sous-réseau. . . . . . . . . : 255.255.255.240 Passerelle par défaut. . . . . . . . . : . Revert image to WSL1 to get back network access . PS C: WINDOWS system32&gt; wsl --set-version Ubuntu-18.04 1 La conversion est en cours. Cette opération peut prendre quelques minutes... La conversion est terminée. . PS C: WINDOWS system32&gt; wsl --list -v NAME STATE VERSION * Ubuntu-18.04 Stopped 1 Ubuntu-20.04 Stopped 2 . access to linux files from windows . For running state distros: . Files are available at wsl$ . . For stopped state distros: . Files are available at C: Users &lt;users&gt; AppData Local Packages CanonicalGroupLimited.Ubuntu* LocalState rootfs .",
            "url": "https://castorfou.github.io/guillaume_blog/wsl/fastai/wsl2/cuda/2020/09/21/Windows10-fastai-wsl2-cuda.html",
            "relUrl": "/wsl/fastai/wsl2/cuda/2020/09/21/Windows10-fastai-wsl2-cuda.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Autodetect Home / Office network + Proxy",
            "content": "IP detection . Command to get IP address is as follow: . IP=`ifconfig | grep &#39;inet &#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;{ print $2}&#39;` . I can then check how IP is setup: . empty: no network attached, in that case nothing to do | HOME_IP=192.168.1.241: based on MAC I give fixed IP to my computers (out of DHCP scope) | S8_IP=192.168.: hotspot from samsung is using 192.168. addresses | OFFICE_IP=10.: office network uses 10. addresses | . Detect if variable IP is set: . if [ -z &quot;$IP&quot; ]; then echo &quot;Not connected to any network&quot; fi . Network detection and proxy settings . Depending on my network, I have to set or unset proxy. . Here is the 1st version: . (xgboost) guillaume@LL11LPC0PQARQ:~$ cat my_ip.sh #!/bin/bash IP=`ifconfig | grep &#39;inet &#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;{ print $2}&#39;` HOME_IP=192.168.1.241 OFFICE_IP=10. S8_IP=192.168. # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY } if [ -z &quot;$IP&quot; ]; then echo &quot;Not connected to any network&quot; else echo &quot;Connected and IP address is: $IP&quot; if [[ &quot;$IP&quot; == &quot;$HOME_IP&quot; ]]; then echo &quot;Connected at home from freebox pop --&gt; no proxy&quot; unsetproxy else if [[ &quot;$IP&quot; == &quot;$S8_IP&quot;* ]]; then echo &quot;Connected with mobile phone --&gt; no proxy&quot; unsetproxy fi if [[ &quot;$IP&quot; == &quot;$OFFICE_IP&quot;* ]]; then echo &quot;Connected from Office --&gt; proxy&quot; setproxy fi fi fi . Call this script: source . If I want these environment variables to be available from parent shell, I have to call my script with source. . (xgboost) guillaume@LL11LPC0PQARQ:~$ source my_ip.sh Connected and IP address is: 10.xxx.xxx.xxx 192.168.1.241 Connected from Office --&gt; proxy Calling setproxy . And I will auto launch this script each time I open a terminal by adding source my_ip.sh at the end of .bashrc . git and keep dot configuration files: config . Another great practice from Jeremy Howard: From https://developer.atlassian.com/blog/2016/02/best-way-to-store-dotfiles-git-bare-repo/ and https://www.atlassian.com/git/tutorials/dotfiles . I will create a blog entry about that later. . config add .bashrc my_ip.sh config commit -m &#39;detect network and set proxy&#39; config push . wget: proxy / no proxy . I store proxy conf files under ~/proxy_files/ . For wget: 2 files . $ cat proxy_files/.wgetrc_noproxy use_proxy=no $ cat proxy_files/.wgetrc_proxy use_proxy=yes http_proxy=proxy_ip:80 https_proxy=proxy_ip:80 . And enabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc . Disabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc . So the updated functions setproxy and unsetproxy are: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc } . apt-get: proxy / no proxy . I store proxy conf files under ~/proxy_files/ . For apt, 1 file . $ cat proxy_files/apt_proxy.conf Acquire { HTTP::proxy &quot;http://proxy_ip:80&quot;; HTTPS::proxy &quot;http://proxy_ip:80&quot;; } . And enabling proxy for apt: sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf . Disabling proxy for wget: sudo rm -f /etc/apt/apt.conf.d/proxy.conf . . Refactor to avoid password request each time it is launched . for the moment I have just commented out these lines . So the updated functions setproxy and unsetproxy are: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf } . Sept-21 2020: IP detection to be changed after WSL2 . With WSL2, IP address is from 172 network. . This looks like a virtual internal address. More detail at that address: https://github.com/microsoft/WSL/issues/4150. . . to update IP detection . Oct-21 2020: Use git with github (ssh) behind corporate proxy . Here is the new configuration explained in my blog entry Use git with github (ssh) behind corporate proxy . It is just a matter of linking appropriate files when I am in or out of corporate network. . As in my_ip.sh: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf #proxy for conda ln -sf ~/proxy_files/.condarc_proxy ~/.condarc #proxy for git git config --global http.proxy http://proxy_ip:80 ln -sf ~/proxy_files/ssh_config_proxy ~/.ssh/config } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf #no proxy for conda ln -sf ~/proxy_files/.condarc_noproxy ~/.condarc #no proxy for git git config --global --unset http.proxy ln -sf ~/proxy_files/ssh_config_noproxy ~/.ssh/config } .",
            "url": "https://castorfou.github.io/guillaume_blog/wsl/linux/bash/2020/09/15/autodetect-home-office-network-and-proxy-settings.html",
            "relUrl": "/wsl/linux/bash/2020/09/15/autodetect-home-office-network-and-proxy-settings.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Fast read Excel files with pandas",
            "content": "Problem description . initial settings . measure_time decorator . from functools import wraps from time import time def measure_time(func): @wraps(func) def _time_it(*args, **kwargs): start = int(round(time() * 1000)) try: return func(*args, **kwargs) finally: end_ = int(round(time() * 1000)) - start print(f&quot;Total execution time: {end_ if end_ &gt; 0 else 0} ms&quot;) return _time_it . read big excel file with pandas . big_excel_file = root_data+&#39;/pandas-caching/big_excel_file.xlsx&#39; . @measure_time def load_excel(file): dataframe = pd.read_excel(file) return dataframe . dataframe = load_excel(big_excel_file) . Total execution time: 36196 ms . . Waouh, 36 sec to read this file! . read converted csv file (turned to csv from excel using excel) . csv_file = root_data+&#39;/pandas-caching/big_csv_file_turned_from_excel.csv&#39; . @measure_time def load_csv(file): dataframe = pd.read_csv(file, sep=&#39;;&#39;, decimal=&#39;,&#39;) return dataframe . df_csv = load_csv(csv_file) . Total execution time: 836 ms . . Much better, 0.8 sec! . Caching library . import os def read_CachedXLS(filename, forceReload = False, **options): &quot;&quot;&quot; Part d&#39;un fichier excel natif (filename). Si le dataframe caché correspondant n&#39;existe pas encore, alors sauve le dataframe caché au format csv dans le rep source. (s&#39;il existe et si forceReload==True, alors écrase le dataframe caché existant par une nouvelle version) Lit le dataframe caché correspondant avec les **options et retourne le dataframe. Examples -- &gt;&gt;&gt; filename = &#39;/mnt/z/data/Stam-CC/ExportData 25625.xlsx&#39; forceReload = False option={&#39;dayfirst&#39;:True, &#39;parse_dates&#39;:[&#39;Fecha de Medida&#39;, &#39;Fecha de Fabricacion&#39;], &#39;sheetname&#39;:0} getCachedXLSRaw(filename, forceRelead, **option).info() Parameters - filename : string Emplacement du fichier XLS. Avec l&#39;extension. Format complet Ex: &#39;/mnt/z/data/Stam-CC/ExportData 25625.xlsx&#39; forceReload : boolean, optional, default value = False Si forceReload == True, le fichier sera relu et sauvé même s&#39;il existe déjà en cache options : **keyword args, optional Arguments de lecture du fichier XLS : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html Ex: sheetname=1 Returns - dataframe Dataframe correspondant &quot;&quot;&quot; #split pour ne garder que le nom sans le chemin de filename : Stam-CC/ExportData 25625 --&gt; ExportData 25625 dataframe_filename = os.path.dirname(filename)+&#39;/&#39;+os.path.basename(filename)+&#39;.csv&#39; #bug de pandas.to_csv quand il y a des espaces ? dataframe_filename = dataframe_filename.replace(&quot; &quot;, &quot;_&quot;) dataframe=[] xls_toget = False #print(dataframe_filename) if (forceReload and os.path.exists(dataframe_filename)): print(&quot;Cached file &quot;+dataframe_filename+&quot; déjà existant mais forceReload=True - FORCE RELOAD&quot;) xls_toget = True if (not os.path.exists(dataframe_filename)): print(&quot;Cached file &quot;+dataframe_filename+&quot; inexistant - read_CachedXLS&quot;) xls_toget = True if (xls_toget): dataframe = pd.read_excel(filename, **options) dataframe.to_csv(dataframe_filename) else: print(&quot;Cached file &quot;+dataframe_filename+&quot; existe en cache, relecture&quot;) #index_col pour ignorer les n° de lignes excel options[&#39;sep&#39;]=&#39;,&#39; options[&#39;decimal&#39;]=&#39;.&#39; options[&#39;skiprows&#39;]=0 options.pop(&#39;sheet_name&#39;) dataframe = pd.read_csv(dataframe_filename,**options) return dataframe . option={&#39;sheet_name&#39;:0} read_CachedXLS(big_excel_file, **option) print(&quot;et voila&quot;) . Cached file /mnt/z/data//pandas-caching/big_excel_file.xlsx.csv existe en cache, relecture et voila .",
            "url": "https://castorfou.github.io/guillaume_blog/pandas/2020/09/14/fast-read-excel-pandas.html",
            "relUrl": "/pandas/2020/09/14/fast-read-excel-pandas.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Git push to github without password",
            "content": "By default everytime I push to github, I have a prompt asking for password. . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Everything up-to-date . Would be great if I could leverage ssh keys to authenticate. . Update remote from https to ssh . From https://stackoverflow.com/questions/14762034/push-to-github-without-a-password-using-ssh-key, . For example, a GitHub project like Git will have an HTTPS URL: https://github.com/&lt;Username&gt;/&lt;Project&gt;.git And the SSH one: git@github.com:&lt;Username&gt;/&lt;Project&gt;.git You can do: git remote set-url origin git@github.com:&lt;Username&gt;/&lt;Project&gt;.git to change the URL. . In my case I have . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git remote -v origin https://github.com/castorfou/guillaume_blog.git (fetch) origin https://github.com/castorfou/guillaume_blog.git (push) . I have just to modify: . git remote set-url origin git@github.com:castorfou/guillaume_blog.git . Results . It looks like working: . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push Counting objects: 4, done. Delta compression using up to 8 threads. Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 1.59 KiB | 812.00 KiB/s, done. Total 4 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:castorfou/guillaume_blog.git 4013108..ae78b99 master -&gt; master . Drawback: doesn&#39;t work behing a firewall . . To find a solution to use a proxy . Here are 2 ways to be tested: https://stackoverflow.com/questions/1728934/accessing-a-git-repository-via-ssh-behind-a-firewall https://stackoverflow.com/questions/18604719/how-to-configure-git-to-clone-repo-from-github-behind-a-proxy-server?noredirect=1&amp;lq=1 .",
            "url": "https://castorfou.github.io/guillaume_blog/git/2020/09/11/git-commit-without-password.html",
            "relUrl": "/git/2020/09/11/git-commit-without-password.html",
            "date": " • Sep 11, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Blog from jupyter notebook",
            "content": "That will be great if I can simply write blog entries using Jupyter Notebook. . I usually paste inner images into jupyter cells. But this feature is not available yet into fastpages. So for the moment I won&#39;t include images into these posts. . That way I could simply use markdown and insert images . And directly see rendered impact before commiting and pushing to my blog. . get local repo from github . As I am behind a proxy most of my time when working from office, the easiest way for me is to work from WSL. . WSL . I won&#39;t detail how to install WSL on Windows. . I use ubuntu images (18.04) on my PC. . set unset proxy in WSL . I have just added some bash commands at the end of my .bashrc file. . # Set Proxy function setproxy() { export {http,https,ftp}_proxy=&quot;http://&lt;my proxy ip address&gt;:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://&lt;my proxy ip address&gt;:80&quot; } # Unset Proxy function unsetproxy() { unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY } . git clone castorfou.github.io . I keep most of my local repos under ~/git/ . cd ~/git setproxy git clone https://github.com/castorfou/castorfou.github.io.git . create a blog entry with Jupyter Notebook . commit and push to github . (base) guillaume@LL11LPC0PQARQ:~$ cd git (base) guillaume@LL11LPC0PQARQ:~/git$ cd castorfou.github.io/ (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git add . (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git commit -m &#39;new blog entry: blog from jupyter&#39; [master 6b7460a] new blog entry: blog from jupyter 4 files changed, 516 insertions(+) create mode 100644 _posts/.ipynb_checkpoints/2020-09-10-blog-from-jupyter-checkpoint.ipynb create mode 100644 _posts/2020-09-10-blog-from-jupyter.ipynb create mode 100644 _posts/2020-09-10-blog-from-jupyter.py create mode 100644 _posts/Untitled.txt (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push fatal: unable to access &#39;https://github.com/castorfou/castorfou.github.io.git/&#39;: gnutls_handshake() failed: The TLS connection was non-properly terminated. . error: gnutls_handshake() failed: The TLS connection was non-properly terminated. . Just googling this error gives some insight: https://github.community/t/unable-to-push-to-repo-gnutls-handshake-failed/885 . It is likely some local firewell issue. . . To be fixed later . switch to mobile wifi without need of proxy . (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ unsetproxy (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Counting objects: 7, done. Delta compression using up to 8 threads. Compressing objects: 100% (6/6), done. Writing objects: 100% (7/7), 141.26 KiB | 10.09 MiB/s, done. Total 7 (delta 1), reused 0 (delta 0) remote: Resolving deltas: 100% (1/1), completed with 1 local object. remote: remote: GitHub found 3 vulnerabilities on castorfou/castorfou.github.io&#39;s default branch (2 high, 1 moderate). To find out more, visit: remote: https://github.com/castorfou/castorfou.github.io/network/alerts remote: To https://github.com/castorfou/castorfou.github.io.git 6adeb02..6b7460a master -&gt; master . check entries into blog . double entries . Double entries: one for the notebook (.ipynb) and one for the auto python export (.py). I will have to update my jupyter settings to avoid this python file creation. In the meantime I can just delete the python file, and commit. . . Change settings of jupyter + .gitignore to avoid these double entries . cannot open notebook into browser . Clicking just ask me to download the notebook, it doesn&#39;t display it into the browser. . checking .gitignore . Just by looking into .gitignore, there is an interesting entry: . *.swp ~* *~ _site .sass-cache .jekyll-cache .jekyll-metadata vendor _notebooks/.ipynb_checkpoints . Wait what is in this last line. . Let&#39;s create _notebooks directory and move my notebook in that directory. . notebooks from _notebooks not rendered . No entries, I guess there is some additional settings to do... . . Why notebooks are not rendered by Jekyl . test entry from md using local repo . There is no problem with that. . Creating a local md file in _poststhen pushing to github is creating the right entry blog. . following fastpages troubleshooting guide . upgrade fastpages . Try the automated upgrade as described in https://github.com/fastai/fastpages/blob/master/_fastpages_docs/UPGRADE.md . Unfortunately I don&#39;t see . I have to follow the manual upgrade. . manual fastpages upgrade . I am surprised because the 1st step from manual upgrade is to copy the fastpages repo. It is what I did 2 days ago. I doubt having an outdated version of fastpages. . fastai forum: fastpages category . I will browse through nbdev &amp; faspages category in fastai forums. I should see people with the same issue. . I have created an entry, into fastai forums: Fastpages - cannot see build process of GitHub Actions . And quite immediately Hamel Hussain answered guiding to the write direction: . I misread the Settings instruction: my github repo should explicitely NOT include my github username and I did exactly the opposite. . . I have to create a new repo: guillaume_blog . nothing visible from Actions tab . And another surprising subject: at github in Actions tab. I have a kind of default page. I expect something like an execution journal of Actions. . Page build failure . Received a notification by email: . The page build failed for the master branch with the following error: . Page build failed. For more information, see https://docs.github.com/github/working-with-github-pages/troubleshooting-jekyll-build-errors-for-github-pages-sites#troubleshooting-build-errors. . For information on troubleshooting Jekyll see: . https://docs.github.com/articles/troubleshooting-jekyll-builds . If you have any questions you can submit a request on the Contact GitHub page at https://support.github.com/contact?repo_id=293820308&amp;page_build_id=202240535 . Move to another repo . repo creation . It was just a matter of creating a new repo: . actions monitoring . Monitoring is effective . merge pull request . actions around ssh keys . Following the steps: . Create keys using ssh utility | Enter Secret Key | Enter Deploy Key | . merge PR . There are conflicts to be fixed before that. . And it works: https://castorfou.github.io/guillaume_blog/ . Get local repo . cd ~/git unsetproxy git clone https://github.com/castorfou/guillaume_blog.git .",
            "url": "https://castorfou.github.io/guillaume_blog/fastpages/jupyter/notebooks/2020/09/10/blog-from-jupyter-sans-images.html",
            "relUrl": "/fastpages/jupyter/notebooks/2020/09/10/blog-from-jupyter-sans-images.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Blogging from github",
            "content": "Blogging from github . fastai and fastpages . I am a big fan of fastai’s spirit and even more of their leaders: . Jeremy Howards | Rachel Thomas | Sylvain Gugger who is know at huggingface. | . They are commited to beautiful ideas, and are inspiring people. I like their courses. I like their softwares. For sure I will discuss about fastai. They have created fastpages. It turns github into a blogging platform. I don’t have the full detail but it is explained in fastpages github repo It is based on github actions, and by just creating a repo from a fastpages template https://github.com/fastai/fast_template/generate and giving a couple of settings, you are ready to go. . And here I have to thank Hamel Husain. He is from github company and I think he is behing github actions and helped fastai to release fastpages. I don’t know Hamel but he looks like a humble, terribly skilled guy, with tons of energy. Thanks Hamel. . my blog . My main audience is the future me. (maybe not entirely true otherwise I would have written in French) In 1 year, I want to turn back to this blog and I would like to see all the learning peaces I went through. I want this platform to be as easy as possible. . fastpages . For the moment it cannot be easier. I have setup the about page. And each blog entry is just a new markdown page into _posts. github _posts: . . By commiting this page, there are internal actions being run automatically (through github actions magic) and after a couple of minutes the new blog pages are generated (using Jekyl and ruby if I am not wrong). For the moment I use github web interface. But I guess it is easier to have a local repo of my blog, create new entries and when satisfied git push to github. (to be tested later) . github accounts . For a reason I used my personal github account (guillaume.ramelet@gmail.com) and not my professional one (guillaume.ramelet@michelin.com). I will see later if I have to move to another account. I had some troubles to setup actions into github. For a reason I thought it was available only for organization account. So I have turned my michelin github account to an organization, and I cannot login anymore. To be fixed later. . markdown . Ok I am not a huge fan of markdown. I use it as a basic text system specially within notebooks. But it is not as easy to insert images. Currently I screenshot what I want to share, insert into images folder of my repo and reference this image from my blog post using markdown language. I definitely have to improve my practice of markdown, and there are multiple cheatsheets to be used. . jupyter . There are options within fastpages to blog from jupyter notebooks. I have to do it. My intent will be to use this place to share my knowledge. Today most of my knowledge comes from experiences I make within jupyter. If I could directly blog from that it will be great. . comments . OK as the sole reader this is maybe a minor concern but there is no commenting system associated with fastpages. I cannot get any feedback from these entries. Would love to get advices, create discussions within that blog. Not for today. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/fastpages/git/2020/09/09/blogging-from-github.html",
            "relUrl": "/blog/fastpages/git/2020/09/09/blogging-from-github.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Becoming a datascientist",
            "content": "March 2019 . This journey has started about 1 year ago. . No wait, that was dormant for a long time before that. I guess I have to go back to my studying time: at that time my days were full of maths and computers. And my days were flying as crazy. It happens to me (to you?) when you’re just in a middle of something you like very much. 10 hours looks like 1. And the opposite is true as well. . 2000 - 2004: software development . Most of my days and weekends at that time were dedicated to code in Java and bash. Java mainly for server-side developpement in J2EE at Unilog Management. Bash from time to time to automate some tasks on my personal PC. At that time it was mainly about learning what is an operating system. I had started with LFS (Linux From Scratch). And in 2002 with Gentoo which was a much more powerful way to mimic LFS. . 2004 - 2009: project management . Strange period. I don’t remember exactly why but I had a shift in my professional orientation. I moved away from software development and turned into a project manager. In 2005 I entered into Michelin company. And sofwtare technical matters at that time were considered as unimportant (and embarrassing subjects) Fortunately in 2009 I have been started my agile journey. A lot to learn, and it was less about software than human relations and empathy. It was like a start from scratch. . 2009 - 2015: agile journey . Quite a new world for me. I had some basic knowledge by following Jono Bacon. At that time he was a community manager|release leader at Ubuntu. And was reporting progress using burndown charts. In 2009 I launched a project to create an employee portal (closed to what netvibes and igoogle were at that time). Using standard java portal technologies and more importantly using agile approach. A lot to learn about Agile, Scrum, and endless discussions about how to introduce Agile into a non-Agile organization. In 2010 I started another more ambitious project, with many colleagues (~30 persons) and a vague vision. It was about to create a product lifecycle management solution for semi-finished products. . 2015 - 2017: lean journey . In 2015, I met lean approaches for office. I was immediately convinced there was powerful and deep roots within lean. And it could bring a lot to people and organizations. I turned into a lean coach, to work with teams identifying what they could improve, how they could work better, with more pleasure. . 2017 - 2019: Welcome to USA! . Nice opportunity at that time to move from Clermont-Ferrand (France) to Greenville, South Carolina (USA). I have loved every part of it. Except maybe that 2 years were too short to make a full tour of this amazing country. It is crazy to think how different we are when we look like the same. . 2019 - : back to France and turning as a datascientist . Sept 2019 - back to France and for 4 months to prepare for a complete new position: datascientist for Manufacturing within Michelin. I spent many days to learn from various sources specially datacamp and Andrew Ng. That was just the beginning. My intent was to move away from project management, team leadership and focus about what I can do by myself. I wanted to return to math domains without giving up an IT landscape. My colleague Francois Deheeger told me about data science and Artificial Intelligence. That looked as interesting as terrifying. I was in. I was not afraid to learn a new language, and to restart my career from scratch. .",
            "url": "https://castorfou.github.io/guillaume_blog/me/2020/09/08/becoming-datascientist.html",
            "relUrl": "/me/2020/09/08/becoming-datascientist.html",
            "date": " • Sep 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About . This blog . This is more a journal where I am adding entries about my (baby steps) learnings. It is likely to be centered around python, git, data-science, … I have been strongly inspired by Rachel Thomas explaining why I should blog. Specially when starting such a journey to turn a datascientist. . My intent would be to regularly add entries to this blog. Ideally at least once a week. Maybe only short ones, the point being to stick on this frequent activity. If it takes days to write posts I am pretty sure I won’t do it. Those entries are personnal thoughts and not those of my employer Michelin. . Me . I am 44 (in 2020). Father of 3. Working for a French tire company. I am French and for sure English is not my mother tongue. .",
          "url": "https://castorfou.github.io/guillaume_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://castorfou.github.io/guillaume_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}