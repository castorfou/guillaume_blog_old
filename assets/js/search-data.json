{
  
    
        "post0": {
            "title": "Fastai on WSL 2 with Cuda",
            "content": "This is based on what is explained in https://forums.fast.ai/t/fastai-on-wsl-2-ubuntu-0-7-0-or-any-version/76651 . install update of nvidia drivers . Based on Deep Learning Course Forums Platform: Windows 10 using WSL2 w/GPU fastai users . create nvidia account | download quadro driver from https://developer.nvidia.com/cuda/wsl/download (460.15) | install | . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ nvidia-smi.exe Mon Sep 21 16:00:46 2020 +--+ | NVIDIA-SMI 460.15 Driver Version: 460.15 CUDA Version: 11.1 | |-+-+-+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Quadro M1000M WDDM | 00000000:01:00.0 On | N/A | | N/A 59C P0 N/A / N/A | 905MiB / 4096MiB | 0% Default | +-+-+-+ . install of WSL2 and convert existing images . Open a PowerShell window as an Administrator . Run wsl --set-default-version 2 . update KB . . --set-default-version 2 is not a valid option. KB4566116 should be installed . This can be downloaded from Catalog Microsoft Update . update kernel version . If you see this message after running the command: WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel. You still need to install the MSI Linux kernel update package. . Download from https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4download-the-linux-kernel-update-package . set default WSL to be version 2 . PS C: WINDOWS system32&gt; wsl --set-default-version 2 . convert existing images . PS C: WINDOWS system32&gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Running 1 PS C: WINDOWS system32&gt; wsl --set-version Ubuntu-18.04 2 La conversion est en cours. Cette opération peut prendre quelques minutes... Pour plus d’informations sur les différences de clés avec WSL 2, visitez https://aka.ms/wsl2 La conversion est terminée. . It took a while (~1 hour) for my unique ubuntu image. . And at the end it has worked. . PS C: WINDOWS system32&gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Stopped 2 . install of nvidia drivers under ubuntu . [Installation instructions)(https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal) . wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda . !cat /usr/local/cuda/version.txt . CUDA Version 11.0.228 . import torch . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-4-eb42ca6e4af3&gt; in &lt;module&gt; -&gt; 1 import torch ModuleNotFoundError: No module named &#39;torch&#39; .",
            "url": "https://castorfou.github.io/guillaume_blog/wsl/fastai/wsl2/cuda/2020/09/21/Windows10-fastai-wsl2-cuda.html",
            "relUrl": "/wsl/fastai/wsl2/cuda/2020/09/21/Windows10-fastai-wsl2-cuda.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Autodetect Home / Office network + Proxy",
            "content": "IP detection . Command to get IP address is as follow: . IP=`ifconfig | grep &#39;inet &#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;{ print $2}&#39;` . I can then check how IP is setup: . empty: no network attached, in that case nothing to do | HOME_IP=192.168.1.241: based on MAC I give fixed IP to my computers (out of DHCP scope) | S8_IP=192.168.: hotspot from samsung is using 192.168. addresses | OFFICE_IP=10.: office network uses 10. addresses | . Detect if variable IP is set: . if [ -z &quot;$IP&quot; ]; then echo &quot;Not connected to any network&quot; fi . Network detection and proxy settings . Depending on my network, I have to set or unset proxy. . Here is the 1st version: . (xgboost) guillaume@LL11LPC0PQARQ:~$ cat my_ip.sh #!/bin/bash IP=`ifconfig | grep &#39;inet &#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;{ print $2}&#39;` HOME_IP=192.168.1.241 OFFICE_IP=10. S8_IP=192.168. # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY } if [ -z &quot;$IP&quot; ]; then echo &quot;Not connected to any network&quot; else echo &quot;Connected and IP address is: $IP&quot; if [[ &quot;$IP&quot; == &quot;$HOME_IP&quot; ]]; then echo &quot;Connected at home from freebox pop --&gt; no proxy&quot; unsetproxy else if [[ &quot;$IP&quot; == &quot;$S8_IP&quot;* ]]; then echo &quot;Connected with mobile phone --&gt; no proxy&quot; unsetproxy fi if [[ &quot;$IP&quot; == &quot;$OFFICE_IP&quot;* ]]; then echo &quot;Connected from Office --&gt; proxy&quot; setproxy fi fi fi . Call this script: source . If I want these environment variables to be available from parent shell, I have to call my script with source. . (xgboost) guillaume@LL11LPC0PQARQ:~$ source my_ip.sh Connected and IP address is: 10.xxx.xxx.xxx 192.168.1.241 Connected from Office --&gt; proxy Calling setproxy . And I will auto launch this script each time I open a terminal by adding source my_ip.sh at the end of .bashrc . git and keep dot configuration files: config . Another great practice from Jeremy Howard: From https://developer.atlassian.com/blog/2016/02/best-way-to-store-dotfiles-git-bare-repo/ and https://www.atlassian.com/git/tutorials/dotfiles . I will create a blog entry about that later. . config add .bashrc my_ip.sh config commit -m &#39;detect network and set proxy&#39; config push . wget: proxy / no proxy . I store proxy conf files under ~/proxy_files/ . For wget: 2 files . $ cat proxy_files/.wgetrc_noproxy use_proxy=no $ cat proxy_files/.wgetrc_proxy use_proxy=yes http_proxy=proxy_ip:80 https_proxy=proxy_ip:80 . And enabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc . Disabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc . So the updated functions setproxy and unsetproxy are: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc } . apt-get: proxy / no proxy . I store proxy conf files under ~/proxy_files/ . For apt, 1 file . $ cat proxy_files/apt_proxy.conf Acquire { HTTP::proxy &quot;http://proxy_ip:80&quot;; HTTPS::proxy &quot;http://proxy_ip:80&quot;; } . And enabling proxy for apt: sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf . Disabling proxy for wget: sudo rm -f /etc/apt/apt.conf.d/proxy.conf . . Refactor to avoid password request each time it is launched . So the updated functions setproxy and unsetproxy are: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt sudo rm -f /etc/apt/apt.conf.d/proxy.conf } . Sept-21 2020: IP detection to be changed after WSL2 . With WSL2, IP address is from 172 network. . This looks like a virtual internal address. More detail at that address: https://github.com/microsoft/WSL/issues/4150. . . to update IP detection .",
            "url": "https://castorfou.github.io/guillaume_blog/wsl/linux/bash/2020/09/15/autodetect-home-office-network-and-proxy-settings.html",
            "relUrl": "/wsl/linux/bash/2020/09/15/autodetect-home-office-network-and-proxy-settings.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fast read Excel files with pandas",
            "content": "Problem description . initial settings . measure_time decorator . from functools import wraps from time import time def measure_time(func): @wraps(func) def _time_it(*args, **kwargs): start = int(round(time() * 1000)) try: return func(*args, **kwargs) finally: end_ = int(round(time() * 1000)) - start print(f&quot;Total execution time: {end_ if end_ &gt; 0 else 0} ms&quot;) return _time_it . read big excel file with pandas . big_excel_file = root_data+&#39;/pandas-caching/big_excel_file.xlsx&#39; . @measure_time def load_excel(file): dataframe = pd.read_excel(file) return dataframe . dataframe = load_excel(big_excel_file) . Total execution time: 36196 ms . . Waouh, 36 sec to read this file! . read converted csv file (turned to csv from excel using excel) . csv_file = root_data+&#39;/pandas-caching/big_csv_file_turned_from_excel.csv&#39; . @measure_time def load_csv(file): dataframe = pd.read_csv(file, sep=&#39;;&#39;, decimal=&#39;,&#39;) return dataframe . df_csv = load_csv(csv_file) . Total execution time: 836 ms . . Much better, 0.8 sec! . Caching library . import os def read_CachedXLS(filename, forceReload = False, **options): &quot;&quot;&quot; Part d&#39;un fichier excel natif (filename). Si le dataframe caché correspondant n&#39;existe pas encore, alors sauve le dataframe caché au format csv dans le rep source. (s&#39;il existe et si forceReload==True, alors écrase le dataframe caché existant par une nouvelle version) Lit le dataframe caché correspondant avec les **options et retourne le dataframe. Examples -- &gt;&gt;&gt; filename = &#39;/mnt/z/data/Stam-CC/ExportData 25625.xlsx&#39; forceReload = False option={&#39;dayfirst&#39;:True, &#39;parse_dates&#39;:[&#39;Fecha de Medida&#39;, &#39;Fecha de Fabricacion&#39;], &#39;sheetname&#39;:0} getCachedXLSRaw(filename, forceRelead, **option).info() Parameters - filename : string Emplacement du fichier XLS. Avec l&#39;extension. Format complet Ex: &#39;/mnt/z/data/Stam-CC/ExportData 25625.xlsx&#39; forceReload : boolean, optional, default value = False Si forceReload == True, le fichier sera relu et sauvé même s&#39;il existe déjà en cache options : **keyword args, optional Arguments de lecture du fichier XLS : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html Ex: sheetname=1 Returns - dataframe Dataframe correspondant &quot;&quot;&quot; #split pour ne garder que le nom sans le chemin de filename : Stam-CC/ExportData 25625 --&gt; ExportData 25625 dataframe_filename = os.path.dirname(filename)+&#39;/&#39;+os.path.basename(filename)+&#39;.csv&#39; #bug de pandas.to_csv quand il y a des espaces ? dataframe_filename = dataframe_filename.replace(&quot; &quot;, &quot;_&quot;) dataframe=[] xls_toget = False #print(dataframe_filename) if (forceReload and os.path.exists(dataframe_filename)): print(&quot;Cached file &quot;+dataframe_filename+&quot; déjà existant mais forceReload=True - FORCE RELOAD&quot;) xls_toget = True if (not os.path.exists(dataframe_filename)): print(&quot;Cached file &quot;+dataframe_filename+&quot; inexistant - read_CachedXLS&quot;) xls_toget = True if (xls_toget): dataframe = pd.read_excel(filename, **options) dataframe.to_csv(dataframe_filename) else: print(&quot;Cached file &quot;+dataframe_filename+&quot; existe en cache, relecture&quot;) #index_col pour ignorer les n° de lignes excel options[&#39;sep&#39;]=&#39;,&#39; options[&#39;decimal&#39;]=&#39;.&#39; options[&#39;skiprows&#39;]=0 options.pop(&#39;sheet_name&#39;) dataframe = pd.read_csv(dataframe_filename,**options) return dataframe . option={&#39;sheet_name&#39;:0} read_CachedXLS(big_excel_file, **option) print(&quot;et voila&quot;) . Cached file /mnt/z/data//pandas-caching/big_excel_file.xlsx.csv existe en cache, relecture et voila .",
            "url": "https://castorfou.github.io/guillaume_blog/pandas/2020/09/14/fast-read-excel-pandas.html",
            "relUrl": "/pandas/2020/09/14/fast-read-excel-pandas.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Git push to github without password",
            "content": "By default everytime I push to github, I have a prompt asking for password. . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Everything up-to-date . Would be great if I could leverage ssh keys to authenticate. . Update remote from https to ssh . From https://stackoverflow.com/questions/14762034/push-to-github-without-a-password-using-ssh-key, . For example, a GitHub project like Git will have an HTTPS URL: https://github.com/&lt;Username&gt;/&lt;Project&gt;.git And the SSH one: git@github.com:&lt;Username&gt;/&lt;Project&gt;.git You can do: git remote set-url origin git@github.com:&lt;Username&gt;/&lt;Project&gt;.git to change the URL. . In my case I have . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git remote -v origin https://github.com/castorfou/guillaume_blog.git (fetch) origin https://github.com/castorfou/guillaume_blog.git (push) . I have just to modify: . git remote set-url origin git@github.com:castorfou/guillaume_blog.git . Results . It looks like working: . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push Counting objects: 4, done. Delta compression using up to 8 threads. Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 1.59 KiB | 812.00 KiB/s, done. Total 4 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:castorfou/guillaume_blog.git 4013108..ae78b99 master -&gt; master . Drawback: doesn&#39;t work behing a firewall . . To find a solution to use a proxy . Here are 2 ways to be tested: https://stackoverflow.com/questions/1728934/accessing-a-git-repository-via-ssh-behind-a-firewall https://stackoverflow.com/questions/18604719/how-to-configure-git-to-clone-repo-from-github-behind-a-proxy-server?noredirect=1&amp;lq=1 .",
            "url": "https://castorfou.github.io/guillaume_blog/git/2020/09/11/git-commit-without-password.html",
            "relUrl": "/git/2020/09/11/git-commit-without-password.html",
            "date": " • Sep 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Blog from jupyter notebook",
            "content": "That will be great if I can simply write blog entries using Jupyter Notebook. . I usually paste inner images into jupyter cells. But this feature is not available yet into fastpages. So for the moment I won&#39;t include images into these posts. . That way I could simply use markdown and insert images . And directly see rendered impact before commiting and pushing to my blog. . get local repo from github . As I am behind a proxy most of my time when working from office, the easiest way for me is to work from WSL. . WSL . I won&#39;t detail how to install WSL on Windows. . I use ubuntu images (18.04) on my PC. . set unset proxy in WSL . I have just added some bash commands at the end of my .bashrc file. . # Set Proxy function setproxy() { export {http,https,ftp}_proxy=&quot;http://&lt;my proxy ip address&gt;:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://&lt;my proxy ip address&gt;:80&quot; } # Unset Proxy function unsetproxy() { unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY } . git clone castorfou.github.io . I keep most of my local repos under ~/git/ . cd ~/git setproxy git clone https://github.com/castorfou/castorfou.github.io.git . create a blog entry with Jupyter Notebook . commit and push to github . (base) guillaume@LL11LPC0PQARQ:~$ cd git (base) guillaume@LL11LPC0PQARQ:~/git$ cd castorfou.github.io/ (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git add . (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git commit -m &#39;new blog entry: blog from jupyter&#39; [master 6b7460a] new blog entry: blog from jupyter 4 files changed, 516 insertions(+) create mode 100644 _posts/.ipynb_checkpoints/2020-09-10-blog-from-jupyter-checkpoint.ipynb create mode 100644 _posts/2020-09-10-blog-from-jupyter.ipynb create mode 100644 _posts/2020-09-10-blog-from-jupyter.py create mode 100644 _posts/Untitled.txt (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push fatal: unable to access &#39;https://github.com/castorfou/castorfou.github.io.git/&#39;: gnutls_handshake() failed: The TLS connection was non-properly terminated. . error: gnutls_handshake() failed: The TLS connection was non-properly terminated. . Just googling this error gives some insight: https://github.community/t/unable-to-push-to-repo-gnutls-handshake-failed/885 . It is likely some local firewell issue. . . To be fixed later . switch to mobile wifi without need of proxy . (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ unsetproxy (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Counting objects: 7, done. Delta compression using up to 8 threads. Compressing objects: 100% (6/6), done. Writing objects: 100% (7/7), 141.26 KiB | 10.09 MiB/s, done. Total 7 (delta 1), reused 0 (delta 0) remote: Resolving deltas: 100% (1/1), completed with 1 local object. remote: remote: GitHub found 3 vulnerabilities on castorfou/castorfou.github.io&#39;s default branch (2 high, 1 moderate). To find out more, visit: remote: https://github.com/castorfou/castorfou.github.io/network/alerts remote: To https://github.com/castorfou/castorfou.github.io.git 6adeb02..6b7460a master -&gt; master . check entries into blog . double entries . Double entries: one for the notebook (.ipynb) and one for the auto python export (.py). I will have to update my jupyter settings to avoid this python file creation. In the meantime I can just delete the python file, and commit. . . Change settings of jupyter + .gitignore to avoid these double entries . cannot open notebook into browser . Clicking just ask me to download the notebook, it doesn&#39;t display it into the browser. . checking .gitignore . Just by looking into .gitignore, there is an interesting entry: . *.swp ~* *~ _site .sass-cache .jekyll-cache .jekyll-metadata vendor _notebooks/.ipynb_checkpoints . Wait what is in this last line. . Let&#39;s create _notebooks directory and move my notebook in that directory. . notebooks from _notebooks not rendered . No entries, I guess there is some additional settings to do... . . Why notebooks are not rendered by Jekyl . test entry from md using local repo . There is no problem with that. . Creating a local md file in _poststhen pushing to github is creating the right entry blog. . following fastpages troubleshooting guide . upgrade fastpages . Try the automated upgrade as described in https://github.com/fastai/fastpages/blob/master/_fastpages_docs/UPGRADE.md . Unfortunately I don&#39;t see . I have to follow the manual upgrade. . manual fastpages upgrade . I am surprised because the 1st step from manual upgrade is to copy the fastpages repo. It is what I did 2 days ago. I doubt having an outdated version of fastpages. . fastai forum: fastpages category . I will browse through nbdev &amp; faspages category in fastai forums. I should see people with the same issue. . I have created an entry, into fastai forums: Fastpages - cannot see build process of GitHub Actions . And quite immediately Hamel Hussain answered guiding to the write direction: . I misread the Settings instruction: my github repo should explicitely NOT include my github username and I did exactly the opposite. . . I have to create a new repo: guillaume_blog . nothing visible from Actions tab . And another surprising subject: at github in Actions tab. I have a kind of default page. I expect something like an execution journal of Actions. . Page build failure . Received a notification by email: . The page build failed for the master branch with the following error: . Page build failed. For more information, see https://docs.github.com/github/working-with-github-pages/troubleshooting-jekyll-build-errors-for-github-pages-sites#troubleshooting-build-errors. . For information on troubleshooting Jekyll see: . https://docs.github.com/articles/troubleshooting-jekyll-builds . If you have any questions you can submit a request on the Contact GitHub page at https://support.github.com/contact?repo_id=293820308&amp;page_build_id=202240535 . Move to another repo . repo creation . It was just a matter of creating a new repo: . actions monitoring . Monitoring is effective . merge pull request . actions around ssh keys . Following the steps: . Create keys using ssh utility | Enter Secret Key | Enter Deploy Key | . merge PR . There are conflicts to be fixed before that. . And it works: https://castorfou.github.io/guillaume_blog/ . Get local repo . cd ~/git unsetproxy git clone https://github.com/castorfou/guillaume_blog.git .",
            "url": "https://castorfou.github.io/guillaume_blog/fastpages/jupyter/notebooks/2020/09/10/blog-from-jupyter-sans-images.html",
            "relUrl": "/fastpages/jupyter/notebooks/2020/09/10/blog-from-jupyter-sans-images.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Blogging from github",
            "content": "Blogging from github . fastai and fastpages . I am a big fan of fastai’s spirit and even more of their leaders: . Jeremy Howards | Rachel Thomas | Sylvain Gugger who is know at huggingface. | . They are commited to beautiful ideas, and are inspiring people. I like their courses. I like their softwares. For sure I will discuss about fastai. They have created fastpages. It turns github into a blogging platform. I don’t have the full detail but it is explained in fastpages github repo It is based on github actions, and by just creating a repo from a fastpages template https://github.com/fastai/fast_template/generate and giving a couple of settings, you are ready to go. . And here I have to thank Hamel Husain. He is from github company and I think he is behing github actions and helped fastai to release fastpages. I don’t know Hamel but he looks like a humble, terribly skilled guy, with tons of energy. Thanks Hamel. . my blog . My main audience is the future me. (maybe not entirely true otherwise I would have written in French) In 1 year, I want to turn back to this blog and I would like to see all the learning peaces I went through. I want this platform to be as easy as possible. . fastpages . For the moment it cannot be easier. I have setup the about page. And each blog entry is just a new markdown page into _posts. github _posts: . . By commiting this page, there are internal actions being run automatically (through github actions magic) and after a couple of minutes the new blog pages are generated (using Jekyl and ruby if I am not wrong). For the moment I use github web interface. But I guess it is easier to have a local repo of my blog, create new entries and when satisfied git push to github. (to be tested later) . github accounts . For a reason I used my personal github account (guillaume.ramelet@gmail.com) and not my professional one (guillaume.ramelet@michelin.com). I will see later if I have to move to another account. I had some troubles to setup actions into github. For a reason I thought it was available only for organization account. So I have turned my michelin github account to an organization, and I cannot login anymore. To be fixed later. . markdown . Ok I am not a huge fan of markdown. I use it as a basic text system specially within notebooks. But it is not as easy to insert images. Currently I screenshot what I want to share, insert into images folder of my repo and reference this image from my blog post using markdown language. I definitely have to improve my practice of markdown, and there are multiple cheatsheets to be used. . jupyter . There are options within fastpages to blog from jupyter notebooks. I have to do it. My intent will be to use this place to share my knowledge. Today most of my knowledge comes from experiences I make within jupyter. If I could directly blog from that it will be great. . comments . OK as the sole reader this is maybe a minor concern but there is no commenting system associated with fastpages. I cannot get any feedback from these entries. Would love to get advices, create discussions within that blog. Not for today. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/fastpages/git/2020/09/09/blogging-from-github.html",
            "relUrl": "/blog/fastpages/git/2020/09/09/blogging-from-github.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Becoming a datascientist",
            "content": "March 2019 . This journey has started about 1 year ago. . No wait, that was dormant for a long time before that. I guess I have to go back to my studying time: at that time my days were full of maths and computers. And my days were flying as crazy. It happens to me (to you?) when you’re just in a middle of something you like very much. 10 hours looks like 1. And the opposite is true as well. . 2000 - 2004: software development . Most of my days and weekends at that time were dedicated to code in Java and bash. Java mainly for server-side developpement in J2EE at Unilog Management. Bash from time to time to automate some tasks on my personal PC. At that time it was mainly about learning what is an operating system. I had started with LFS (Linux From Scratch). And in 2002 with Gentoo which was a much more powerful way to mimic LFS. . 2004 - 2009: project management . Strange period. I don’t remember exactly why but I had a shift in my professional orientation. I moved away from software development and turned into a project manager. In 2005 I entered into Michelin company. And sofwtare technical matters at that time were considered as unimportant (and embarrassing subjects) Fortunately in 2009 I have been started my agile journey. A lot to learn, and it was less about software than human relations and empathy. It was like a start from scratch. . 2009 - 2015: agile journey . Quite a new world for me. I had some basic knowledge by following Jono Bacon. At that time he was a community manager|release leader at Ubuntu. And was reporting progress using burndown charts. In 2009 I launched a project to create an employee portal (closed to what netvibes and igoogle were at that time). Using standard java portal technologies and more importantly using agile approach. A lot to learn about Agile, Scrum, and endless discussions about how to introduce Agile into a non-Agile organization. In 2010 I started another more ambitious project, with many colleagues (~30 persons) and a vague vision. It was about to create a product lifecycle management solution for semi-finished products. . 2015 - 2017: lean journey . In 2015, I met lean approaches for office. I was immediately convinced there was powerful and deep roots within lean. And it could bring a lot to people and organizations. I turned into a lean coach, to work with teams identifying what they could improve, how they could work better, with more pleasure. . 2017 - 2019: Welcome to USA! . Nice opportunity at that time to move from Clermont-Ferrand (France) to Greenville, South Carolina (USA). I have loved every part of it. Except maybe that 2 years were too short to make a full tour of this amazing country. It is crazy to think how different we are when we look like the same. . 2019 - : back to France and turning as a datascientist . Sept 2019 - back to France and for 4 months to prepare for a complete new position: datascientist for Manufacturing within Michelin. I spent many days to learn from various sources specially datacamp and Andrew Ng. That was just the beginning. My intent was to move away from project management, team leadership and focus about what I can do by myself. I wanted to return to math domains without giving up an IT landscape. My colleague Francois Deheeger told me about data science and Artificial Intelligence. That looked as interesting as terrifying. I was in. I was not afraid to learn a new language, and to restart my career from scratch. .",
            "url": "https://castorfou.github.io/guillaume_blog/me/2020/09/08/becoming-datascientist.html",
            "relUrl": "/me/2020/09/08/becoming-datascientist.html",
            "date": " • Sep 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About . This blog . This is more a journal where I am adding entries about my (baby steps) learnings. It is likely to be centered around python, git, data-science, … I have been strongly inspired by Rachel Thomas explaining why I should blog. Specially when starting such a journey to turn a datascientist. . My intent would be to regularly add entries to this blog. Ideally at least once a week. Maybe only short ones, the point being to stick on this frequent activity. If it takes days to write posts I am pretty sure I won’t do it. Those entries are personnal thoughts and not those of my employer Michelin. . Me . I am 44 (in 2020). Father of 3. Working for a French tire company. I am French and for sure English is not my mother tongue. .",
          "url": "https://castorfou.github.io/guillaume_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://castorfou.github.io/guillaume_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}