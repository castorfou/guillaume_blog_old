{
  
    
        "post0": {
            "title": "using SOCKS5 proxy - with git, apt, pip, ...",
            "content": "setup socks5 server . using dante server . Installation . sudo apt-get install dante-server . Conf file . sudo nano /etc/danted.conf logoutput: stderr internal: enp3s0 port = 1080 external: enp3s0 socksmethod: none clientmethod: none user.privileged: proxy user.unprivileged: nobody user.libwrap: nobody client pass { from: 0.0.0.0/0 to: 0.0.0.0/0 log: error connect disconnect } client block { from: 0.0.0.0/0 to: 0.0.0.0/0 log: connect error } socks pass { from: 0.0.0.0/0 to: 0.0.0.0/0 log: error connect disconnect } socks block { from: 0.0.0.0/0 to: 0.0.0.0/0 log: connect error } . Start and monitor usage . sudo service danted restart tail -f /var/log/syslog . Git setup . $ cat .ssh/config Host github.com IdentityFile ~/.ssh/id_rsa_gmail ProxyCommand /bin/nc -X 5 -x 192.168.50.202:1080 %h %p . Proxychains . installation . # to be downloaded from apt mirrors: # libproxychains proxychains sudo dpkg -i libproxychains3_3.1-7_amd64.deb proxychains_3.1-7_all.deb . configuration . sudo vi /etc/proxychains.conf [ProxyList] # add proxy here ... # meanwile # defaults set to &quot;tor&quot; socks5 192.168.50.202 1080 . usage . sudo proxychains apt update sudo proxychains apt upgrade proxychains pip install pycaret .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/wsl2-cuda-conda.html",
            "relUrl": "/blog/wsl2-cuda-conda.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "setup wsl2 with cuda and conda",
            "content": "wsl2 and network + proxychains . workaround explained in this blog entry . sudo . cuda . https://docs.nvidia.com/cuda/wsl-user-guide/index.html#installing-nvidia-drivers . install nvidia cuda specific driver for WSL: https://developer.nvidia.com/cuda/wsl on windows. (version 470.14_quadro_win10-dch_64bit_international in my case) . proxychains wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub sudo proxychains add-apt-repository &quot;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /&quot; sudo proxychains apt-get update sudo proxychains apt-get -y install cuda-toolkit-11-2 . conda . from https://docs.conda.io/en/latest/miniconda.html . download https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . and install with ./Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3 . pycaret . conda create --name pycaret python=3.7 conda activate pycaret3 proxychains pip install pycaret shap proxychains conda install -c conda-forge nb_conda jupyter_contrib_nbextensions fire pyfiglet openpyxl jupyter contrib nbextensions install --user proxychains conda upgrade nbconvert .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/git-avec-proxy-socks.html",
            "relUrl": "/blog/git-avec-proxy-socks.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Logbook for April 21",
            "content": "Week 13 - Apr 21 . Thursday 4/1 . Aniti RLVS - Deep Q-Networks and its variants . Collège de France - Algorithmes quantiques : quand la physique quantique défie la thèse de Church-Turing Leçon inaugurale . Friday 4/2 . Aniti RLVS - From Policy Gradients to Actor Critic methods . Aniti RLVS - Policy Gradient in pratice . git to use socks server to github (to go through local firewall) . Aniti RLVS - Exploration in Deep RL . Week 14 - Apr 21 . Wednesday 4/7 . tabnet: pytorch (and fastai with Zach Mueller) implementations . Thursday 4/8 . Aniti RLVS - Evolutionary Reinforcement Learning . Jupyter notebook turned into slides with RISE . Aniti RLVS - Micro-data Policy Search . Friday 4/9 . Aniti RLVS - RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3 . setup wsl-vpnkit to workaround wsl2 and network issues (explained here) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-April.html",
            "relUrl": "/blog/logbook-April.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "ANITI's first Reinforcement Learning Virtual School",
            "content": ". https://rlvs.aniti.fr/ . Schedule is . RLVS schedule . This condensed schedule does not include class breaks and social events. Times are Central European Summer Time (UTC+2). . Schedule       . March 25th | 9:00-9:10 | Opening remarks | S. Gerchinovitz | .   | 9:10-9:30 | RLVS Overview | E. Rachelson | .   | 9:30-13:00 | RL fundamentals | E. Rachelson | .   | 14:00-16:00 | Introduction to Deep Learning | D. Wilson | .   | 16:30-17:30 | Reward Processing Biases in Humans and RL Agents | I. Rish | .   | 17:45-18:45 | Introduction to Hierarchical Reinforcement Learning | D. Precup | . March 26th | 10:00-12:00 | Stochastic bandits | T. Lattimore | .   | 14:00-16:00 | Monte Carlo Tree Search | T. Lattimore | .   | 16:30-17:30 | Multi-armed bandits in clinical trials | D. A. Berry | . April 1st | 9:00-15:00 | Deep Q-Networks and its variants | B. Piot, C. Tallec | .   | 15:15-16:15 | Regularized MDPs | M. Geist | .   | 16:30-17:30 | Regret bounds of model-based reinforcement learning | M. Wang | . April 2nd | 9:00-12:30 | Policy Gradients and Actor Critic methods | O. Sigaud | .   | 14:00-15:00 | Pitfalls in Policy Gradient methods | O. Sigaud | .   | 15:30-17:30 | Exploration in Deep RL | M. Pirotta | . April 8th | 9:00-11:00 | Evolutionary Reinforcement Learning | D. Wilson, J.-B. Mouret | .   | 11:30-12:30 | Evolving Agents that Learn More Like Animals | S. Risi | .   | 14:00-16:00 | Micro-data Policy Search | K. Chatzilygeroudis, J.-B. Mouret | .   | 16:30-17:30 | Efficient Motor Skills Learning in Robotics | D. Lee | . April 9th | 9:00-13:00 | RL tips and tricks | A. Raffin | .   | 14:30-15:30 | Symbolic representations and reinforcement learning | M. Garnelo | .   | 15:45-16:45 | Leveraging model-learning for extreme generalization | L. P. Kaelbling | .   | 17:00-18:00 | RLVS wrap-up | E. Rachelson | . (4/1/21) - Deep Q-Networks and its variants . Speaker is Bilal Piot. . Deep Q network as a solution for a practicable control theory. . Introduction of ALE (Atari Learning Environment) . DQN is (almost) end-to-end: from raw observations to actions. Bilal explains the preprocessing part (from 160x210x3 to 84x84 + stacking 4 frames + downsampling to 15 Hz) . Value Iteration (VI) algorithm: Recurrent algorithm to get Q. $Q_{k+1}=T^*Q$ . But it is not practical in a real-world case. What we can do is use interactions with real world. And estimate $Q^*$ using a regression. . Would be interesting to have slides. I like the link between regression notations and VI notation. . From neural Fitted-$Q$ to DQN. Main difference is data collection (in DQN you have updated interactions and it allows exploration, and size of architecture) . With DQN we have acting part and learning part. Acting is the data collection. (using $ epsilon$-greedy policy) . hands-on based on DQN tutorial notebook. . had to export LD_LIBRARY_PATH=/home/explore/miniconda3/envs/aniti/lib/ . Nice introduction to JAX and haiku. Haiku is similar modules in pytorch and can turn NN into pure version. Which is useful for Jax. . overview of the literature . . (4/2/21) - From Policy Gradients to Actor Critic methods . Olivier Sigaud is the speaker. . He has pre-recorded his lecture in videos. I have missed the start so I will have to watch them later. . Policy Gradient in pratice . Don’t become an alchemist ;) . As stochastic policies, squashed gaussian is interesting because it allows continuous variable + bounds. . Exploration in Deep RL . (4/8/21) - Evolutionary Reinforcement Learning . pdf version of the slides are available here . then Evolving Agents that Learn More Like Animals . This morning was more about what we can do when we have infinite calculation power and data. . Afternoon will be the opposite. . (4/8/21) - Micro-data Policy Search . Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible when experiments takes time or are expensive (for instance, with physical robot or with an aerodynamics simulator). This class focuses on the extreme other end of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word “big-data”, we refer to this challenge as “micro-data reinforcement learning”. We will describe two main strategies: (1) leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators), and (2) create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup. . all material: https://rl-vs.github.io/rlvs2021/micro-data.html . (4/9/21) - RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3 . ​ Abstract: The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library. . Pre-requisites: Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab). . Additional material: Website: https://github.com/DLR-RM/stable-baselines3 Doc: https://stable-baselines3.readthedocs.io/en/master/ . Outline: Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots . Introduction (3 minutes) | RL Tips and tricks (45 minutes) General Nuts and Bolts of RL experimentation (10 minutes) | RL in practice on a custom task (custom environment) (30 minutes) | Questions? (5 minutes) | | The Challenges of Applying RL to Real Robots (45 minutes) Learning to control an elastic robot - DLR David Neck Example (15 minutes) | Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes) | Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes) | Questions? (5 minutes+) | | Part II: Practical Session with Stable-Baselines3 . Stable-Baselines3 Overview (20 minutes) | Questions? (5 minutes) | Practical Session - Code along (1h+) | action space . When using continuous space, you need to normalize! (normalized action space -1, -1) . there is a checker for that in stable baselines 3. . reward . start with reward shaping. . termination condition . early stopping makes learning faster (and safer for robots) . . for hyperparameter tuning, Antonin recommends Optuna. . about the Henderson paper: Deep Reinforcement Learning that Matters . . and then the controller will use latent representation / current speed + history as observation space. . Learning to drive takes then 10 min, and to race 2 hours. . handson . slides: https://araffin.github.io/slides/rlvs-sb3-handson/ . notebook: https://github.com/araffin/rl-handson-rlvs21 . RL zoo: https://github.com/DLR-RM/rl-baselines3-zoo . documentation for SB3 usefull for completing exercises: https://stable-baselines3.readthedocs.io/en/master/ . https://excalidraw.com/ .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html",
            "relUrl": "/blog/Aniti-RLVS-seminaire-RL.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Headless raspberry pi: create a wifi to ethernet bridge",
            "content": "After my internet provider router stopped unexpectedly yesterday, I had to find a solution with internet access from phones and raspberry pi to broadcast internet to full home devices. . Headless raspberry pi . Installation on SD from ubuntu . for a reason, raspberry pi imager snap doesn’t work (due to a bug linked to QT+wayland). . I download deb ubuntu version (imager_1.6_amd64.deb) from https://www.raspberry.org/software and install with dpkg. (sudo dpkg -i imager_1.6_amd64.deb) . With rpi-imager, I can install by selecting the default OS (raspberry Pi OS 32-bit), and SD card as storage. . Headless wifi . As explained in https://www.raspberrypi.org/documentation/configuration/wireless/headless.md . Create (touch) wpa_supplicant.conf in /boot of SD card and paste this content: . ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=FR network={ ssid=&quot;AndroidAP&quot; psk=&quot;&lt;Password for your wireless LAN&gt;&quot; } . Headless ssh . As explained in https://www.raspberrypi.org/documentation/remote-access/ssh/README.md . Create (touch) ssh in /boot of SD card . If it is found, SSH is enabled and the file is deleted. The content of the file does not matter; it could contain text, or nothing at all. . Test installation . Boot. After a couple of minutes, I have a notification on phone saying a device is connected on my phone hotspot. . . And ssh raspberry (default username/password are pi/raspberry) . $ ssh -l pi 192.168.43.179 pi@192.168.43.179&#39;s password: Linux raspberrypi 5.4.83-v7+ #1379 SMP Mon Dec 14 13:08:57 GMT 2020 armv7l The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. Last login: Thu Mar 25 06:23:17 2021 SSH is enabled and the default password for the &#39;pi&#39; user has not been changed. This is a security risk - please login as the &#39;pi&#39; user and type &#39;passwd&#39; to set a new password. . Headless raspberry is ready to be used. . Wifi to ethernet bridge . I will use https://willhaley.com/blog/raspberry-pi-wifi-ethernet-bridge/ . The only package that is needed is dnsmasq however from a clean install it is a good idea to make sure everything is up-to-date: . get up-to-date system . sudo apt-get update &amp;&amp; sudo apt-get upgrade -y &amp;&amp; sudo apt-get install rpi-update dnsmasq -y sudo rpi-update . Option 1 - Same Subnet . Save this script as a file named bridge.sh on your Pi. . #!/usr/bin/env bash set -e [ $EUID -ne 0 ] &amp;&amp; echo &quot;run as root&quot; &gt;&amp;2 &amp;&amp; exit 1 ########################################################## # You should not need to update anything below this line # ########################################################## # parprouted - Proxy ARP IP bridging daemon # dhcp-helper - DHCP/BOOTP relay agent apt update &amp;&amp; apt install -y parprouted dhcp-helper systemctl stop dhcp-helper systemctl enable dhcp-helper # Enable ipv4 forwarding. sed -i&#39;&#39; s/#net.ipv4.ip_forward=1/net.ipv4.ip_forward=1/ /etc/sysctl.conf # Service configuration for standard WiFi connection. Connectivity will # be lost if the username and password are incorrect. systemctl restart wpa_supplicant.service # Enable IP forwarding for wlan0 if it&#39;s not already enabled. grep &#39;^option ip-forwarding 1$&#39; /etc/dhcpcd.conf || printf &quot;option ip-forwarding 1 n&quot; &gt;&gt; /etc/dhcpcd.conf # Disable dhcpcd control of eth0. grep &#39;^denyinterfaces eth0$&#39; /etc/dhcpcd.conf || printf &quot;denyinterfaces eth0 n&quot; &gt;&gt; /etc/dhcpcd.conf # Configure dhcp-helper. cat &gt; /etc/default/dhcp-helper &lt;&lt;EOF DHCPHELPER_OPTS=&quot;-b wlan0&quot; EOF # Enable avahi reflector if it&#39;s not already enabled. sed -i&#39;&#39; &#39;s/#enable-reflector=no/enable-reflector=yes/&#39; /etc/avahi/avahi-daemon.conf grep &#39;^enable-reflector=yes$&#39; /etc/avahi/avahi-daemon.conf || { printf &quot;something went wrong... n n&quot; printf &quot;Manually set &#39;enable-reflector=yes in /etc/avahi/avahi-daemon.conf&#39; n&quot; } # I have to admit, I do not understand ARP and IP forwarding enough to explain # exactly what is happening here. I am building off the work of others. In short # this is a service to forward traffic from WiFi to Ethernet. cat &lt;&lt;&#39;EOF&#39; &gt;/usr/lib/systemd/system/parprouted.service [Unit] Description=proxy arp routing service Documentation=https://raspberrypi.stackexchange.com/q/88954/79866 Requires=sys-subsystem-net-devices-wlan0.device dhcpcd.service After=sys-subsystem-net-devices-wlan0.device dhcpcd.service [Service] Type=forking # Restart until wlan0 gained carrier Restart=on-failure RestartSec=5 TimeoutStartSec=30 # clone the dhcp-allocated IP to eth0 so dhcp-helper will relay for the correct subnet ExecStartPre=/bin/bash -c &#39;/sbin/ip addr add $(/sbin/ip -4 -br addr show wlan0 | /bin/grep -Po &quot; d+ . d+ . d+ . d+&quot;)/32 dev eth0&#39; ExecStartPre=/sbin/ip link set dev eth0 up ExecStartPre=/sbin/ip link set wlan0 promisc on ExecStart=-/usr/sbin/parprouted eth0 wlan0 ExecStopPost=/sbin/ip link set wlan0 promisc off ExecStopPost=/sbin/ip link set dev eth0 down ExecStopPost=/bin/bash -c &#39;/sbin/ip addr del $(/sbin/ip -4 -br addr show wlan0 | /bin/grep -Po &quot; d+ . d+ . d+ . d+&quot;)/32 dev eth0&#39; [Install] WantedBy=wpa_supplicant.service EOF systemctl daemon-reload systemctl enable parprouted systemctl start parprouted dhcp-helper . Step 2: Execute the script on your Pi like so. . sudo bash bridge.sh . Step 3: Reboot. . sudo reboot . Done! .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/headless-raspberry-pi-bridge-network.html",
            "relUrl": "/blog/headless-raspberry-pi-bridge-network.html",
            "date": " • Mar 25, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Git - How To Contribute To A Project",
            "content": "Based on http://qpleple.com/how-to-contribute-to-a-project-on-github/ . Using clustergit as an example . Fork . Make your own working copy of the project by forking it: go on the project page (https://github.com/mnagel/clustergit) and click “Fork”. You can access you copy at: https://github.com/castorfou/clustergit . Clone . Clone your fork git repository on your local computer: . git clone git@github.com:castorfou/clustergit.git . Branch . git branch master-to-main git checkout master-to-main . This is very important, create one branch per patch. And never submit a patch that has been done on the branch master or main! . Develop . Here I want to reflect change from Oct/20 where default branch name in github is now main . sed -i &#39;s/master/main/g&#39; clustergit . Commit . git add -u git commit -m &quot;default branch name &#39;main&#39;&quot; . Push to github . git push origin master-to-main . Create pull request . Go on your fork page (https://github.com/castorfou/clustergit), then select master-to-main in the branch list and click “Pull Request”. . Submit patch . Check the diff, write a message explaining what you have done and why the repository owner should accept your pull request and submit. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/contribute-to-a-project-with-git.html",
            "relUrl": "/blog/contribute-to-a-project-with-git.html",
            "date": " • Mar 25, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Stable baselines 3 - 1st steps",
            "content": "What is stable baselines 3 (sb3) . I have just read about this new release. This is a complete rewrite of stable baselines 2, without any reference to tensorflow, and based on pytorch (&gt;1.4+). . There is a lot of running implementations of RL algorithms, based on gym. A very good introduction in this blog entry . Stable-Baselines3: Reliable Reinforcement Learning Implementations | Antonin Raffin | Homepage . Links . GitHub repository: https://github.com/DLR-RM/stable-baselines3 . | Documentation: https://stable-baselines3.readthedocs.io/ . | RL Baselines3 Zoo: https://github.com/DLR-RM/rl-baselines3-zoo . | Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib . | RL Tutorial: https://github.com/araffin/rl-tutorial-jnrr19 . | . My installation . Standard installation . conda create --name stablebaselines3 python=3.7 conda activate stablebaselines3 pip install stable-baselines3[extra] conda install -c conda-forge jupyter_contrib_nbextensions conda install nb_conda . !conda list . # packages in environment at /home/explore/miniconda3/envs/stablebaselines3: # # Name Version Build Channel _libgcc_mutex 0.1 main _pytorch_select 0.1 cpu_0 absl-py 0.12.0 pypi_0 pypi atari-py 0.2.6 pypi_0 pypi attrs 20.3.0 pyhd3deb0d_0 conda-forge backcall 0.2.0 pyh9f0ad1d_0 conda-forge backports 1.0 py_2 conda-forge backports.functools_lru_cache 1.6.1 py_0 conda-forge blas 1.0 mkl bleach 3.3.0 pyh44b312d_0 conda-forge box2d 2.3.10 pypi_0 pypi box2d-py 2.3.8 pypi_0 pypi ca-certificates 2021.1.19 h06a4308_1 cachetools 4.2.1 pypi_0 pypi certifi 2020.12.5 py37h06a4308_0 cffi 1.14.5 py37h261ae71_0 chardet 4.0.0 pypi_0 pypi cloudpickle 1.6.0 pypi_0 pypi cudatoolkit 11.0.221 h6bb024c_0 cycler 0.10.0 pypi_0 pypi decorator 4.4.2 py_0 conda-forge defusedxml 0.7.1 pyhd8ed1ab_0 conda-forge entrypoints 0.3 pyhd8ed1ab_1003 conda-forge fire 0.4.0 pyh44b312d_0 conda-forge freetype 2.10.4 h5ab3b9f_0 future 0.18.2 pypi_0 pypi google-auth 1.28.0 pypi_0 pypi google-auth-oauthlib 0.4.3 pypi_0 pypi grpcio 1.36.1 pypi_0 pypi gym 0.18.0 pypi_0 pypi icu 58.2 hf484d3e_1000 conda-forge idna 2.10 pypi_0 pypi importlib-metadata 3.7.3 py37h89c1867_0 conda-forge intel-openmp 2019.4 243 ipykernel 5.5.0 py37h888b3d9_1 conda-forge ipython 7.21.0 py37h888b3d9_0 conda-forge ipython_genutils 0.2.0 py_1 conda-forge jedi 0.18.0 py37h89c1867_2 conda-forge jinja2 2.11.3 pyh44b312d_0 conda-forge jpeg 9b h024ee3a_2 jsonschema 3.2.0 pyhd8ed1ab_3 conda-forge jupyter_client 6.1.12 pyhd8ed1ab_0 conda-forge jupyter_contrib_core 0.3.3 py_2 conda-forge jupyter_contrib_nbextensions 0.5.1 pyhd8ed1ab_2 conda-forge jupyter_core 4.7.1 py37h89c1867_0 conda-forge jupyter_highlight_selected_word 0.2.0 py37h89c1867_1002 conda-forge jupyter_latex_envs 1.4.6 pyhd8ed1ab_1002 conda-forge jupyter_nbextensions_configurator 0.4.1 py37h89c1867_2 conda-forge kiwisolver 1.3.1 pypi_0 pypi lcms2 2.11 h396b838_0 ld_impl_linux-64 2.33.1 h53a641e_7 libffi 3.3 he6710b0_2 libgcc-ng 9.1.0 hdf63c60_0 libmklml 2019.0.5 0 libpng 1.6.37 hbc83047_0 libsodium 1.0.18 h36c2ea0_1 conda-forge libstdcxx-ng 9.1.0 hdf63c60_0 libtiff 4.2.0 h85742a9_0 libuv 1.40.0 h7b6447c_0 libwebp-base 1.2.0 h27cfd23_0 libxml2 2.9.10 hb55368b_3 libxslt 1.1.34 hc22bd24_0 lxml 4.6.3 py37h9120a33_0 lz4-c 1.9.3 h2531618_0 markdown 3.3.4 pypi_0 pypi markupsafe 1.1.1 py37hb5d75c8_2 conda-forge matplotlib 3.3.4 pypi_0 pypi mistune 0.8.4 py37h4abf009_1002 conda-forge mkl 2020.2 256 mkl-service 2.3.0 py37he8ac12f_0 mkl_fft 1.3.0 py37h54f3939_0 mkl_random 1.1.1 py37h0573a6f_0 nb_conda 2.2.1 py37_0 nb_conda_kernels 2.3.1 py37h06a4308_0 nbconvert 5.6.1 py37hc8dfbb8_1 conda-forge nbformat 5.1.2 pyhd8ed1ab_1 conda-forge ncurses 6.2 he6710b0_1 ninja 1.10.2 py37hff7bd54_0 notebook 5.7.10 py37hc8dfbb8_0 conda-forge numpy 1.20.1 pypi_0 pypi numpy-base 1.19.2 py37hfa32c7d_0 oauthlib 3.1.0 pypi_0 pypi olefile 0.46 py37_0 opencv-python 4.5.1.48 pypi_0 pypi openssl 1.1.1j h27cfd23_0 packaging 20.9 pyh44b312d_0 conda-forge pandas 1.2.3 pypi_0 pypi pandoc 2.12 h7f98852_0 conda-forge pandocfilters 1.4.2 py_1 conda-forge parso 0.8.1 pyhd8ed1ab_0 conda-forge pexpect 4.8.0 pyh9f0ad1d_2 conda-forge pickleshare 0.7.5 py_1003 conda-forge pillow 7.2.0 pypi_0 pypi pip 21.0.1 py37h06a4308_0 prometheus_client 0.9.0 pyhd3deb0d_0 conda-forge prompt-toolkit 3.0.18 pyha770c72_0 conda-forge protobuf 3.15.6 pypi_0 pypi psutil 5.8.0 pypi_0 pypi ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge pyasn1 0.4.8 pypi_0 pypi pyasn1-modules 0.2.8 pypi_0 pypi pycparser 2.20 py_2 pyglet 1.5.0 pypi_0 pypi pygments 2.8.1 pyhd8ed1ab_0 conda-forge pyparsing 2.4.7 pyh9f0ad1d_0 conda-forge pyrsistent 0.17.3 py37h4abf009_1 conda-forge python 3.7.10 hdb3f193_0 python-dateutil 2.8.1 py_0 conda-forge python_abi 3.7 1_cp37m conda-forge pytorch 1.7.1 py3.7_cuda11.0.221_cudnn8.0.5_0 pytorch pytz 2021.1 pypi_0 pypi pyyaml 5.3.1 py37hb5d75c8_1 conda-forge pyzmq 19.0.2 py37hac76be4_2 conda-forge readline 8.1 h27cfd23_0 requests 2.25.1 pypi_0 pypi requests-oauthlib 1.3.0 pypi_0 pypi rsa 4.7.2 pypi_0 pypi scipy 1.6.1 pypi_0 pypi send2trash 1.5.0 py_0 conda-forge setuptools 52.0.0 py37h06a4308_0 six 1.15.0 pyh9f0ad1d_0 conda-forge sqlite 3.35.2 hdfb4753_0 stable-baselines3 1.0 pypi_0 pypi tensorboard 2.4.1 pypi_0 pypi tensorboard-plugin-wit 1.8.0 pypi_0 pypi termcolor 1.1.0 py37h06a4308_1 terminado 0.9.3 py37h89c1867_0 conda-forge testpath 0.4.4 py_0 conda-forge tk 8.6.10 hbc83047_0 torchaudio 0.7.2 py37 pytorch torchvision 0.8.2 py37_cu110 pytorch tornado 6.1 py37h4abf009_0 conda-forge traitlets 5.0.5 py_0 conda-forge typing-extensions 3.7.4.3 0 typing_extensions 3.7.4.3 py_0 conda-forge urllib3 1.26.4 pypi_0 pypi wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge webencodings 0.5.1 py_1 conda-forge werkzeug 1.0.1 pypi_0 pypi wheel 0.36.2 pyhd3eb1b0_0 xz 5.2.5 h7b6447c_0 yaml 0.2.5 h516909a_0 conda-forge zeromq 4.3.4 h2531618_0 zipp 3.4.1 pyhd8ed1ab_0 conda-forge zlib 1.2.11 h7b6447c_3 zstd 1.4.5 h9ceee32_0 . SB3 tutorials . import gym from stable_baselines3 import A2C from stable_baselines3.common.monitor import Monitor from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback # Save a checkpoint every 1000 steps checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=&quot;/home/explore/git/guillaume/stable_baselines_3/logs/&quot;, name_prefix=&quot;rl_model&quot;) # Evaluate the model periodically # and auto-save the best model and evaluations # Use a monitor wrapper to properly report episode stats eval_env = Monitor(gym.make(&quot;LunarLander-v2&quot;)) # Use deterministic actions for evaluation eval_callback = EvalCallback(eval_env, best_model_save_path=&quot;/home/explore/git/guillaume/stable_baselines_3/logs/&quot;, log_path=&quot;/home/explore/git/guillaume/stable_baselines_3/logs/&quot;, eval_freq=2000, deterministic=True, render=False) # Train an agent using A2C on LunarLander-v2 model = A2C(&quot;MlpPolicy&quot;, &quot;LunarLander-v2&quot;, verbose=1) model.learn(total_timesteps=20000, callback=[checkpoint_callback, eval_callback]) # Retrieve and reset the environment env = model.get_env() obs = env.reset() # Query the agent (stochastic action here) action, _ = model.predict(obs, deterministic=False) . Issues and fix . CUDA error: CUBLAS_STATUS_INTERNAL_ERROR . Downgrade pytorch to 1.7.1 . to avoid RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasCreate(handle) . pip install torch==1.7.1 . RuntimeError: CUDA error: invalid device function . !nvidia-smi . Thu Mar 25 09:13:49 2021 +--+ | NVIDIA-SMI 450.102.04 Driver Version: 450.102.04 CUDA Version: 11.0 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Quadro RTX 4000 Off | 00000000:01:00.0 On | N/A | | N/A 41C P5 18W / N/A | 2104MiB / 7982MiB | 32% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1153 G /usr/lib/xorg/Xorg 162MiB | | 0 N/A N/A 1904 G /usr/lib/xorg/Xorg 268MiB | | 0 N/A N/A 2076 G /usr/bin/gnome-shell 403MiB | | 0 N/A N/A 2697 G ...gAAAAAAAAA --shared-files 54MiB | | 0 N/A N/A 7220 G ...AAAAAAAAA= --shared-files 84MiB | | 0 N/A N/A 57454 G /usr/lib/firefox/firefox 2MiB | | 0 N/A N/A 59274 C ...ablebaselines3/bin/python 1051MiB | +--+ . CUDA version is 11.0 on my workstation. . !nvcc --version . nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Sun_Jul_28_19:07:16_PDT_2019 Cuda compilation tools, release 10.1, V10.1.243 . !conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch . Collecting package metadata (current_repodata.json): done Solving environment: done # All requested packages already installed. . Everything seems fine after these updates. . Stable baselines 3 user guide . There is an impressive documentation associated with stable baselines 3. Quickstart . Tips and tricks . This page covers general advice about RL (where to start, which algorithm to choose, how to evaluate an algorithm, …), as well as tips and tricks when using a custom environment or implementing an RL algorithm. . Be familiar with RL, see resource page | read SB3 documentation | do the tutorial | . Tune hyperparameters RL zoo is introduced. It contains some hyperparameter optimization. . RL evaluation We suggest you reading Deep Reinforcement Learning that Matters for a good discussion about RL evaluation. . which algorithm to choose 1st criteria is discrete vs continuous actions. And 2nd is capacity to parallelize training. . Discrete Actions . Discrete Actions - Single Process | . DQN with extensions (double DQN, prioritized replay, …) are the recommended algorithms. We notably provide QR-DQN in our contrib repo. DQN is usually slower to train (regarding wall clock time) but is the most sample efficient (because of its replay buffer). . Discrete Actions - Multiprocessed | . You should give a try to PPO or A2C. . Continuous Actions . Continuous Actions - Single Process | . Current State Of The Art (SOTA) algorithms are SAC, TD3 and TQC (available in our contrib repo). Please use the hyperparameters in the RL zoo for best results. . Continuous Actions - Multiprocessed | . Take a look at PPO or A2C. Again, don’t forget to take the hyperparameters from the RL zoo for continuous actions problems (cf Bullet envs). . Creating a custom env . multiple times there are advices about normalizing: observation and action space. A good practice is to rescale your actions to lie in [-1, 1]. This does not limit you as you can easily rescale the action inside the environment . tips and tricks to reproduce a RL paper . Reinforcement Learning Tips and Tricks — Stable Baselines3 1.1.0a1 documentation . A personal pick (by @araffin) for environments with gradual difficulty in RL with continuous actions:&gt; &gt; 1. Pendulum (easy to solve) . HalfCheetahBullet (medium difficulty with local minima and shaped reward) . | BipedalWalkerHardcore (if it works on that one, then you can have a cookie) . | in RL with discrete actions:&gt; &gt; 1. CartPole-v1 (easy to be better than random agent, harder to achieve maximal performance) . LunarLander . | Pong (one of the easiest Atari game) . | other Atari games (e.g. Breakout) . | Resource page . Reinforcement Learning Resources — Stable Baselines3 1.1.0a1 documentation . Stable-Baselines3 assumes that you already understand the basic concepts of Reinforcement Learning (RL). . However, if you want to learn about RL, there are several good resources to get started: . OpenAI Spinning Up . | David Silver’s course . | Lilian Weng’s blog . | Berkeley’s Deep RL Bootcamp . | Berkeley’s Deep Reinforcement Learning course . | More resources . | . Examples . I will run these examples in 01 -hands-on.ipynb from handson_stablebaselines3 . DQN lunarlander . My module is never landing :( . Note: animated gif created with peek. . PPO with multiprocessing cartpole . . Monitor training using callback . This could be useful when you want to monitor training, for instance display live learning curves in Tensorboard (or in Visdom) or save the best agent. . . Atari game such as pong (A2C with 6 envt) or breakout . . Here the list of valid gym atari environments: https://gym.openai.com/envs/#atari . . pybullet . This is a SDK to real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc. . https://github.com/bulletphysics/bullet3/ . . We need to install it: pip install pybullet . I don&#39;t have rendering capacity when playing with it. Because robotic is far from my need, I will skip on this one . Hindsight Experience Replay (HER) . using Highway-Env . installation with pip install highway-env . After 1h15m of training, some 1st results: . . And after that some technical stuff such as: . Learning Rate Schedule: start with high value and reduce it as learning goes | Advanced Saving and Loading: how to easily create a test environment to evaluate an agent periodically, use a policy independently from a model (and how to save it, load it) and save/load a replay buffer. | Accessing and modifying model parameters: These functions are useful when you need to e.g. evaluate large set of models with same network structure, visualize different layers of the network or modify parameters manually. | Record a video or make a gif | . Make a GIF of a Trained Agent . pip install imageio . and this time the lander is getting closer to moon but not at all between flags. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html",
            "relUrl": "/blog/stable-baselines-3.html",
            "date": " • Mar 24, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Git - How to find all *unpushed* commits for all projects in a directory?",
            "content": "Very basic question to help keep my repo clean. . Installation clustergit . clustergit seems a good candidate . . cd ~/Applications git clone git@github.com:mnagel/clustergit.git # add export PATH=&quot;$PATH:$HOME/Applications/clustergit&quot; to ~.bashrc source ~.bashrc . or using .local/bin . cd ~/Applications/ git clone git@github.com:castorfou/clustergit.git cd ~ mkdir -p .local/bin cd .local/bin/ ln -s ~/Applications/clustergit/clustergit . source .profile . Usage clustergit . clustergit status . $ clustergit Scanning sub directories of . ./Deep-Reinforcement-Learning-Hands-On : Changesn . (1/17) ./Deep_reinforcement_learning_Course : Changes ./ReinforcementLearning_references : On branch main, Untracked files ./blog : Untracked files ./d059 : On branch main, Changes ./data-scientist-skills : Clean ./deeplearning_specialization : Clean ./fastai : Changes ./fastai_experiments : Changes ./fastbook : Changes ./gan_specialization : Clean ./hello_nbdev : Clean ./introduction-reinforcement-learning-david-silver: On branch main, Untracked files ./mit_600.2x Introduction to Computational Thinking and Data Science: Clean ./mit_6S191_Intro_to_deep_learning : On branch main, No Changes ./pytorch_tutorial : On branch main, Changes ./squeezebox : On branch main, No Changes Done . clustergit status (detailed) . $ clustergit -v [...] - ./squeezebox -- running LC_ALL=C git status On branch main Your branch is up to date with &#39;origin/main&#39;. nothing to commit, working tree clean ./squeezebox : On branch main, No Changes - ./squeezebox -- Done . clustergit status (less detailed: hide Clean) . $ clustergit -H Scanning sub directories of . ./d059 : On branch main, Changes ./fastai : Changes ./fastai_experiments : Changes ./fastbook : Changes ./introduction-reinforcement-learning-david-silver: On branch main, Untracked files ./mit_6S191_Intro_to_deep_learning : On branch main, No Changes ./pytorch_tutorial : On branch main, Changes ./squeezebox : On branch main, No Changes Done . Clean vs On branch main, No Changes . seems related to branch name. If branch is named master, then clean is displayed. . (Mar/25 21) I have just changed clustergit to have main as default branch name instead of master (github having set main as the new standard) . Rename everything from master to main . Git pull, push . I am not sure I will use it. But allows to recursively launch pull commands to update repos (if no local changes) . Rename branches from main to master . Renaming a branch from github website. . Rename branch main to master from github website . . Update local clones . git branch -m main master git fetch origin git branch -u origin/master master . Rename branches from master to main (I know) . Renaming a branch from github website. . Rename branch master to main from github website . . Update local clones . git branch -m master main git fetch origin git branch -u origin/main main . RabbitVCS . From this page . Installation . sudo apt install rabbitvcs-nautilus . Result . . These overlay icons are not automatically updated (have to hit Ctrl-F5, it is a cache issue?) Which is not a surprise: number of actions are fired based on file modifications, and here status (commited, pushed) is not at all linked to file modifications. The system doesn’t know that overlay icon should be changed because file was not touched. . git-nautilus-icons . Just to check if it works better than RabbitVCS regarding overlay icon cache issue. . No I didn’t manage to make it work. Back to RabbitVCS. . Activate git with GlobalProtect . move from ssh to https, keeping password . $ git remote -v origin git@github.com:castorfou/guillaume_blog.git (fetch) origin git@github.com:castorfou/guillaume_blog.git (push) . move to https://github.com/castorfou/guillaume_blog.git . git remote set-url origin https://github.com/castorfou/guillaume_blog.git . Make Git store the username and password and it will never ask for them. | . git config --global credential.helper store . Save the username and password for a session (cache it); | . git config --global credential.helper cache . and to activate trace . $ GIT_TRACE_PACKET=1 GIT_TRACE=1 GIT_CURL_VERBOSE=1 git fetch . we can enrich certificates with Global Protect CA . ~/anaconda3/ssl$ sudo cp certPG.pem /etc/ssl/certs/ . Add a ca-certificate in ubuntu . Go to /usr/local/share/ca-certificates/ | Create a new folder, i.e. sudo mkdir school | Copy the . crt file into the school folder. | Make sure the permissions are OK (755 for the folder, 644 for the file) | Run sudo update-ca-certificates | We should see effects in /etc/ssl/certs . /etc/ssl/certs$ ll -tr [..] lrwxrwxrwx 1 root root 86 mars 24 10:02 cert_M_X5C_sase-net-sslfwd-trust-ca.pem -&gt; /usr/local/share/ca-certificates/globalprotect/cert_M_X5C_sase-net-sslfwd-trust-ca.crt lrwxrwxrwx 1 root root 39 mars 24 10:02 0dc7de9e.0 -&gt; cert_M_X5C_sase-net-sslfwd-trust-ca.pem .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/clustergit.html",
            "relUrl": "/blog/clustergit.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Introduction to Reinforcement Learning with David Silver",
            "content": "This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL. . Website with 10 lectures: videos and slides . My repo with slides . . 3/9/21 - Lecture 1: Introduction to Reinforcement Learning . This introduction is essentially about giving examples of RL to have a good intuition about this field and to provide definitions or context: . Definitions: rewards, actions, agent, environment, state (and history) | Major components: policy, value function, model | Categorizing RL agents (taxonomy): value based, policy based, actor critic, model free, model based | Learning and planning | Prediction and control | . And David gives 2 references: . well known Introduction to Reinforcement Learning, Sutton and Barto, 1998 | Algorithms for Reinforcement Learning, Szepesvari. Available online. | . Policy π piπ(s): essentially a map from state to action. Can be deterministic π piπ(s) or stochastic π piπ(a|s). . Value function v$ pi$(s): is a prediction of expected future reward. . Model: it is not the environment itself but useful to predict what the environment will do next. 2 types of models: transitions model and rewards model. Transition model predicts the next state (e.g. based on dynamics). Reward model predicts the next immediate reward. . A lot of algorithms are model-free and doesn’t require these models. It is a fundamental distinctions in RL. . . And then David explains 2 fundamental different problems with Learning vs Planning. . With Learning, environment is unknown, agent interacts directly with the environment and improves its policy. . With Planning, a model of environment is known, and agent “plays” with this model and improves its policy. . These 2 problems may be linked where you start to learn from the environment and apply planning then. . 2 examples based on atari games. . Another topic is exploration vs exploitation then prediction and control. . 3/10/21 - Lecture 2: Markov Decision Processes . Markov decision processes formally describe an environment for reinforcement learning. . Markov property: the future is independent of the past given the present. . Markov Process (or Markov Chain) is the tuple (S, P) . . We can take sample episodes from this chain. (e.g. C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep) . We can formalize the transition matrix from s to s’. . When you add reward you get Markov reward process (S, P, R, γ gammaγ) . Reward here is a function to map for each state the immediate reward. . γ gammaγ is the discounted factor, ϵ epsilonϵ [0,1]. David explains why we could need such discount. . Return Gt is the total discounted reward at time-step t for a given sample. . . Value function v(s) is really what we care about, it is the long-term value of state s. . . Bellman Equation for MRPs . The value function can be decomposed into two parts: . immediate reward Rt+1 | discounted value of next state γ gammaγ.v (St+1) | . . We use that to calculate value function with γ gammaγ $ neq$ 0. . And calculating value function can be seen as the resolution of this linear equation: . . And now we introduce actions and it gives Markov Decision Process . . And we introduce policy . . Then we can define the state-value function v$ pi$(s,a) for a given policy π piπ . . and action-value function q$ pi$(s,a) for a given policy π piπ . . And impact on Bellman Equation ends like that: . . v is giving us how good it is to be in a state. q is giving us how good is it to take an action. . And then we have the Bellman equation expressed with v and q. . We don’t care much about a given v$ pi$, we want to get the best policy. And ultimately to get q* which is the optimal action value function. . . The optimal value function specifies the best possible performance in the MDP. A MDP is “solved” when we know the optimal value function q*. . What we really care about is optimal policy π piπ*. There is a partial ordering about policies. And a theorem saying that for any MDP, there exists at least one optimal policy. . So the optimal value function calculation is similar to what we did earlier when we averaged the value of the next state but now we take the max instead of average. . So no we can write the Bellman Optimality Equation. Unfortunately this is non-linear. . There are many approaches such as iterative ones. . Value Iteration | Policy Iteration | Q-learning | Sarsa | . 3/12/21 - Lecture 3: Planning by Dynamic Programming . Will discuss from the agent side: how to solve these MDP problems. . David starts with general ideas on dynamic programming. (programming in a sense of policy) . Value function is an important idea for RL because it sotres valuable information that you can later reuse (it embeds solutions). And Bellman equation gives the recursive decomposition. . Planning by Dynamic Programming . We assume full knowledge of the MDP. Dynamic programming is used for planning in an MDP. With 2 usages: . prediction: given MDP and policy π piπ, we predict the value of this policy v$ pi$. | control: given MDP, we get optimal value function v&amp;ast; and optimal policy $ pi$&amp;ast;. | . And by full MDP it would mean for an atari game to have access to internal code to calculate everything. . We need the 2 aspects to solve MDP: prediction to value policy, and control to get the best one. . Policy Evaluation . Problem: evaluate a given policy π Solution: iterative application of Bellman expectation backup . (Bellman expectation is used in prediction, Bellman optimality is used in control) . David takes an example with a small grid-world and calculates iteratively (k=0, 1, 2, …) v(s) for a uniform random policy (north, south, east, west with prob 0.25) (left column). And then we follow policy greedily using v function. (right column) . Policy Iteration . In small grid-world example, just by evaluating the policy and act greedily were sufficient to get the optimal policy. This is not generally the case. In general, need more iterations of evaluation (iterative policy evaluation) / improvement (greedy policy). But this process of policy iteration always converges to π∗ . David uses Jack’s Car Rental where it needs 4 steps to get the optimal policy. And explains why acting greedy improves the policy. And if improvement stops, Bellman optimality equation is satisfied, we have our optimal policy. . Some question then about convergence of v$ pi$ . Why not update policy at each step of evaluation -&gt; this is value iteration. . Value Iteration . Problem: find optimal policy π Solution: iterative application of Bellman optimality backup . Extensions to dynamic programming . DP uses full-width backups. It is effective for medium-sized problems. Curse of dimensionality for large problems. Even one backup can be too expensive. . One solution is to sample backups. . Advantages: Model-free: no advance knowledge of MDP required Breaks the curse of dimensionality through sampling Cost of backup is constant, independent of n = |S| . 3/15/21 - Lecture 4: Model-Free Prediction . Model-Free: no-one gives us the MDP. And we still want to solve it. . Monte-Carlo learning: basically methods which goes all the way to the end of trajectory and estimates value by looking at sample returns. . | Temporal-Difference learning: goes one step ahead and estimates after one step | TD(λ lambdaλ): unify both approaches | . We give up the assumption giving how the environment works (which is highly unrealistic for interesting problems). We break it down in 2 pieces (as with previous lecture with planning): . policy evaluation case (this lecture) - how much reward we get from that policy (in model-free envt) | control (next lecture) - find the optimum value function and then optimum policy | . Monte-Carlo Reinforcement Learning . We go all the way through the episodes and we take sample returns. So the estimated value function can be the average of all returns. You have to terminate to perform this mean. . It means we use the empirical mean return in place of expected return. (by law of large numbers, this average returns will converge to value function as the number of episodes for that state tends to infinity) . Temporal-Difference Reinforcement Learning . TD learns from incomplete episodes, by bootstrapping . David takes an example from Sutton about predicting time to commute home, comparing MC and TD. . TD target (Rt+1+γ gammaγVt+1) is biased estimate of v$ pi$(St), but has lower variance than the return Gt. . David compares perf of MC, TD(0), … using Random Walk example and different values of α alphaα. . 3/18/21 - Lecture 5: Model-Free Control . Distinction between on-policy (learning by doing the job) and off-policy (following someone else behavior) . on-policy . In Monte-Carlo approach, we have 2 issues. First is that we don’t have access to model so we should use Q(s, a) instead of v(s). Second is lack of exploration so we should use ϵ epsilonϵ-greedy policy. . With GLIE (Greedy in the Limit with Infinite Exploration), we can update Q after each episodes. . We will now use TD: . Natural idea: use TD instead of MC in our control loop . Apply TD to Q(S, A) | Use ϵ epsilonϵ-greedy policy improvement | Update every time-step | . This is SARSA update. Every single time-step we update our diagram. . A generalisation is n-step Sarsa. n=1 is standard Sarsa. n=∞ infty∞ is MC. . To get the best of both worlds, we consider Sarsa(λ lambdaλ). We have a forward version . . And a backward version which allows online experience. Thanks to eligibility traces. . off-policy . Why is this important? . Learn from observing humans or other agents | Re-use experience generated from old policies π 1 , π 2 , …, π t−1 | Learn about optimal policy while following exploratory policy | Learn about multiple policies while following one policy | . We can apply it in importance sampling for off-policy. With Monte-Carlo it is however useless due to high variance. It is imperative to to TD. . We can apply that to Q-learning. We can use greedy slection on target policy π piπ and ϵ epsilonϵ greedy on behaviour policy μ muμ. . 3/31/21 - Lecture 6: Value Function Approximation . How to scale up value function approach. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html",
            "relUrl": "/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Use of gpg under linux",
            "content": "from best ways to encrypt files on linux . gpg . setup the key . gpg --gen-key . and enter a strong passphrase. . export public key . gpg --armor --output mypubkey.gpg --export &lt;E-mail that you registered&gt; . import from windows box . gpg --import mypubkey.gpg . encrypt files from windows box . gpg --output test.txt.gpg --encrypt --recipient &lt;Receiver&#39;s E-Mail ID&gt; test.txt . decrypt files on linux box . gpg --output test.txt --decrypt test.txt.gpg . find + gpg + tmpfs . encrypt from Windows . find . -name &#39;df_76*.csv&#39; -exec gpg --output {}.gpg --encrypt --recipient guillaume.ramelet@michelin.com {} ; . decrypt from Linux . There should be better ways to do it. . Here is my process: . Before starting: call mount_decrypt.sh. It mounts a tmpfs in secured_data/data, and decrypt all gpg files to this directory | | After work is done: call umount_decrypt.sh | gpg_decrypt.sh . #!/bin/bash gpg_name=&quot;$1&quot; src_name=${gpg_name%.*} TARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data echo &quot;gpg decrypt $gpg_name -&gt; $src_name&quot; gpg --output $TARGET_DATA/$src_name --decrypt $gpg_name(base) . mount_decrypt.sh . #!/bin/bash GPG_DEC_CMD=/home/explore/git/guillaume/d059/secured_data/gpg_decrypt.sh TARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data sudo mount -t tmpfs -o size=1G tmpfs $TARGET_DATA cd /media/explore/CHACLEF/janus find . -name &#39;df_76*.csv.gpg&#39; -exec $GPG_DEC_CMD {} ; . umount_decrypt.sh . #!/bin/bash TARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data sudo umount $TARGET_DATA .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gpg-linux.html",
            "relUrl": "/blog/gpg-linux.html",
            "date": " • Mar 3, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Logbook for March 21",
            "content": "Week 9 - Mar 21 . Monday 3/1 . MIT 6S191 Deep Generative Modeling (lecture 4) - vaes and gans. . MIT 6S191 De-biasing Facial Recognition Systems (lab 2): CNN, VAE, DB-VAE . Tuesday 3/2 . College de France Approximations non linéaires et réseaux de neurones (lecture 4) . RL Course by David Silver lecture 1 - intro (22’/88’) . Future of Manufacturing@MIT - interesting landscape about Manufacturing and AI . Wednesday 3/3 . Interpretable Machine Learning by Christoph Molnar. LIME reading to understand context of local surrogate models. SHAP chapter using Janus data. . Deep Reinforcement Learning by Thomas Simonini (Chapter 3 v1) on DQN with temporal limitation using LSTM, and experience replay. (replay buffer) . Thursday 3/4 . Interpretable Machine Learning by Christoph Molnar. PDP chapter using Janus data. . Friday 3/5 . RL - Sutton book (p220-223) - full vs sample backups, trajectory sampling, heuristic search . RL - Sutton book (p223+) - start of Approximate Solution Methods, why to use NN. . Week 10 - Mar 21 . Monday 3/8 . MIT 6S191 Deep Reinforcement Learning. Q-learning vs Policy Gradient. . Tuesday 3/9 . College de France Ondelettes et échantillonnage (lecture 5) . RL Course by David Silver Introduction to Reinforcement Learning (lecture 1) . Installation of clustergit to detect local (=uncommited or unpushed) changes in repo . Wednesday 3/10 . Deep Reinforcement Learning by Thomas Simonini (Chapter 4 v1) on four strategies to improve DQN (fixed Q-targets, double DQN, dueling DQN (DDQN), Prioritized Experience Replay (PER)) . t-SNE using Janus data. . RL Course by David Silver Markov Decision Processes (lecture 2) . Friday 3/12 . RL - Sutton book (p287-352) - Applications and case studies, end of the book . RL Course by David Silver Planning by Dynamic Programming (lecture 3) . Week 11 - Mar 21 . Monday 3/15 . MIT 6S191 Limitations and New Frontiers. . MIT 6S191 Pixels-to-Control Learning (lab 3): Cartpole and Pong . RL Course by David Silver Model-Free Prediction (lecture 4) . Tuesday 3/16 . College de France Multi-résolutions (lecture 6) . Wednesday 3/17 . Deep Reinforcement Learning by Thomas Simonini (Chapter 5 v1) - Policy Gradient . Thursday 3/18 . Deep Reinforcement Learning by Thomas Simonini (Chapter 5 v1) - Policy Gradient notebooks . RL Course by David Silver Model-Free Control (lecture 5) . Friday 3/19 . Deep Reinforcement Learning by Thomas Simonini (Chapter 6 v1) - Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) . College de France - l’apprentissage profond par Yann Lecunn 2016 - Pourquoi l’apprentissage profond ? . Week 12 - Mar 21 . Monday 3/22 . MIT 6S191 Evidential Deep Learning and Uncertainty (lecture 7). . Deep Reinforcement Learning by Thomas Simonini (v1 Part 5) - Advantage Actor Critic (A2C) - implementation and video . Tuesday 3/23 . College de France Bases orthonormales d’ondelettes (lecture 7) . Wednesday 3/24 . Deep Reinforcement Learning by Thomas Simonini (Chapter 7 v1) - Proximal Policy Optimization PPO . Stable baselines 3 - init and 1st tutorial . Thursday 3/25 . setup headless raspberry pi to bridge wifi (tethering from phone) to ethernet (to wifi-router) . Stable baselines 3 - finalize init and go through documentation . Create a patch for a github project (by forking and pulling request) . Friday 3/26 . Stable baselines 3 - Documentation &gt; Examples . Rename my branches named master to main . Week 13 - Mar 21 . Monday 3/29 . MIT 6S191 Bias and Fairness (lecture 8). . Wednesday 3/31 . College de France Parcimonie et compression d’images (lecture 8) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-March.html",
            "relUrl": "/blog/logbook-March.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Logbook for February 21",
            "content": "This is a test. I will try to keep words on a monthly (this page), weekly (per heading), daily basis. Just some short entries with possibly some links to more detailed materials. . Week 8 - Feb 21 . Monday 2/22 . To develop knowledge about RL, here is my learning process on a weekly basis. . Monday MIT 6S191 . Tuesday College de France . Wednesday Deep Reinforcement Learning by Thomas Simonini . Friday RL readings: papers, books, … . Friday 2/26 . blog fastpages - setup automated upgrade (instructions from _fastpages_docs) v2.1.42 . blog fastpages - display image preview (update of _config.yml) . RL - understood differences between Q-learning and Sarsa algorithms in end of step2 part2 . RL - Sutton book (p200-220) - eligibility traces, and start of planning vs learning .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-Februrary.html",
            "relUrl": "/blog/logbook-Februrary.html",
            "date": " • Feb 26, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
            "content": "A course by Thomas Simonini . Syllabus (from 2018) . Course introduction (from 2020) . Everything available in github . I appreciate the effort to update examples, and some 2018 implementations became obsolete. Historical Atari VC2600 games are now Starcraft 2 or minecraft, and news series on building AI for video games in Unity and Unreal Engine.. . (2/19/21) - Chapter 1 - An Introduction to Deep Reinforcement Learning? . Previous version from 2018: What is Deep Reinforcement Learning? is quite interesting. With 3 parts: . What Reinforcement Learning is, and how rewards are the central idea | The three approaches of Reinforcement Learning | What the “Deep” in Deep Reinforcement Learning means | . . Rewards, long-term future reward, discount rate. . . Episodic (starting and ending point) vs Continuous (e.g. stock trading) tasks. . Way of learning: Monte Carlo (MC: rewards collected at the end of an episode) vs Temporal Difference (TD: estimate rewards at each step) . . Exploration/Exploitation trade off. Will see later different ways to handle that trade-off. . . Three approaches to Reinforcement Learning . These are value-based, policy-based, and model-based. . Value Based . In value-based RL, the goal is to optimize the value function V(s). . The value function is a function that tells us the maximum expected future reward the agent will get at each state. . . Policy Based . In policy-based RL, we want to directly optimize the policy function π(s) without using a value function. . The policy is what defines the agent behavior at a given time. . We have two types of policy: . Deterministic: a policy at a given state will always return the same action. | Stochastic: output a distribution probability over actions. | . . Model Based . In model-based RL, we model the environment. This means we create a model of the behavior of the environment. Not addressed in this course. . Deep Reinforcement Learning . In Q-learning, we keep a table of actions to take for each state (based on reward). This can be huge. . Deep Learning allows to approximate this Q function. . . Updated version from 2020 (and video version) . This is a good starting point, well explained. . Reinforcement Learning is just a computational approach of learning from action. . A formal definition . Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback. . Some explanations about observations (partial description) vs states (fully observed envt). Only differs in implementation, all theoretical background stays the same. . Action space where we can distinguish discrete (e.g. fire, up) actions from continuous (e.g. turn 23deg) ones. . Reward part is the same as the one from 2018. With cheese, mouse, maze example. . Episodic and continuous tasks part is the same as the one from 2018. . Exploration/Exploitation trade-off is explained the same way + an additional example taken from berkley - CS 294-112 - Deep Reinforcement Learning course. I want to learn more about this course! . About solving RL problems, it is now presented as 2 main approaches: . policy-based methods | value-based methods | . . And bedore to explain that, nice presentation of what is a policy $ pi$. Solving RL problem is to find that optimal policy: directly with policy-based method, indirectly (through value function) with value-based method. . There is an explanation about different types of policy: deterministic and stochastic. . And that we use deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based). Thomas suggests to go further with deep learning with MIT 6.S191, which is the one (version 2021) I follow these days. . (2/19/21) - Chapter 2 - part 1 - Q-Learning, let’s create an autonomous Taxi . And in video (I like to read + watch the video at the same time) . Here in Step 2 we focus on a value-based method: Q-learning. And what is seen in part 1 and 2: . . Value-based method . Remember what we mean in value-based method . . you don’t train your policy, you define a simple function such as greedy function to select the best association State-Action, so the best action. . Bellman equation . each value as the sum of the expected return, which is a long process. This is equivalent to the sum of immediate reward + the discounted value of the state that follows. . . Monte Carlo vs Temporal Difference . And then an explanation about 2 types of method to learn a policy or a value-function: . Monte Carlo: learning at the end of the episode. With Monte Carlo, we update the value function from a complete episode and so we use the actual accurate discounted return of this episode. | TD learning: learning at each step. With TD learning, we update the value function from a step, so we replace Gt that we don’t have with an estimated return called TD target. (chich is the immediate reward + the discounted value of the next state) | . . It was not clear to me that these methods could be used for policy-based approach. It is now! . (2/24/21) - Chapter 2 - part 2 - Q-Learning, let’s create an autonomous Taxi . But the video is not yet available. . What is Q-Learning? . Q-Learning is an off-policy value-based method that uses a TD approach to train its action-value function: . “Off-policy”: we’ll talk about that at the end of this chapter. | “Value-based method”: it means that it finds its optimal policy indirectly by training a value-function or action-value function that will tell us what’s the value of each state or each state-action pair. | “Uses a TD approach”: updates its action-value function at each step. | . Q stands for quality (quality of action). After training we’ll get the optimal Q-function. . When choosing an action, we have to balance between exploration and exploitation with ϵ epsilonϵ - greedy: . . But at beginning Q table is not trained yet so we have to increase exploitation. It is done with some decreasing ϵ epsilonϵ. . . The Q-learning algorithm is a 4-step process: . step1: Q-Table init | step2: Choose action (ϵ epsilonϵ - greedy strategy) | step3: Perform action At and get Rt+1 and St+1 | step4: Update Q(St, At) | . . Why it is called off-policy? Because we don’t have the same logic to select action (ϵ epsilonϵ - greedy) and update Q (greedy). . With On-policy: we use the same policy for acting and updating. Sarsa is such an algorithm. . . Nice and simple manual example with mouse, cheese in a maze. We run Q-learning and make all calculation by hands. . . implement with numpy+gym this algorithm should be a nice exercise. There is an exercise to implement a taxi, within this notebook at colab google. Taxi V3 is an env from opengym. . (3/3/21) - back to 2018 - Chapter 3 - Deep Q-learning with Doom . Article, Notebook, Video . We’ll create an agent that learns to play Doom. Doom is a big environment with a gigantic state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient at all. . The best idea in this case is to create a neural network that will approximate, given a state, the different Q-values for each action. . . . Addresses pb of temporal limitation: get multiple frames to have sense of motion. . Video is nice because it goes from start and follows closely all steps. . I wil try to implement in my own by creating an environment and running under a clone of Deep_reinforcement_learning_Course Thomas’s repo . Here at Deep Q learning with Doom.ipynb . I had to switch to tensorflow-gpu 1.13. Manage some cuda memory issue. But then was able to run it. . However as Thomas says, I should do it step by step on my own. . (3/10/21) - Chapter 4: Improvements in Deep Q Learning V1 . Article, Notebook, Video . four strategies that improve — dramatically — the training and the results of our DQN agents: . fixed Q-targets | double DQNs | dueling DQN (aka DDQN) | Prioritized Experience Replay (aka PER) | . fixed Q-targets to avoid chasing a moving target . Using a separate network with a fixed parameter (let’s call it w-) for estimating the TD target. | At every T TauT step, we copy the parameters from our DQN network to update the target network. | . . Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed… . Implementation . Implementing fixed q-targets is pretty straightforward: . First, we create two networks (DQNetwork, TargetNetwork) . | Then, we create a function that will take our DQNetwork parameters and copy them to our TargetNetwork . | Finally, during the training, we calculate the TD target using our target network. We update the target network with the DQNetwork every T TauT step (T TauT is an hyper-parameter that we define). . | . double DQNs to handle overestimating of Q-values (at the beginning of training, taking the maximum q value (which is noisy) as the best action to take can lead to false positives) . we move from this TD target logic . . to the use of 2 networks . use our DQN network to select what is the best action to take for the next state (the action with the highest Q value). | use our target network to calculate the target Q value of taking that action at the next state. | . . Implementation . . Dueling DQN (aka DDQN) . based on this paper Dueling Network Architectures for Deep Reinforcement Learning. . With DDQN, we want to separate the estimator of these two elements, using two new streams: . one that estimates the state value V(s) | one that estimates the advantage for each action A(s,a) | . . and this can be combined with Prioritized experience replay. . This is nicely explained in this article. DDQN explanation is clearer than Thomas’. . The key here is to deal efficiently with experiences. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay (PER) is one strategy that tries to leverage this fact by changing the sampling distribution. . I guess there are several options to manage this prioritization (we would prefer transitions that do not fit well to our current estimate of Q function). And a key aspect is the performance of this selection. One implementation is SumTree. . I have to see full implementation in the notebook to fully understand the logic. . About the video . Thomas has insisted about the importance to master these architecture (DQN then DDQN, etc) before going further with state of the art architectures (Policy Gradient, PPO…) . Approach in videos is now different. In previous videos it was about explaining articles. Now it is more turned to implementation details based on notebooks. . Thomas has given a reference to Arthur Juliani who is a senior ML engineer at Unity. I would like to browse though this reference and see what can be done. . Should follow video and run/update notebook in //. . (3/17/21) - Chapter 5: Policy Gradients V1 . Article, Notebook, Video . In policy-based methods, instead of learning a value function that tells us what is the expected sum of rewards given a state and an action, we learn directly the policy function that maps state to action (select actions without using a value function). . 3 main advantages to use Policy Gradients vs Q learning: . convergence - have better convergence properties | effective in high dimension, or with continuous actions | stochastic policy - no need for exploration,/exploitation tradeoff | . But can be longer to train. . Policy search . We can dfine our policy as the probability distribution of actions (for a given state) . . And how good is this policy? Measured with J(θ thetaθ) . . We must find θ thetaθ to maximize J(θ thetaθ). How? . 2 steps: . Measure the quality of a π (policy) with a policy score function J(θ) | Use policy gradient ascent to find the best parameter θ that improves our π. | . the Policy Score function J(θ) . 3 ways (maybe more) . Calculate the mean of the return from the first time step (G1). This is the cumulative discounted reward for the entire episode. . . In a continuous environment, we can use the average value, because we can’t rely on a specific start state. Each state value is now weighted (because some happen more than others) by the probability of the occurrence of the respected state. . . Third, we can use the average reward per time step. The idea here is that we want to get the most reward per time step. . . Policy gradient ascent . because we want to maximize our Policy score function . . The solution will be to use the Policy Gradient Theorem. This provides an analytic expression for the gradient ∇ of J(θ) (performance) with respect to policy θ that does not involve the differentiation of the state distribution. (using likelihood ratio trick) . . It gives . . R(τ tauτ) is like a scalar value score. . Implementation . As with the previous section, this is good to watch the video at the same time. . And now this is the implementation in . doom deathmatch notebook . . as with Pong, we stack frames to understand dynamic with deque. . Even with GPU growth setup, I run an error after the 1st epoch. . ========================================== Epoch: 1 / 5000 Number of training episodes: 15 Total reward: 7.0 Mean Reward of that batch 0.4666666666666667 Average Reward of all training: 0.4666666666666667 Max reward for a batch so far: 7.0 . ResourceExhaustedError: OOM when allocating tensor with shape[5030,32,24,39] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [[]] Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. . I have to reduce batch size (to 1000) to make it work. . And I can monitor gpu memory consumption with watch nvidia-smi . . or we can use gpustat -i 2 . [0] Quadro RTX 4000 | 59’C, 34 %, 39 W | 7819 / 7982 MB | explore(6729M) gdm(162M) explore(388M) explore(282M) explore(86M) explore(89M) explore(3M) | . (3/19/21) - Chapter 6: Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) V1 . Article, Notebook, Video . “hybrid method”: Actor Critic. We’ll using two neural networks: . an Actor that controls how our agent behaves (policy-based) | a Critic that measures how good the action taken is (value-based) | . . Actor is using a policy function π(s,a,θ) pi(s, a, theta)π(s,a,θ) Critic is using a value function . q^(s,a,w) widehat{q}(s,a,w)q​(s,a,w) Which means 2 sets of weights to be optimized separately θ thetaθ and w. . . We can use advantage function to stabilize learning: . . Two different strategies: Asynchronous or Synchronous . We have two different strategies to implement an Actor Critic agent: . A2C (aka Advantage Actor Critic) | A3C (aka Asynchronous Advantage Actor Critic) | . Here we focus on A2C. . (3/22/21) - Implementation and video . It is a little bit confusing. I won’t run it. I would have liked a more pregressive approach and to understand all steps Thomas did to get to that final implementation. . (3/24/21) - Chapter 7: Proximal Policy Optimization PPO V1 . Article, Notebook . The central idea of Proximal Policy Optimization is to avoid having too large policy update. (we use a ratio that will tells us the difference between our new and old policy and clip this ratio from 0.8 to 1.2) . Clipped Surrogate Objective Function . . We will penalize changes that lead to a ratio that will away from 1 (in the paper ratio can only vary from 0.8 to 1.2). By doing that we’ll ensure that not having too large policy update because the new policy can’t be too different from the older one. . 2 implementations are known TRPO (Trust Region Policy Optimization) and PPO clip. TRPO being complex and costly, we focus on PPO: . . And the final loss will be: . . Now the implementation . By looking at the implementation, I ran into Stable baselines3. . This is a major update of Stable Baselines based on pytorch. It seems interesting! . I like this comment from Stable Baselines3 in the v1.0 blog post: . Motivation . Deep reinforcement learning (RL) research has grown rapidly in recent years, yet results are often difficult to reproduce. A major challenge is that small implementation details can have a substantial effect on performance – often greater than the difference between algorithms. It is particularly important that implementations used as experimental baselines are reliable; otherwise, novel algorithms compared to weak baselines lead to inflated estimates of performance improvements. . To help with this problem, we present Stable-Baselines3 (SB3), an open-source framework implementing seven commonly used model-free deep RL algorithms, relying on the OpenAI Gym interface. . I will create a new blog entry about Stable Baselines3. . as for previous notebook, I need to purchase Sonic2-3 to make it worked. Not for now maybe later. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html",
            "relUrl": "/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Conda and jupyter tips",
            "content": "Keeping track of python environments . I manage all my python environments with conda from miniconda. . manual way . However I don&#39;t have a strong process to keep track of my environment specifications. Usually I manually create an env.txt file under my projects. Keeping all commands I have used to create that environment. . !cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/env mit_6S191.txt . env_name: mit_6S191 libraries: python 3.7, tensorflow 2 Installation commands: conda create -n mit_6S191 python=3.7 conda activate mit_6S191 conda install tensorflow tensorflow-gpu conda install -c conda-forge jupyter_contrib_nbextensions conda install matplotlib numpy opencv conda install -c pytorch torchvision conda install nb_conda . What happens if I add packages in that environment. Or want to use that environment in another project. I have to remember the link between env name and project name. . That is not robust. . yml way . Keeping a yml file could be a solution to keep track of environment specifications. It doesn&#39;t answer to my last concern though (linking env name and project name) . But there is a limitation linked with channels. . !conda env export --from-history . name: fastai channels: - defaults dependencies: - python=3.8 - fastai - jupyter - jupyter_contrib_nbextensions - fastbook prefix: /home/explore/miniconda3/envs/fastai . In that example, fastai package should come from fastai channel but conda doesn&#39;t keep that information. . Using . conda install -n my_env rdkit::rdkit . could be an option. . automate yml way . Since conda keeps active environment in env variable CONDA_DEFAULT_ENV, we can automatically create up-to-date yml file. . !echo $CONDA_DEFAULT_ENV . fastai . !conda env export --from-history &gt; ~/temp/env_`echo $CONDA_DEFAULT_ENV`.yml !ls ~/temp/env_`echo $CONDA_DEFAULT_ENV`.yml . /home/explore/temp/env_fastai.yml . But for it to be usable, I will have to install package using the &lt;channel&gt;::&lt;package&gt; way. . !cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/create_yml.sh . #!/bin/bash conda env export --from-history &gt; env_`echo $CONDA_DEFAULT_ENV`.yml . Conda commands . When managing conda environments, I very often fall on this documentation page which is simply great: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html . Next time I visit this page, I will enter entries here to track my common commands. . Jupyter installation . Jupyter extensions . I have already explained how to install jupyter extensions and the one I use. update jupyter to include extensions) . nb_conda . This is usefull to switch from environment to another without having to stop/restart jupyter. . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/conda-and-jupyter-tips.html",
            "relUrl": "/blog/conda-and-jupyter-tips.html",
            "date": " • Feb 16, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Learning: Collège de France - Représentations parcimonieuses",
            "content": "Un exposé en 8 cours au collège de France de Stéphane Mallat sur les représentations parcimonieuses - 2021. . Cela donne envie d’aller voir ses autres cours: . 2018: L’apprentissage face à la malédiction de la grande dimension | 2019: L’apprentissage par réseaux de neurones profonds | 2020: Modèles multi-échelles et réseaux de neurones convolutifs | . A peu près 16 vidéos de 1h30 par cours. Et des notes de cours en pdf. . 2/15/21 - Le triangle « Régularité, Approximation, Parcimonie » (lecture 1) . C’est l’introduction du cours. J’apprécie les références historiques et philosphiques partant du rasoir d’Ockam. C’est le principe d’économie ou de parcimonie: le beau, le vrai viendrait du simple. . La 1ere fois que j’entends une référence précise sur l’opposition entre biais (erreur sur modèle) et variance (erreur sur données ou mesures) . Et une invitation à consulter une méthodologie d’analyse de données par Pierre Courtiol en utilisant Kaggle. L’idée d’une approche simple linéaire pour bien comprendre quelles étapes successives à emprunter pour améliorer son approche. Me semble assez orthogonal à ce que peut proposer Jeremy Howard: commencer tôt, overfitting n’est pas un probleme, pas de early stopping, etc. . 2/10/21 - Approximations linéaires et analyse de Fourier (lecture 2) . J’ai commencé par ce cours conseillé par Rémi mon pote enseignant chercheur en math. C’est un peu le grand écart avec des méthodes d’enseignement anglo-saxonnes mais ça fait du bien. C’est finalement plus proche de ce que j’ai connu dans ma formation initiale. . S.Mallat présente les équivalences (sous certaines conditions) entre . Régularité | Approximation en basse dimension | et représentation parcimonieuse | . dans le cadre des approximations linéaires. Il parle des 2 mondes: traitement du signal et analyse de la donnée. Je suis moins intéressé par le 1er monde, mais j’apprécie la piqure de rappel. Je ne me rappelais pas du tout l’importance de l’analyse de Fourier et la construction des bases de L[0,1] par exemple. . Et il revient sur les singularités, beaucoup d’informations sont portées par les singularités (par exemple les frontières dans une image) . Je crois bien que je vais me faire toute la session, et sans doute les autres années. . 2/23/21 - Grande dimension et composantes principales (lecture 3) . Dans ce cadre linéaire grande dimension, quelle meilleure base - approche PCA et base Karhunen-Loeve. . Quid quand on passe en non linéaire. . Réseau neurone à 1 couche cachée, théoreme de representation universel. . Retour sur les bases de L²[0,1] qui sont les bases de Fourier en variables complexes. . Pour un passage en dimension q, on remplace n par (n1, …, nq) et la multiplication n*u par le produit scalaire &lt;n, u&gt;. . En travaillant sur les équivalences du triangle, il montre pourquoi on est très limité en approximation lineaire quand la dimension augmente. . En approximation lineaire, il suffit de prendre les 1ers vecteurs (se limiter à une dimension q) (en base de fourier par exemple) pour avoir une assez bonne approximation. Dans des signaux plus perturbés (avec des singularités) on perd plus d’énergie: il faudrait échantilloner plus fin dans ces zones de singularités et si on dispose d’une base orthonormée il s’agirait non plus de prendre les q 1ers vecteurs mais de prendre ceux d’intéret. . 3/2/21 - Approximations non linéaires et réseaux de neurones (lecture 4) . Le triangle (approximation basse dimensions, représentation parcimonieuse, régularité) d’un point de vue non linéaire. . Ici plutôt qu’approximer un signal en prenant les M 1ers coefficients de Fourier (basses dimensions), on va prendre M coefficients mais dépendamment de x. C’est ici qu’on introduit la non-linéarité. L’erreur est alors la queue de distribution des coefficients ordonnés. On veut que l’énergie des plus petits coefficients soit négligeable. . Pas facile d’obtenir cet ordre, on cherche une façon de limiter les coefficients non ordonnés nous donnant une représentation parcimonieuse. En utilisant la nome l$ alpha$ avec α alphaα petit (inférieur à 2 et proche de 0), on introduit cette décroissance mais cette fois-ci sur les coefficients non ordonnés. . Intéressant d’avoir des normes convexes, et dans ce cas on ne peut prendre que α alphaα=1. C’est pour ça qu’on voit apparaître partout les normes l1 dans les algorithmes d’apprentissage (norme convexe garantissant une forme de sparsité). . On passe aux réseaux de neurones à 1 couche cachée. Et on va basculer dans les notations de x(u) à f(x)., avec x ϵ epsilonϵ [0, 1]d. . . Ici on projette f dans l’espace engendré par ces vecteurs { ρ rhoρ(x.wm+bm) }n&lt;=M. . On peut facilement calculer l’erreur quadratique comme l’intégrale sur les x ϵ epsilonϵ [0, 1]d de la norme l² ( f(x)-ftilde(x) ) et il y a un belle démonstration qui est le théorème d’approximation universelle (démontrée entre 1988 et 1992) qui montre que l’erreur tend vers 0 quand M tend vers l’infini. . La démonstration avec ρ rhoρ = eia revient à une décomposition d’en Fourier. Et pour d’autres non régularité comme reLu ou sigmoid, il s’agit d’un changement de base. . Et là on arrive à la malédiction de la dimensionnalité car quand d est grand (disons 1M), les coefficients baissent à une faible vitesse. Que faut-il faire pour battre cette malédiction? . Baron en 1993 introduit une hypothèse de regularité qui permet de borner l’erreur par un terme qui ne dépend pas de la dimension. C’est donc gagné sauf que l’hypothèse de régularité n’est généralement pas valide dans les cas qui nous intéressent. . Stéphane Mallat, de façon brillante mais est-ce étonnant, explique pourquoi l’approche des mathématiciens est une impasse et pourquoi ce qu’on cherche à faire se ramène à un problème bayésien. Car les problèmes qui nous intéressent (par exemple la classification d’objets, ne va solliciter qu’un minuscule espace (même si de grande dimension) parmi toutes les images possibles). On va donc chercher à caractériser x pour chaque y (classe). (revoir vidéo entre 49’ et 1h03) . L’enjeu est de caractériser le support qui est beaucoup plus concentré que [0,1]d. . Donc on va retravailler sur les approximations non linéaires de x, le signal lui-même (et non plus f), et d’essayer de comprendre pourquoi on peut faire beaucoup mieux que la transformée de Fourier et quelle genre de bases vont nous permettre de faire bcp mieux. Une des applications va être la compression, qui va nous amener à étudier la théorie de l’information et la théorie de l’information c’est exactement la théorie probabiliste qui explique ces phénomènes de concentration et les mesure avec l’entropie. . Introduction des bases d’ondelettes qui vont permettre de représenter les singularités locales. Les ondelettes sont à la fois localisées (paramètre v) et dilatées (paramètre s). Il faudra à partir de ces ondelettes construire des bases orthogonales pour arriver à des approximations basses dimensions (et garder les grands coefficients) . On introduit la notion de régularité locale exprimée avec lipchitz α alphaα. Avec α alphaα &lt;1 pour exprimer les singularités. . 3/9/21 - Ondelettes et échantillonnage (lecture 5) . On était resté sur une représentation de signaux qui ne présentent pas de régularité uniforme mais qui présentent des singularités que nous voulons capter, ces singularités étant porteuses d’informations importantes (par exemple les contours dans une image). Ces singularités n’étant pas très nombreuses, on peut toujours parler de régularité locale. . On va donc utiliser des ondelettes pour décomposer ces signaux, d’où la notion de représentation parcimonieuse, exprimée sur la base d’ondelettes orthonormales. Et enfin en en sélectionnant un petit nombre nous revenons sur nos approximations en basse dimension. . Le produit scalaire du signal x(u) par l’ondelette ψ psiψv,s revient à un produit de convolution de x par l’ondelette conjuguée. Ca veut dire que sur les points de singularités les produits scalaires vont être maximisés. . Stéphane Mallat passe un long moment pour nous amener à la construction de ces bases d’ondelettes orthonormales. Il part des bases de Haar puis de Shannon et arrive à une construction plus récente par Yves Meyer en 1986. . 3/16/21 - Multi-résolutions (lecture 6) . On a vu la dernière fois qu’on pouvait construire une base d’ondelette le long des indices de dilatations en 2j. . On va voir maintenant qu’on peut translater les ondelettes par des facteurs 2j.n. . Donc quand j est grand, les échelles sont de plus en plus grande. Et j petit va amener un échantillonnage de plus en plus fin. . {Ψ(j,n)(u)=12jΨ(u−2jn2j)}(j,n)ϵZ2 left { Psi_{(j,n)}(u)= frac{1}{ sqrt{2^j}} Psi left( frac{u-2^jn}{2^j} right) right }_{(j, n) epsilon Z^2}{Ψ(j,n)​(u)=2j . ​1​Ψ(2ju−2jn​)}(j,n)ϵZ2​ . sont-elles des bases orthonormales. Ensuite on appliquerait les techniques d’approximations consistant à éliminer les petits coefficients. . Les multi-résolutions sont des espaces linéaires sur lesquels nous allons projeter ces signaux. On va chercher à réduire les dimensions (par ex d’une image) en projetant sur ces espaces emboîtés. Et conserver le maximum d’information. . Un produit scalaire avec une fonction translatée peut toujours s’écrire comme un produit de convolution (Stéphane Mallat répète souvent cette propriété) . Stéphane Mallat fait ensuite le lien avec les algorithmes en bancs de filtre (cascades de filtrage + échantillonnage). . Dans ces opérations il y a sans arrêt des passages du continu au discret. Par exemple si je prends un signal et que je le projette sur ces espaces je me retrouve avec les coordonnées, qui sont les produits scalaires avec mes ϕ phiϕj,n (car base orthogonale), ce qui revient à filtrer et sous échantillonner. . 3/23/21 - Bases orthonormales d’ondelettes (lecture 7) . On repart sur notre triangle. Depuis 2 cours on est sur l’approximation basse dimension. . Stéphane Mallat applique le théorème sur des cas particuliers de la base de Haar, puis de la base de Shannon. Et revient sur la construction d’une base orthonormales avec des ondelettes “optimales”. . . Quand on prend le produit scalaire de notre signal f avec les ondelettes, on obtient des résultats presque nuls lorsque le signal est régulier. Et plus on a de moments nuls avec nos ondelettes, plus la régularité est ignorée (l’approximation par projection sur un espace vectorielle des monômes à l’ordre n). . On va cascader les projections aj (et les détails dj), et ça va revenir à cascader les filtres (les coefficients et les ondelettes). . Pour cela on calcule les valeurs des aj et dj en fonction de aj-1. On montre que cela s’obtient en filtrant (respectivement avec les h‾ overline{h}h et g‾ overline{g}g​) puis en sous-échantillonnant. En cascadant on obtient une série de filtrages, sous-échantillonnages, filtrages, sous-échantillonnages, , etc. . Les filtrages sont des convolutions. Si h a un support compact, ça va réduire le temps de calcul.(le nombre d’éléments non nuls correspond à la taille du filtre). Le nombre d’opérations pour passer de aL à aL-1, dL-1 est N*2m (où N: nombre de coefficients de aL et m est le nombre de moments nuls) . Le nombre d’opérations est linéaire, et la constante correspond à la taille des filtres. . On peut inverser cet algorithmes (car base o.n.) et la structure emboîtée va nous donner algorithme de reconstruction. On va sur-échantillonner (augmenter d’un facteur 2 en intercalant des 0) et appliquer les filtres g et h, et sommer pour obtenir le résultat. . Donc en gardant la base fréquence aJ et tous les détails {dj}, on reconstitue aL. (les signaux sur des grilles de plus en plus fines) . Stéphane Mallat finit sur des exemples en 2 dimensions. En 2 dimensions on aura 3 ondelettes à chaque échelle (1 avec les hautes fréquences dans une direction, 1 avec les hautes fréquences dans l’autre direction, et la dernière avec haute fréquence sur les 2 directions (les coins)). . 3/30/21 - Parcimonie et compression d’images (lecture 8) . Stéphane Mallat propose un survol de tout le cours pour montrer la logique dans laquelle on a évolué. . En reprenant le triangle Régularité - Approximation en basse dimension (au cœur du traitement de donnée) - Représentation parcimonieuse. Les équivalences entre régularité et la construction de représentations parcimonieuses permettent de construire des approximations en basse dimension. . Mais on peut les interpréter différemment : . d’un point de vue linéaire : on peut construire des approximations linéaires qui vont correspondre à des formes de régularité et certains types de représentations parcimonieuses (en particulier dans la base de Fourrier quand on a des invariants par translation) | en prenant un point de vue non linéaire : qui consiste non pas à faire des projections dans des espaces linéaires mais plutôt des projections dans des unions d’espaces linéaires obtenus en sélectionnant de façon libre dans une base orthogonale les plans les plus représentatifs. | . Il reprend en détail ce qu’on a vu en repartant de la théorie développée par Fourier (1822 ça ne date pas d’hier). Et reprend les réseaux de neurones à 1 couche cachée. . fM(x)=∑mw(m)ρ(⟨x,wm⟩+bm)f_M(x)= displaystyle sum_{m}w(m) rho( langle{x,w_m} rangle+b_m)fM​(x)=m∑​w(m)ρ(⟨x,wm​⟩+bm​) . L’entrée est x en dimension d, dans la première couche on calcule des produits scalaires avec les vecteurs $v_m$ qui sont les colonnes d’un opérateur linéaire $W_1$ et ces M produits scalaires vont être regroupés avec un relu (ou toute autre non-régularité) et un biais, et dans la dernière couche on fait une combinaison linéaire pour construire l’approximation. M est le nombre d’éléments dans la couche cachée, peut-on bien approximer f(x) à partir de cette construction ? . Ces réseaux, en prenant comme non-régularité un cosinus, nous font retomber sur des séries de Fourier. . fM(x)=∑∥vm∥&lt;Rw(m)cos⁡(⟨x,wm⟩+bm)f_M(x)= displaystyle sum_{ | v_m |&lt;R}w(m) cos ( langle{x,w_m} rangle+b_m)fM​(x)=∥vm​∥&lt;R∑​w(m)cos(⟨x,wm​⟩+bm​) . Faire une décomposition avec un réseau de neurone à 1 couche cachée est très similaire à décomposer la fonction dans une base de Fourier. Prendre un relu consisterait à faire un changement de base entre le relu et le cosinus. . Si on veut approximer une fonction uniformément régulière, il va falloir garder les basses fréquences. Mais $x$ n’est pas en dimension 1 mais en dimension $d$. Les fréquences qu’il va falloir prendre ici sont dans $ Z^d$, il va falloir garder toutes les fréquences dans une boule de rayon plus petit que $R$. Mais quand on est en dimension $q$, le nombre d’éléments dans une boule plus petit que $R$ va croître comme $R^q$. Donc il va falloir garder énormément d’éléments. . On a la possibilité d’approximer n’importe quelle fonction dans $L^2$ avec une erreur qui va décroître vers 0 quand le nombre de termes $M$ tend vers $ infty$ parce qu’on a une base orthogonale et donc n’importe quelle fonction peut être représentée à partir de la base . f∈L2  ⟹  lim⁡M→∞∥f−fM∥=0f in L^2 implies lim limits_{M to infty} | f-f_M |=0f∈L2⟹M→∞lim​∥f−fM​∥=0 . C’est le théorème d’approximation universelle. . Par contre si on a une régularité on peut préciser la vitesse de décroissance de l’erreur et en particulier si ma fonction est $ alpha$ dérivée dans un espace de Sobolev de degré $ alpha$, l’erreur va décroître d’autant plus vite que la régularité est grande, parce que les coefficients de Fourier vont décroître, et la vitesse de décroissance dépend de $ alpha/d$. . f∈Hα  ⟹  ∥f−fM∥=o(M−α/d)f in H^ alpha implies |f-f_M | = o(M^{- alpha/d})f∈Hα⟹∥f−fM​∥=o(M−α/d) . C’est la malédiction de la dimensionnalité. . Une autre approche consiste à reprendre ce cercle d’un point de vue non-linéaire. Au lieu de toujours prendre les mêmes coefficients pour approximer les fonctions qui m’intéressent, je vais adapter les coefficients à la fonction. C’est l’esprit des approximations non-linéaires. . Si je considère les vecteurs de Fourier, et ses coefficients ont une norme $L^p$ qui converge, pour un $p&lt;2$. Alors on a vu que les coefficients vont décroître à une vitesse qui dépend de $p$. Ca veut dire qu’il y a quelques grands coefficients et beaucoup de petits. Si on choisit les grands coefficients alors on va avoir une erreur qui décroît comme $-2/(p+1)$, l’erreur décroît lorsque $M$ augmente, indépendamment de la dimension. . SparseFouriercoefficients:Barronp&lt;2∑v∈Zd∣⟨f(x),Fv(x)⟩∣p&lt;∞  ⟹  ∥f−fM∥=o(M−2/p+1)Sparse quad Fourier quad coefficients: Barron quad p&lt;2 displaystyle sum_{v in Z^d} | langle{f(x), F_v(x)} rangle|^p &lt; infty implies |f-f_M |=o(M^{-2/p+1})SparseFouriercoefficients:Barronp&lt;2v∈Zd∑​∣⟨f(x),Fv​(x)⟩∣p&lt;∞⟹∥f−fM​∥=o(M−2/p+1) . no curse. Mais résultat tautologique. Pourquoi cette fonction serait approximable avec quelques coefficients de Fourier. Ça n’explique en rien pourquoi on peut améliorer fortement ce résultat en augmentant le nombre de couche. C’est simple mais ça n’explique pas les performances des réseaux de neurones profonds. . D’où l’approche par ondelettes. . Et la nécessité de construire des bases orthogonales d’ondelettes à décroissance rapide. Travaux de Yves Meyer. (en essayant de démontrer que ça n’était pas possible il a réussi à en construire ;)) Et S.Mallat a amélioré cette approche en se basant sur des approches de multi résolutions avec des espaces imbriqués. . On peut construire ces ondelettes en cascadant des filtres à différentes échelles (passe bas et passe bande à différentes échelles). . I.Daubechies a montré qu’on peut construire des ondelettes à support compact. . Y.Meyer a montré ce que ça donnait en dimension 2 (et c’est généralisable en dimension q) avec 3 ondelettes. . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html",
            "relUrl": "/blog/college-de-france-representations-parcimonieuses.html",
            "date": " • Feb 10, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
            "content": "From http://introtodeeplearning.com/ . I keep all content (lectures, notebooks) in github . This is done with google contribution, and therefore all examples are in tensorflow. I will try to adapt notebooks in PyTorch. . 2/5/21 - Intro to Deep Learning - lecture 1 . Lecturer: Alexander Amini . Intro is just jaw-dropping! . 2020 intro was top. . 2021 intro is just awesome. . It is a standard overview of simple deep learning concepts: Perceptron, multi-perceptron, dense layers, loss, gradient-descent, backprop, SGD, regularization, dropout, early stoppping . 2/15/21 - Deep Sequence Modeling - lecture 2 . New lecturer: Ava Soleimany . Nice introduction to sequence modeling with Many-to-One, One-to-Many, Many-to-Many. . RNN and implementation in TensorFlow. And NLP examples: next word problem. (and NLP concepts such as Vocabulary, Indexing, Embedding) . And what we need for sequence modeling: . handle variable-length sequences | track long-term dependencies | maintain information about order | share parameters across the sequence | . Backpropagation through time and problem of exploding/vanishing gradients. . Against exploding: gradient clipping. Against vanishing: 3 ways explained - activation functions, weight init, network arch. . Gated cell: to control what information is passed through. Ex: LSTM Long Short Term Memory. They support something closed to Forget Store Update Output. Ava explains graphically which part of LSTM cells is providing which function. . And then examples: Music generation (to generate 4th movement of last symphony from Schubert!), sentiment classification, machine translation (with Attention mechanisms which provide learnable memory access to solve Not long memory), trajectory prediction, environmental modeling. . 2/16/21 - Intro to TensorFlow; Music Generation - software lab 1 . As an exercise I have completed labs in TensorFlow and adapted them in PyTorch. . With LSTM, I ran into this error: UnknownError: Fail to find the dnn implementation. [Op:CudnnRNN] . Which is solved by calling tf.config.experimental.set_memory_growth. . import tensorflow as tf gpus = tf.config.list_physical_devices(&#39;GPU&#39;) if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(&#39;GPU&#39;) print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;) except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) . Music lab is nice to play with. I am not sure I would be able to convert to PyTorch. It would require time! . 2/22/21 - Deep Computer Vision - lecture 3 . I have never been a big fan of computer vision. . I like the idea developed by Alexander Amini about hierarchy of features. (low level: edges, spots; mid level: eyes, noses) . And how he explains limitation of FC layers for visual detection, and introduction of spatial structure (feature extraction with convolutions) . Some nice examples of hand-engineered convolution filters for different needs: sharpen, edge detect, strong edge detect. . Then classic explanations of CNN with convolution, max pooling. . I like the way classification problems are broken down between feature learning (convolution+relu, pooling, repeated several times) and classification (flatten, FC, softmax) which is a task learning part. . The second part (task learning part) can be anything: classification, object detection, segmentation, probabilistic control, … . . Nice explanation of R-CNN to learn region proposals. . Introduction to Software lab2: de-biaising facial recognition systems. . 3/1/21 - Deep Generative Modeling - lecture 4 . From pattern discovered from data (underlying structure of the data), generate examples following these patterns. . Autoencoder: foundational generative model which builds up latent variable representation by self-encoding the input. To train such network, we create a decoder to go from latent variable to generated output, and then compare input to generated output. . . Variational autoencoder (vae): with vae we try to encode inputs as distributions defined by mean μ muμ and variance σ sigmaσ. And we want to achieve continuity and completeness: . continuity: points that are close in latent space –&gt; similar content after decoding | completeness: sampling from latent space –&gt; ‘meaningful’ content after decoding | . Regularization is pushing to get these properties. . . And the learning process is about minimizing reconstruction loss + a regularization term: . . Ava is then explaining the smart trick to allow backpropagation to happen. Indeed by introducing stochastic term in the sampling layer, we are breaking the backpropagation logic. . We are moving z from a normal distribution to μ muμ+σ sigmaσ.ϵ epsilonϵ where ϵ epsilonϵ follow a normal distribution of mean 0, std 1. . Explanation then of space disentanglement via β betaβ-VAEs. It allows latent variables to be independent. . . And then some introduction about *GANs (Generative Adversarial Network) which are a way to make a generative model by having 2 neural networks (generator and discriminator) compete with each other. . And share some recent advances on GAN such as StyleGAN(2), conditional GAN, CycleGAN. CycleGAN is famous for turning horses in zebras, but it can be used to transform speech as well (used in the synthesis of Obama’s voice) . . 3/1/21 - De-biasing Facial Recognition Systems - Software Lab 2 . Part 1 MNIST . starts with FC layers. With some overfitting but a good accuracy of 96%. . then move to a CNN architecture. I ran into gpu issues. Accuracy is now 99%. . I didn’t manage to make the last part working. (using tape.gradient) . Part 2 Debiasing . Fit a CNN model to classify faces based on celebA dataset. And see the bias effect by predicting on Fitzpatrick scale skin type classification system. . Use VAE to learn latent structure. . . To then debias using DB-VAE model. . . There is a lack of progressive unit tests to validate each step. Cannot go to the end. . Would be interested to see how to apply to non computer vision problems. . 3/8/21 - Deep Reinforcement Learning - lecture 5 . Q-function captures the expected total future reward an agent in state s can receive by executing a certain action a. . Distinction between Value Learning (learn Q function) and Policy Learning (find directly π piπ(s)). . . Value Learning or DQN . . . The key thing is about handling of continuous actions. . Let’s see how to do it with policy learning: . Policy learning or Policy Gradient (PG) . . . Alexanders ends the lecture by discussing about Deepmind progress: . alphaGo - 2016: with a pretrain in supervised mode then standard DRL | alphaGo Zero - 2017: standard DRL without pretraining | alphaZero - 2018: standard DRL without pretraining and applied to several games (Go, Chess, Shogi) | MuZero - 2020: learns the rules of the game by itself, create unknown dynamics | . 3/15/21 - Limitations and New Frontiers - lecture 6 . Universal Approximation Theorem: A feedforward network with a single layer is sufficient to approximate, to an arbitrary precision, any continuous function. . Ava emphasizes importance of training data (e.g. for generalization) and mentions a paper called “Understanding Deep Neural Networks Requires Rethinking Generalization”. . Some fail examples with dogs colorization (BW -&gt; colors) creating pink zone under the mouth. . And another one with Tesla autopilot. It motivates working on uncertainty in Deep Learning. . we need uncertainty metrics to assess the noise inherent to the data: aleatoric uncertainty | we need uncertainty metrics to assess the network’s confidence in its predictions: epistemic uncertainty | . Ava cites an example of a real 3D printed turtle designed to fool a classifier from turtle to rifle. . New frontier: Encoding Structure into Deep Learning. . CNN is a nice way to extract features from an image. But not all kind of data can express features in an euclidean way. Graphs is used as a structure for representing data in a lot of cases. . It drives us to Graph Convolutional Networks (GCNs). The graph convolutional operator is going to associate weights with each of the edges and apply the weights across the graph and then the kernel is going to be moved to the next node in the graph extracting information about its local connectivity. That local information is going to be aggregated and the NN is going to then learn a function that encodes that local information into a higher level representation. . New frontier: Automated Machine Learning &amp; AI. . Using a neural architecture search algorithm. At each step the model samples a brand new network. For each layer, defines number of fileters, filet height, width, stride height, width, nbr of fileters, etc. Update RNN controller based on the accuracy of the child network after training. . From autoML to autoAI: an automated complete pipeline for designing and deploying ML and AI models. . 3/15/21 - Pixels-to-Control Learning - Software Lab 3 . This is about reinforcement learning. . . We install (apt) xvfb and python-opengl. . And will learn with cartpole and pong. . Still this issue . UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [[node sequential_8/conv2d_4/Conv2D (defined at :19) ]] [Op:__inference_distributed_function_2442603] . Solved by running . import tensorflow as tf gpus = tf.config.list_physical_devices(&#39;GPU&#39;) if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(&#39;GPU&#39;) print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;) except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) . I couldn’t go through the training of Pong agent due to GPU limitation? . 2021-03-15 10:54:19.479775: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at conv_grad_input_ops.cc:1254 : Resource exhausted: OOM when allocating tensor with shape[3944,48,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfcbash . 3/22/21 - Evidential Deep Learning and Uncertainty - lecture 7 . . . . 3/29/21 - Bias and Fairness - lecture 8 . This starts as a standard lecture about bias. . I like emphasis about bias that could stand in all stages of AI life cycle: . data (obviously) | model | training and deployment | evaluation | interpretation | . Good explanation about biases due to class imbalance. It develops my intuition about it. . Balanced batches can be the answer. . Example weighting is another option using inverse frequency as a weight. . . Adversarial learning to mitiage Bias. . Application in NLP to complete analogies. He is to she, as doctor is to ? . Same thing with Learned Latent Structure. (can be used to create fair and representative dataset) . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021.html",
            "relUrl": "/blog/learning-MIT-6.S191-2021.html",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Reinforcement learning readings",
            "content": "1/26/21 - Reinforcement learning for real-world robotics . from https://www.youtube.com/watch?v=Obek04C8L5E&amp;feature=youtu.be . at 26’ idea that you can tackle over-optimism models by using ensemble models. See paper at 2018 Model-Ensemble Trust-Region Policy Optimization . 1/26/21 - Reinforcement Learning algorithms — an intuitive overview . from https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc . . give an overview of various RL models. Model-based vs model-free. . And papers and codes. . 1/26/21 - Reinforcement learning, partie 1 : introduction (in French) . There is a reference to an introduction paper: from Sutton, Richard S., and Andrew G. Barto « Reinforcement learning : an introduction. » (2011). (I have an updated version from 2015) . There is a reference to a blog article [2] Steeve Huang. “Introduction to Various Reinforcement Learning Algorithms. Part I” (Q-Learning, SARSA, DQN, DDPG)”. (2018) . And the paper for OpenAI Gym [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba. “OpenAI Gym”. (2016) . 1/27/21 - Reinforcement learning : an introduction - I tabular solution methods . as a ref. from Reinforcement learning, partie 1 : introduction (in French) . I like this summary about RL . Reinforcement learning is a computational approach to understanding and automating goal-directed learning and decision-making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without relying on exemplary supervision or complete models of the environment. In our opinion, reinforcement learning is the first field to seriously address the computational issues that arise when learning from interaction with an environment in order to achieve long-term goals. Reinforcement learning uses a formal framework defining the interaction between a learning agent and its environment in terms of states, actions, and rewards. This framework is intended to be a simple way of representing essential features of the artificial intelligence problem. These features include a sense of cause and effect, a sense of uncertainty and nondeterminism, and the existence of explicit goals. . There is some history about RL. Bellman equation and dynamic programming are at the beginning of RL. . I read about HJB equation from Huyên PHAM (from a French Math magazine). It is funny to see why dynamic programming has been named that way, and how to deal with management. . The class of methods for solving optimal control problems by solving this equation came to be known as dynamic programming (Bellman, 1957a). Bellman (1957b) also introduced the discrete stochastic version of the optimal control problem known as Markovian decision processes (MDPs), and Ronald Howard (1960) devised the policy iteration method for MDPs. All of these are essential elements underlying the theory and algorithms of modern reinforcement learning. . All the vocabulary around RL is coming from dynamic programming and MDP. . Markov decision process - Wikipedia . . Interesting to read that the famous cart pole experiment (learning to balance a pole hinged to a movable cart) came from Michie and Chambers in 1968, 53 years ago! (and derived from tic-tac-toe experiment) . I don’t understand the subtlety behind the move from “learning with a teacher” to “learning with a critic” following the modified Least-Mean-Square (LMS) algorithm; Widrow and Hoff (1973) . And some explanations about temporal-difference. I have just understood that a convergence effort happened (in 1989) by Chris Watkin who brought together temporal-difference and optimal control by developing Q-learning. . After this introduction, here is the content: . 1st part is about finite markov decision processes—and its main ideas including Bellman equations and value functions. . 2nd part is about describing three fundamental classes of methods for solving finite Markov decision problems: dynamic programming, Monte Carlo methods, and temporal-difference learning. Each class of methods has its strengths and weaknesses. Dynamic programming methods are well developed mathematically, but require a complete and accurate model of the environment. Monte Carlo methods don’t require a model and are conceptually simple, but are not suited for step-by-step incremental computation. Finally, temporal-difference methods require no model and are fully incremental, but are more complex to analyze. . 3rd part is about combining these methods to offer a complete and unified solution to the tabular reinforcement learning problem. . We can think of terms agent, environment, and action as engineers’ terms controller, controlled system (or plant), and control signal. . . Explanation about agent vs environment. Often not the same as physical boundaries of a robot: this boundary represents the limit of the agent’s absolute control, not of its knowledge. Many different agents can be operated at once. . The agent’s goal is to maximize the total mount of reward it receives. . I should re-read the full chapter3 because a lot of concepts coming from MDP is exposed, and their links to RL. At the end I should be able to answer most of end-of-chapter exercises. Have clearer view about how to define what are my agents/environment in my case; how to define actions (low-level definition (e.g. V in level1 electrical grid vs high level decision)); everything related to q* and Q-learning. . dynamic programming (DP) (chap4 - 103-126) . What is key here is to have an exact way to describe your environment. Which is not always feasible. And we need computer power to go through all states, compute value function. There is a balance between policy evaluation and policy improvement but this is not crystal clear to me. And I don’t understand asynchronous DP. I haven’t developed enough intuitions behind DP, and I am unable to answer exercises. I understand though that reinforcement learning can solve some problems by approximating part of it (evaluation, environment, …) . monte carlo (MC) methods (chap5 - 127-156) . first-visit vs every-visit methods. First-visit has been widely studied. Blackjack example. Explanation of Monte Carlo ES (exploratory starts); and how to avoid this unlikely assumption thanks to on-policy or off-policy methods (on-policy estimate the value of a policy while using it for control. In off-policy methods these two functions are separated (behavior and target)). . One issue with MC methods is to ensure sufficient exploration. One approach is to start with a random state-action pair, could work with simulated episodes but unlikely to learn from real experience. . MC methods do not bootstrap (i.e. they don’t update their value estimates based on other value estimates) (TODO learn more about bootstrapping) . temporal-difference (TD) learning (chap6 - 157-180) . TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap, or said differently they learn a guess from a guess). . If you consider optimization as a 2 phases approach: prediction problem (ie policy evaluation) and control problem (ie optimal policy), DP, TD, MC differences are at the prediction problem. On control problem they use variations of generalized policy iteration (GPI). . TD methods combine the sampling of Monte Carlo with the bootstrapping of DP. . Example based on Driving Home. In TD you update prediction at each step, not waiting for the final return as in MC. . eligibility traces (chap7 - 181-208) . TD(λ lambdaλ) is a way to integrate MC and TD. . If one wants to use TD methods because of their other advantages, but the task is at least partially non-Markov, then the use of an eligibility trace method is indicated. Eligibility traces are the first line of defense against both long-delayed rewards and non-Markov tasks. . I am not sure to understand the effect of bootstrap. . Planning and Learning with Tabular Methods (chap8 - 209-220-236) . planning = require a model (dynamic programming, heuristic search) . learning = can be used without a model (MC, TD) . The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment. . 2/18/21 - from A Deep Reinforcement Learning Based Multi-Criteria Decision Support System for Optimizing Textile Chemical Process . This is a more practical paper and should help to figure out what could be our own implementation. . Overall MDP (markov decision process) structure is quite interesting with 3 blocks: . RF (random forest) models (one per objective) | AHP (analytic hierarchy process) which is a MCDM (Multiple criteria decision-making) method | DQN which is the reinforcement learning part to approximate the Q function | . . there are interesting references. . [2] K. Suzuki, ARTIFICIAL NEURAL NETWORKS - INDUSTRIAL AND CONTROL ENGINEERING APPLICATIONS. 2011. . It is nearly impossible to upgrade the textile chemical manufacturing processes directly by only following the cases from other industries without considering the detailed characteristics of this sector and specific investigations in the applicable advanced technologies. To this end, the construction of accurate models for simulating manufacturing processes using intelligent techniques is rather necessary[2] . [4]A. Ghosh, P. Mal, and A. Majumdar, Advanced Optimization and Decision-Making Techniques in Textile Manufacturing.2019. . [..] Therefore, production decision-makers cannot effectively control the processes in order to obtain desired product functionalities [4] . [53] T.L. Saaty, “What is the analytic hierarchy process?” Mathematical models for decision support, Springer, 1988, pp.109 121. . The AHP is a MCDM method introduced by Saaty [53] . [54]R. S. Sutton and A. G. Barto, Introduction to reinforcement learning, vol. 135. MIT press Cambridge, 1998. . The Markov property indicates that the state transitions are only dependent on the current state and current action is taken, but independent to all prior states and actions[54]. . [66] Z. Chourabi, F.Khedher, A. Babay and M. Cheikhrouhou, “Multi-criteria decision making in workforce choice using AHP, WSM and WPM”, J.Text.Inst., 2018 . However, it is worth remarking that certain features of this framework may hinder the massive promotion and application of it. The AHP has been successfully implemented in MCDM problems [41], [66] . 2/18/21 - The Complete Reinforcement Learning Dictionary . recommandations: . If you’re looking for a quick, 10-minutes crash course into RL with code examples, checkout my Qrash Course series: Introduction to RL and Q-Learning and Policy Gradients and Actor-Critics. | I you’re into something deeper, and would like to learn and code several different RL algorithms and gain more intuition, I can recommend this series by Thomas Simonini and this series by Arthur Juliani. | If you’re ready to master RL, I will direct you to the “bible” of Reinforcement Learning — “Reinforcement Learning, an introduction” by Richard Sutton and Andrew Barto. The second edition (from 2018) is available for free (legally) as a PDF file. | . 3/5/21 - Reinforcement learning : an introduction - II Approximate Solution Methods . This is the 2nd part of the book. . On-policy Approximation of Action Values . As mentioned in introduction of part II, what is developed in part I (our estimates of value functions are represented as a table with one entry for each state or for each state–action pair) is instructive, but of course it is limited to tasks with small numbers of states and actions. . How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset? . This is a generalization issue (or function approximation) one could consider as an instance of supervised learning, where we use the s-&gt;v of each backup as a training example, and then interpret the approximate function produced as an estimated value function. . Bertsekas and Tsitsiklis (1996) present the state of the art in function approximation in reinforcement learning. . Policy approximation . Actor-Critic: The policy structure is known as the actor, because it is used to select actions, and the estimated value function is known as the critic, because it criticizes the actions made by the actor. . (3/12/21) end of book. Chapter 14 - Applications and case studies. . I like this statement: . Applications of reinforcement learning are still far from routine and typically require as much art as science. Making applications easier and more straightforward is one of the goals of current research in reinforcement learning. . TD backgammon (1995). It uses a neural net (1 hidden layer, from 40 to 80 units) to approximate the predicted probability of winning v(s) for a given state. In later version, some domain features were used but still using self-play TD learning method. (I don’t know specifics for these domain features). And last versions give an interest to opponent reactions (possible dice rolls and moves) . Samuel’s Checkers Player (~1960). (Checkers c’est le jeu de dames). It is based on minimax procedure to find the best move from current position. 1st learning used was rote learning (storing position(s value). 2nd learning used alpha-beta (linked to minimax procedure) and hierarchical lookup tables instead of linear function approximation. . Acrobot (1993). Use of Sarsa(λ lambdaλ). Interesting to see that an exploration step can spoil a whole sequence of good actions. This is why greedy policy is used (ϵ epsilonϵ=0). . Elevator dispatching (1996). With a reward being the negative of the sum of the squared waiting times of all waiting passengers. (squared to push the system to avoid big waiting times). We use an extension of Q-learning to semi-Markov decision problems. For function approximation, a nonlinear neural network trained by back-propagation was used to represent the action-value function. . Dynamic Channel Allocation (1997). The channel assignment problem can be formulated as a semi-Markov decision process much as the elevator dispatching problem was in the previous section. . Job-Shop Scheduling (1996). Zhang and Dietterich’s job-shop scheduling system is the first successful instance of which we are aware in which reinforcement learning was applied in plan-space, that is, in which states are complete plans (job-shop schedules in this case), and actions are plan modifications. This is a more abstract application of reinforcement learning than we are used to thinking about. . Chapter 15 - Prospects . . This is a map to distinguish where to use different techniques. And considerations of a 3rd dimension regarding function approximation, or on/off-policy. . And then opening to non markov case such as the theory of partially observable MDPs (POMDPs). (StarCraft!) . References . 25 pages of references! Woawww. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-readings.html",
            "relUrl": "/blog/reinforcement-learning-readings.html",
            "date": " • Jan 26, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Aristotle and Deep learning",
            "content": "By reading some references in recent paper, I have started to read “artificial intelligence structures and strategies for complex problem solving” by George Luger. . . This is a massive book from 2005 in its 6th edition. I don’t think it has been updated since that. And the author starts a writing of AI history. . I have been intrigued by the use of the opening sentence from Aristotle in the Metaphysics: “All men by nature desire to know…”. I remembered that sentence (without knowing it was from Aristotle), and I jumped to The Metaphysics Aristotle’s [wikipedia page](https://fr.wikipedia.org/wiki/M%C3%A9taphysique%28Aristote%29) (the French one). There is a nice presentation of The MetaPhysics and some extracts that I have found quite interesting. One of them following “All men by nature desire to know” is detailing what is art and science; and for art: one need to be able to recognize similar cases and be able to generalize to an (more) universal rule. . I cannot not see a link with what is happening in what we do on a daily basis in AI and deep learning. I had been surprised by Jeremy Howard’s curriculum (if I am not wrong he has a major) in Philosophy, and I better understand why he is so good in what he does. . Should have studied Philosophy and ancient Greek! . Would love to know your thoughts about that. (and if anyone can ask Jeremy’s without directly @ him) . Here is a more detailed analysis of Aristotle thought: (again from wikipedia, not my own ;)) . By nature, all animals are sentient; but sensation is not yet sufficient to produce knowledge: indeed, remarks Aristotle, sensation engenders memory or not. But animals endowed with memory are the most intelligent and the best able to learn. However, man “lives on art and reasoning.” To learn, you have to feel, remember, but man has the capacity to draw experience from these simple images and from a multitude of experimental notions emerges a single judgment that is universal in all similar cases: it is what constitutes art: “Science and art arise for men through experience” 10. Art therefore presupposes: the ability to recognize similar cases and the ability to apply a universal rule to these cases. Of experience and art, which is more perfect? In practical life, experience seems superior to art, because it is knowledge of the particular, of the individual: sensations, the foundation of knowledge of the particular, are not science and do not teach us the why ( διότι). Art, for its part, knows the universal and goes beyond individual things, it is to art that knowledge and the faculty of understanding belong: men of art know the why and the cause. The wisest are wise not by practical skill, but by theory (λόγος) and knowledge of the causes. This explains the superiority of the architect over the maneuver. The sign of this knowledge is that it can be taught; now men of art can teach. However, among the arts some relate to the necessities of life and others come from “leisure” which is knowledge sought for itself, as in mathematics. And through these appears the highest knowledge, wisdom, which has for its object the first causes and the first principles of what-is; therefore the theoretical sciences are superior to the practical sciences. . From observations (rows of data) we can recognize similar cases (patterns or embeddings) and identify universal rules (models?) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Aristotle-and-deep-learning.html",
            "relUrl": "/blog/Aristotle-and-deep-learning.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "seaborn cheatsheet",
            "content": "Introduction to Data Visualization with Seaborn . pdf lectures in github . Introduction to Seaborn . Introduction to Seaborn . # Getting started import seaborn as sns import matplotlib.pyplot as plt # Example 1: Scatter plot import seaborn as sns import matplotlib.pyplot as plt height = [62, 64, 69, 75, 66, 68, 65, 71, 76, 73] weight = [120, 136, 148, 175, 137, 165, 154, 172, 200, 187] sns.scatterplot(x=height, y=weight) plt.show() # Example 2: Create a count plot import seaborn as sns import matplotlib.pyplot as plt gender = [&quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;] sns.countplot(x=gender) plt.show() . Using pandas with Seaborn . # Using DataFrames with countplot() import pandas as pd import matplotlib.pyplot as plt import seaborn as sns df = pd.read_csv(&quot;masculinity.csv&quot;) sns.countplot(x=&quot;how_masculine&quot;, data=df) plt.show() . Adding a third variable with hue . # Tips dataset import pandas as pd import seaborn as sns tips = sns.load_dataset(&quot;tips&quot;) tips.head() # A basic scatter plot import matplotlib.pyplot as plt import seaborn as sns sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips) plt.show() # A scatter plot with hue import matplotlib.pyplot as plt import seaborn as sns sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, hue=&quot;smoker&quot;) plt.show() # Setting hue order import matplotlib.pyplot as plt import seaborn as sns sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, hue=&quot;smoker&quot;, hue_order=[&quot;Yes&quot;,&quot;No&quot;]) plt.show() # Specifying hue colors import matplotlib.pyplot as plt import seaborn as sns hue_colors = {&quot;Yes&quot;: &quot;black&quot;, &quot;No&quot;: &quot;red&quot;} sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, hue=&quot;smoker&quot;, palette=hue_colors) plt.show() # Using HTML hex color codes with hue import matplotlib.pyplot as plt import seaborn as sns hue_colors = {&quot;Yes&quot;: &quot;#808080&quot;, &quot;No&quot;: &quot;#00FF00&quot;} sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, hue=&quot;smoker&quot;, palette=hue_colors) plt.show() # Using hue with count plots import matplotlib.pyplot as plt import seaborn as sns sns.countplot(x=&quot;smoker&quot;, data=tips, hue=&quot;sex&quot;) plt.show() . Visualizing Two Quantitative Variables . Introduction to relational plots and subplots . # Using relplot() import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;scatter&quot;) plt.show() # Subplots in columns import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;scatter&quot;, col=&quot;smoker&quot;) plt.show() # Subplots in rows import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;scatter&quot;, row=&quot;smoker&quot;) plt.show() # Subplots in rows and columns import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;scatter&quot;, col=&quot;smoker&quot;, row=&quot;time&quot;) plt.show() # Wrapping columns import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,col=&quot;day&quot;,col_wrap=2) plt.show() # Ordering columns import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,col=&quot;day&quot;,col_wrap=2,col_order=[&quot;Thur&quot;,&quot;Fri&quot;,&quot;Sat&quot;,&quot;Sun&quot;]) plt.show() . Customizing scatter plots . # Subgroups with point size import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,size=&quot;size&quot;) plt.show() # Point size and hue import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,size=&quot;size&quot;,hue=&quot;size&quot;) plt.show() # Subgroups with point style import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,hue=&quot;smoker&quot;,style=&quot;smoker&quot;) plt.show() # Changing point transparency import seaborn as sns import matplotlib.pyplot as plt # Set alpha to be between 0 and 1 sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,alpha=0.4) plt.show() . Introduction to line plots . # Line plot import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2_mean&quot;,data=air_df_mean,kind=&quot;line&quot;) plt.show() # Subgroups by location import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2_mean&quot;,data=air_df_loc_mean,kind=&quot;line&quot;,style=&quot;location&quot;,hue=&quot;location&quot;) plt.show() # Adding markers import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2_mean&quot;,data=air_df_loc_mean,kind=&quot;line&quot;,style=&quot;location&quot;,hue=&quot;location&quot;,markers=True) plt.show() # Turning off line style import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2_mean&quot;,data=air_df_loc_mean,kind=&quot;line&quot;,style=&quot;location&quot;,hue=&quot;location&quot;,markers=True,dashes=False) plt.show() # Multiple observations per x-value # Line plot import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2&quot;,data=air_df,kind=&quot;line&quot;) plt.show() # Replacing confidence interval with standard deviation import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2&quot;,data=air_df,kind=&quot;line&quot;,ci=&quot;sd&quot;) plt.show() # Turning off confidence interval import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2&quot;,data=air_df,kind=&quot;line&quot;,ci=None) plt.show() . Visualizing a Categorical and a Quantitative Variable . Count plots and bar plots . # countplot() vs. catplot() import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;how_masculine&quot;,data=masculinity_data,kind=&quot;count&quot;) plt.show() # Changing the order import matplotlib.pyplot as plt import seaborn as sns category_order = [&quot;No answer&quot;,&quot;Not at all&quot;,&quot;Not very&quot;,&quot;Somewhat&quot;,&quot;Very&quot;] sns.catplot(x=&quot;how_masculine&quot;,data=masculinity_data,kind=&quot;count&quot;,order=category_order) plt.show() # Bar plots import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;day&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;bar&quot;) plt.show() # Turning off confidence intervals import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;day&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;bar&quot;,ci=None) plt.show() # Changing the orientation import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;total_bill&quot;,y=&quot;day&quot;,data=tips,kind=&quot;bar&quot;) plt.show() . Box plots . # How to create a box plot import matplotlib.pyplot as plt import seaborn as sns g = sns.catplot(x=&quot;time&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;box&quot;) plt.show() # Change the order of categories import matplotlib.pyplot as plt import seaborn as sns g = sns.catplot(x=&quot;time&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;box&quot;,order=[&quot;Dinner&quot;,&quot;Lunch&quot;]) plt.show() # Omitting the outliers using `sym` import matplotlib.pyplot as plt import seaborn as sns g = sns.catplot(x=&quot;time&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;box&quot;,sym=&quot;&quot;) plt.show() # Changing the whiskers using `whis` import matplotlib.pyplot as plt import seaborn as sns g = sns.catplot(x=&quot;time&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;box&quot;,whis=[0, 100]) plt.show() . Point plots . # Creating a point plot import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;age&quot;,y=&quot;masculinity_important&quot;,data=masculinity_data,hue=&quot;feel_masculine&quot;,kind=&quot;point&quot;) plt.show() # Disconnecting the points import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;age&quot;,y=&quot;masculinity_important&quot;,data=masculinity_data,hue=&quot;feel_masculine&quot;,kind=&quot;point&quot;,join=False) plt.show() # Displaying the median import matplotlib.pyplot as plt import seaborn as sns from numpy import median sns.catplot(x=&quot;smoker&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;point&quot;,estimator=median) plt.show() # Customizing the confidence intervals import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;smoker&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;point&quot;,capsize=0.2) plt.show() # Turning off confidence intervals import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;smoker&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;point&quot;,ci=None) plt.show() . Customizing Seaborn Plots . Changing plot style and color . # Figure style: &quot;whitegrid&quot; sns.set_style(&quot;whitegrid&quot;) sns.catplot(x=&quot;age&quot;,y=&quot;masculinity_important&quot;,data=masculinity_data,hue=&quot;feel_masculine&quot;,kind=&quot;point&quot;) plt.show() # Other styles: sns.set_style(&quot;ticks&quot;) sns.set_style(&quot;dark&quot;) sns.set_style(&quot;darkgrid&quot;) # Example (diverging palette) sns.set_palette(&quot;RdBu&quot;) category_order = [&quot;No answer&quot;,&quot;Not at all&quot;,&quot;Not very&quot;,&quot;Somewhat&quot;,&quot;Very&quot;] sns.catplot(x=&quot;how_masculine&quot;,data=masculinity_data,kind=&quot;count&quot;,order=category_order) plt.show() # Custom palettes custom_palette = [&quot;red&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;blue&quot;,&quot;yellow&quot;, &quot;purple&quot;] sns.set_palette(custom_palette) # Custom palettes custom_palette = [&#39;#FBB4AE&#39;, &#39;#B3CDE3&#39;, &#39;#CCEBC5&#39;,&#39;#DECBE4&#39;, &#39;#FED9A6&#39;, &#39;#FFFFCC&#39;,&#39;#E5D8BD&#39;, &#39;#FDDAEC&#39;, &#39;#F2F2F2&#39;] sns.set_palette(custom_palette) # Larger context: &quot;talk&quot; #Smallest to largest: &quot;paper&quot;, &quot;notebook&quot;, &quot;talk&quot;, &quot;poster&quot; sns.set_context(&quot;talk&quot;) . Adding titles and labels: Part 1 . # Adding a title to FacetGrid g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;) g.fig.suptitle(&quot;New Title&quot;) plt.show() # Adjusting height of title in FacetGrid g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;) g.fig.suptitle(&quot;New Title&quot;,y=1.03) plt.show() . Adding titles and labels: Part 2 . # Adding a title to AxesSubplot g = sns.boxplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data) g.set_title(&quot;New Title&quot;,y=1.03) # Titles for subplots g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;,col=&quot;Group&quot;) g.fig.suptitle(&quot;New Title&quot;,y=1.03) g.set_titles(&quot;This is {col_name}&quot;) # Adding axis labels g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;) g.set(xlabel=&quot;New X Label&quot;,ylabel=&quot;New Y Label&quot;) plt.show() # Rotating x-axis tick labels g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;) plt.xticks(rotation=90) plt.show() . Intermediate Data Visualization with Seaborn . pdf lectures in github . Seaborn Introduction . Introduction to Seaborn . # Seaborn distplot import seaborn as sns sns.distplot(df[&#39;alcohol&#39;]) . Using the distribution plot . # Creating a histogram sns.distplot(df[&#39;alcohol&#39;], kde=False, bins=10) # Alternative data distributions sns.distplot(df[&#39;alcohol&#39;], hist=False, rug=True) # Further Customizations sns.distplot(df[&#39;alcohol&#39;], hist=False,rug=True, kde_kws={&#39;shade&#39;:True}) . Regression Plots in Seaborn . # Introduction to regplot sns.regplot(x=&quot;alcohol&quot;, y=&quot;pH&quot;, data=df) # lmplot faceting sns.lmplot(x=&quot;quality&quot;, y=&quot;alcohol&quot;,data=df, hue=&quot;type&quot;) sns.lmplot(x=&quot;quality&quot;, y=&quot;alcohol&quot;,data=df, col=&quot;type&quot;) . Customizing Seaborn Plots . Using Seaborn Styles . # Setting Styles # Seaborn has default configurations that can be applied with sns.set() # These styles can override matplotlib and pandas plots as well sns.set() # Theme examples with sns.set_style() for style in [&#39;white&#39;,&#39;dark&#39;,&#39;whitegrid&#39;,&#39;darkgrid&#39;,&#39;ticks&#39;]: sns.set_style(style) sns.distplot(df[&#39;Tuition&#39;]) plt.show() # Removing axes with despine() sns.set_style(&#39;white&#39;) sns.distplot(df[&#39;Tuition&#39;]) sns.despine(left=True) . Colors in Seaborn . # Defining a color for a plot sns.set(color_codes=True) sns.distplot(df[&#39;Tuition&#39;], color=&#39;g&#39;) # Palettes for p in sns.palettes.SEABORN_PALETTES: sns.set_palette(p) sns.distplot(df[&#39;Tuition&#39;]) # Displaying Palettes for p in sns.palettes.SEABORN_PALETTES: sns.set_palette(p) sns.palplot(sns.color_palette()) plt.show() # Defining Custom Palettes # Circular colors = when the data is not ordered sns.palplot(sns.color_palette(&quot;Paired&quot;, 12)) # Sequential colors = when the data has a consistent range from high to low sns.palplot(sns.color_palette(&quot;Blues&quot;, 12)) # Diverging colors = when both the low and high values are interesting sns.palplot(sns.color_palette(&quot;BrBG&quot;, 12)) . Customizing with matplotlib . # Matplotlib Axes fig, ax = plt.subplots() sns.distplot(df[&#39;Tuition&#39;], ax=ax) ax.set(xlabel=&quot;Tuition 2013-14&quot;) # Further Customizations fig, ax = plt.subplots() sns.distplot(df[&#39;Tuition&#39;], ax=ax) ax.set(xlabel=&quot;Tuition 2013-14&quot;,ylabel=&quot;Distribution&quot;, xlim=(0, 50000),title=&quot;2013-14 Tuition and Fees Distribution&quot;) # Combining Plots fig, (ax0, ax1) = plt.subplots(nrows=1,ncols=2, sharey=True, figsize=(7,4)) sns.distplot(df[&#39;Tuition&#39;], ax=ax0) sns.distplot(df.query(&#39;State == &quot;MN&quot;&#39;)[&#39;Tuition&#39;], ax=ax1) ax1.set(xlabel=&quot;Tuition (MN)&quot;, xlim=(0, 70000)) ax1.axvline(x=20000, label=&#39;My Budget&#39;, linestyle=&#39;--&#39;) ax1.legend() . Additional Plot Types . Categorical Plot Types . # Plots of each observation - stripplot sns.stripplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;, jitter=True) # Plots of each observation - swarmplot sns.swarmplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;) # Abstract representations - boxplot sns.boxplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;) # Abstract representation - violinplot sns.violinplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;) # Abstract representation - lvplot sns.lvplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;) # Statistical estimates - barplot sns.barplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;, hue=&quot;Region&quot;) # Statistical estimates - pointplot sns.pointplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;, hue=&quot;Region&quot;) # Statistical estimates - countplot sns.countplot(data=df, y=&quot;DRG_Code&quot;, hue=&quot;Region&quot;) . Regression Plots . # Plotting with regplot() sns.regplot(data=df, x=&#39;temp&#39;, y=&#39;total_rentals&#39;, marker=&#39;+&#39;) # Evaluating regression with residplot() sns.residplot(data=df, x=&#39;temp&#39;, y=&#39;total_rentals&#39;) # Polynomial regression sns.regplot(data=df, x=&#39;temp&#39;, y=&#39;total_rentals&#39;, order=2) # residplot with polynomial regression sns.residplot(data=df, x=&#39;temp&#39;, y=&#39;total_rentals&#39;, order=2) # Categorical values sns.regplot(data=df, x=&#39;mnth&#39;, y=&#39;total_rentals&#39;, x_jitter=.1, order=2) # Estimators sns.regplot(data=df, x=&#39;mnth&#39;, y=&#39;total_rentals&#39;, x_estimator=np.mean, order=2) # Binning the data sns.regplot(data=df,x=&#39;temp&#39;,y=&#39;total_rentals&#39;, x_bins=4) . Matrix plots . # Getting data in the right format pd.crosstab(df[&quot;mnth&quot;], df[&quot;weekday&quot;], values=df[&quot;total_rentals&quot;],aggfunc=&#39;mean&#39;).round(0) # Build a heatmap sns.heatmap(pd.crosstab(df[&quot;mnth&quot;], df[&quot;weekday&quot;], values=df[&quot;total_rentals&quot;], aggfunc=&#39;mean&#39;) ) # Customize a heatmap sns.heatmap(df_crosstab, annot=True, fmt=&quot;d&quot;, cmap=&quot;YlGnBu&quot;, cbar=False, linewidths=.5) # Centering a heatmap sns.heatmap(df_crosstab, annot=True, fmt=&quot;d&quot;, cmap=&quot;YlGnBu&quot;, cbar=True, center=df_crosstab.loc[9, 6]) # Plotting a correlation matrix sns.heatmap(df.corr()) . Creating Plots on Data Aware Grids . Using FacetGrid, factorplot and lmplot . # FacetGrid Categorical Example g = sns.FacetGrid(df, col=&quot;HIGHDEG&quot;) g.map(sns.boxplot, &#39;Tuition&#39;, order=[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;]) # factorplot() sns.factorplot(x=&quot;Tuition&quot;, data=df, col=&quot;HIGHDEG&quot;, kind=&#39;box&#39;) # FacetGrid for regression # FacetGrid() can also be used for sca er or regression plots g = sns.FacetGrid(df, col=&quot;HIGHDEG&quot;) g.map(plt.scatter, &#39;Tuition&#39;, &#39;SAT_AVG_ALL&#39;) # lmplot # lmplot plots sca er and regression plots on a FacetGrid sns.lmplot(data=df, x=&quot;Tuition&quot;, y=&quot;SAT_AVG_ALL&quot;, col=&quot;HIGHDEG&quot;, fit_reg=False) # lmplot with regression sns.lmplot(data=df, x=&quot;Tuition&quot;, y=&quot;SAT_AVG_ALL&quot;, col=&quot;HIGHDEG&quot;, row=&#39;REGION&#39;) . Using PairGrid and pairplot . # Creating a PairGrid g = sns.PairGrid(df, vars=[&quot;Fair_Mrkt_Rent&quot;, &quot;Median_Income&quot;]) g = g.map(plt.scatter) # Customizing the PairGrid diagonals g = sns.PairGrid(df, vars=[&quot;Fair_Mrkt_Rent&quot;, &quot;Median_Income&quot;]) g = g.map_diag(plt.hist) g = g.map_offdiag(plt.scatter) # Pairplot sns.pairplot(df, vars=[&quot;Fair_Mrkt_Rent&quot;, &quot;Median_Income&quot;], kind=&#39;reg&#39;, diag_kind=&#39;hist&#39;) # Customizing a pairplot sns.pairplot(df.query(&#39;BEDRMS &lt; 3&#39;),vars=[&quot;Fair_Mrkt_Rent&quot;,&quot;Median_Income&quot;, &quot;UTILITY&quot;],hue=&#39;BEDRMS&#39;, palette=&#39;husl&#39;, plot_kws={&#39;alpha&#39;: 0.5}) . Using JointGrid and jointplot . # Basic JointGrid g = sns.JointGrid(data=df, x=&quot;Tuition&quot;,y=&quot;ADM_RATE_ALL&quot;) g.plot(sns.regplot, sns.distplot) # Advanced JointGrid g = sns.JointGrid(data=df, x=&quot;Tuition&quot;,y=&quot;ADM_RATE_ALL&quot;) g = g.plot_joint(sns.kdeplot) g = g.plot_marginals(sns.kdeplot, shade=True) g = g.annotate(stats.pearsonr) # jointplot() sns.jointplot(data=df, x=&quot;Tuition&quot;,y=&quot;ADM_RATE_ALL&quot;, kind=&#39;hex&#39;) # Customizing a jointplot g = (sns.jointplot(x=&quot;Tuition&quot;, y=&quot;ADM_RATE_ALL&quot;, kind=&#39;scatter&#39;, xlim=(0, 25000), marginal_kws=dict(bins=15,rug=True), data=df.query(&#39;UG &lt; 2500 &amp; Ownership == &quot;Public&quot;&#39;)) .plot_joint(sns.kdeplot)) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/seaborn-cheatsheet.html",
            "relUrl": "/blog/seaborn-cheatsheet.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Java installation on Ubuntu 20.04",
            "content": "Following these instructions: https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-on-ubuntu-20-04-fr. . Current configuration . !java --version . openjdk 11.0.9.1 2020-11-04 OpenJDK Runtime Environment (build 11.0.9.1+1-Ubuntu-0ubuntu1.20.04) OpenJDK 64-Bit Server VM (build 11.0.9.1+1-Ubuntu-0ubuntu1.20.04, mixed mode, sharing) . Download Oracle JDK 11 . From https://launchpad.net/~linuxuprising/+archive/ubuntu/java/+packages, I can identify the focal version: . oracle-java11-installer-local - 11.0.9-1~linuxuprising0 (changes file) logix2 2020-10-22 Published Focal Java . I download the given version from Oracle website: https://www.oracle.com/java/technologies/javase-jdk11-downloads.html. Java SE Development Kit 11.0.9 Linux x64 Compressed Archive . And yes you have to login with an oracle account to download it. . Installation via linuxuprising/java . sudo add-apt-repository ppa:linuxuprising/java sudo apt update sudo mkdir -p /var/cache/oracle-jdk11-installer-local/ sudo cp ~/Downloads/jdk-11.0.9_linux-x64_bin.tar.gz /var/cache/oracle-jdk11-installer-local/ sudo apt install oracle-java11-installer-local . After accepting the license agreement, installation is running . Check . $ sudo update-alternatives --config java There are 2 choices for the alternative java (providing /usr/bin/java). Selection Path Priority Status 0 /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1111 auto mode 1 /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1111 manual mode * 2 /usr/lib/jvm/java-11-oracle/bin/java 1091 manual mode . Environment variable . Enter /usr/lib/jvm/java-11-oracle as your JAVA_HOME variable in /etc/environment . $ cat /etc/environment PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin&quot; JAVA_HOME=&quot;/usr/lib/jvm/java-11-oracle&quot; $ source /etc/environment $ echo $JAVA_HOME /usr/lib/jvm/java-11-oracle .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/java-installation-on-ubuntu-20.04.html",
            "relUrl": "/blog/java-installation-on-ubuntu-20.04.html",
            "date": " • Jan 14, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "pandas cheatsheet",
            "content": "Data Manipulation with pandas . Transforming Data . Introducing DataFrames . # Exploring a DataFrame: .head() dogs.head() # Exploring a DataFrame: .info() dogs.info() # Exploring a DataFrame: .shape dogs.shape # Exploring a DataFrame: .describe() dogs.describe() # Components of a DataFrame: .values dogs.values # Components of a DataFrame: .columns and .index dogs.columns dogs.index . Sorting and subsetting . #Sorting by multiple variables dogs.sort_values([&quot;weight_kg&quot;, &quot;height_cm&quot;], ascending=[True, False]) #Subsetting based on dates dogs[dogs[&quot;date_of_birth&quot;] &gt; &quot;2015-01-01&quot;] #Subsetting based on multiple conditions is_lab = dogs[&quot;breed&quot;] == &quot;Labrador&quot; is_brown = dogs[&quot;color&quot;] == &quot;Brown&quot; dogs[is_lab &amp; is_brown] dogs[ (dogs[&quot;breed&quot;] == &quot;Labrador&quot;) &amp; (dogs[&quot;color&quot;] == &quot;Brown&quot;) ] #Subsetting using .isin() is_black_or_brown = dogs[&quot;color&quot;].isin([&quot;Black&quot;, &quot;Brown&quot;]) dogs[is_black_or_brown] . New columns . # Adding a new column dogs[&quot;height_m&quot;] = dogs[&quot;height_cm&quot;] / 100 . Aggregating Data . Summary statistics . #Summarizing numerical data dogs[&quot;height_cm&quot;].mean() .median() , .mode() .min() , .max() .var() , .std() .sum() .quantile() #The .agg() method def pct30(column): return column.quantile(0.3) dogs[&quot;weight_kg&quot;].agg(pct30) #Multiple summaries def pct40(column): return column.quantile(0.4) dogs[&quot;weight_kg&quot;].agg([pct30, pct40]) #Cumulative sum dogs[&quot;weight_kg&quot;].cumsum() #Cumulative statistics .cummax() .cummin() .cumprod() . Counting . #Dropping duplicate names vet_visits.drop_duplicates(subset=&quot;name&quot;) #Dropping duplicate pairs unique_dogs = vet_visits.drop_duplicates(subset=[&quot;name&quot;, &quot;breed&quot;]) #Counting unique_dogs[&quot;breed&quot;].value_counts(sort=True) . Grouped summary statistics . #Summaries by group dogs[dogs[&quot;color&quot;] == &quot;Black&quot;][&quot;weight_kg&quot;].mean() dogs[dogs[&quot;color&quot;] == &quot;Brown&quot;][&quot;weight_kg&quot;].mean() #Grouped summaries dogs.groupby(&quot;color&quot;)[&quot;weight_kg&quot;].mean() #Multiple grouped summaries dogs.groupby(&quot;color&quot;)[&quot;weight_kg&quot;].agg([min, max, sum]) #Grouping by multiple variables dogs.groupby([&quot;color&quot;, &quot;breed&quot;])[&quot;weight_kg&quot;].mean() #Many groups, many summaries dogs.groupby([&quot;color&quot;, &quot;breed&quot;])[[&quot;weight_kg&quot;, &quot;height_cm&quot;]].mean() . Pivot tables . #pivot table dogs.pivot_table(values=&quot;weight_kg&quot;,index=&quot;color&quot;) #Different statistics import numpy as np dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, aggfunc=np.median) #Multiple statistics dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, aggfunc=[np.mean, np.median]) #Pivot on two variables dogs.groupby([&quot;color&quot;, &quot;breed&quot;])[&quot;weight_kg&quot;].mean() dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, columns=&quot;breed&quot;) #Filling missing values in pivot tables dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, columns=&quot;breed&quot;, fill_value=0) # Summing with pivot tables dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, columns=&quot;breed&quot;, fill_value=0, margins=True) . Slicing and Indexing . Explicit indexes . # Setting a column as the index dogs_ind = dogs.set_index(&quot;name&quot;) # Removing an index dogs_ind.reset_index() # Dropping an index dogs_ind.reset_index(drop=True) # Indexes make subsetting simpler dogs[dogs[&quot;name&quot;].isin([&quot;Bella&quot;, &quot;Stella&quot;])] # versus dogs_ind.loc[[&quot;Bella&quot;, &quot;Stella&quot;]] # Multi-level indexes a.k.a. hierarchical indexes dogs_ind3 = dogs.set_index([&quot;breed&quot;, &quot;color&quot;]) # Subset the outer level with a list dogs_ind3.loc[[&quot;Labrador&quot;, &quot;Chihuahua&quot;]] # Subset inner levels with a list of tuples dogs_ind3.loc[[(&quot;Labrador&quot;, &quot;Brown&quot;), (&quot;Chihuahua&quot;, &quot;Tan&quot;)]] # Sorting by index values dogs_ind3.sort_index() # Controlling sort_index dogs_ind3.sort_index(level=[&quot;color&quot;, &quot;breed&quot;], ascending=[True, False]) . Slicing and subsetting with .loc and .iloc . # Sort the index before you slice dogs_srt = dogs.set_index([&quot;breed&quot;, &quot;color&quot;]).sort_index() # Slicing the outer index level dogs_srt.loc[&quot;Chow Chow&quot;:&quot;Poodle&quot;] # Slicing the inner index levels correctly dogs_srt.loc[(&quot;Labrador&quot;, &quot;Brown&quot;):(&quot;Schnauzer&quot;, &quot;Grey&quot;)] # Slicing columns dogs_srt.loc[:, &quot;name&quot;:&quot;height_cm&quot;] # Slice twice dogs_srt.loc[ (&quot;Labrador&quot;, &quot;Brown&quot;):(&quot;Schnauzer&quot;, &quot;Grey&quot;), &quot;name&quot;:&quot;height_cm&quot;] # Dog days dogs = dogs.set_index(&quot;date_of_birth&quot;).sort_index() # Slicing by dates # Get dogs with date_of_birth between 2014-08-25 and 2016-09-16 dogs.loc[&quot;2014-08-25&quot;:&quot;2016-09-16&quot;] # Slicing by partial dates # Get dogs with date_of_birth between 2014-01-01 and 2016-12-31 dogs.loc[&quot;2014&quot;:&quot;2016&quot;] # Subsetting by row/column number print(dogs.iloc[2:5, 1:4]) . Working with pivot tables . # Pivoting the dog pack dogs_height_by_breed_vs_color = dog_pack.pivot_table( &quot;height_cm&quot;, index=&quot;breed&quot;, columns=&quot;color&quot;) # The axis argument dogs_height_by_breed_vs_color.mean(axis=&quot;index&quot;) # Calculating summary stats across columns dogs_height_by_breed_vs_color.mean(axis=&quot;columns&quot;) . Creating and Visualizing DataFrames . Visualizing your data . # Histograms import matplotlib.pyplot as plt dog_pack[&quot;height_cm&quot;].hist(bins=20) # Bar plots avg_weight_by_breed = dog_pack.groupby(&quot;breed&quot;)[&quot;weight_kg&quot;].mean() avg_weight_by_breed.plot(kind=&quot;bar&quot;, title=&quot;Mean Weight by Dog Breed&quot;) # Line plots sully.head() sully.plot(x=&quot;date&quot;, y=&quot;weight_kg&quot;, kind=&quot;line&quot;) # Rotating axis labels sully.plot(x=&quot;date&quot;, y=&quot;weight_kg&quot;, kind=&quot;line&quot;, rot=45) # Scatter plots dog_pack.plot(x=&quot;height_cm&quot;, y=&quot;weight_kg&quot;, kind=&quot;scatter&quot;) # Layering plots dog_pack[dog_pack[&quot;sex&quot;]==&quot;F&quot;][&quot;height_cm&quot;].hist() dog_pack[dog_pack[&quot;sex&quot;]==&quot;M&quot;][&quot;height_cm&quot;].hist() # Add a legend plt.legend([&quot;F&quot;, &quot;M&quot;]) # Transparency dog_pack[dog_pack[&quot;sex&quot;]==&quot;F&quot;][&quot;height_cm&quot;].hist(alpha=0.7) dog_pack[dog_pack[&quot;sex&quot;]==&quot;M&quot;][&quot;height_cm&quot;].hist(alpha=0.7) plt.legend([&quot;F&quot;, &quot;M&quot;]) . Missing values . # Detecting missing values dogs.isna() # Detecting any missing values dogs.isna().any() # Counting missing values dogs.isna().sum() # Plotting missing values import matplotlib.pyplot as plt dogs.isna().sum().plot(kind=&quot;bar&quot;) plt.show() # Removing rows containing missing values dogs.dropna() # Replacing missing values dogs.fillna(0) . Reading and writing CSVs . # CSV to DataFrame import pandas as pd new_dogs = pd.read_csv(&quot;new_dogs.csv&quot;) # DataFrame to CSV new_dogs.to_csv(&quot;new_dogs_with_bmi.csv&quot;) # CSV to dataframe parsing dates, and having date as index climate_change = pd.read_csv(prefix+&#39;climate_change.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) . Joining data with pandas . Data merging basics . Inner join . # Inner join wards_census = wards.merge(census, on=&#39;ward&#39;) # Suffixes wards_census = wards.merge(census, on=&#39;ward&#39;, suffixes=(&#39;_ward&#39;,&#39;_cen&#39;)) . One-to-many relationships . # One-to-many example ward_licenses = wards.merge(licenses, on=&#39;ward&#39;, suffixes=(&#39;_ward&#39;,&#39;_lic&#39;)) . Merging multiple DataFrames . # Single merge grants.merge(licenses, on=[&#39;address&#39;,&#39;zip&#39;]) # Merging multiple tables grants_licenses_ward = grants.merge(licenses, on=[&#39;address&#39;,&#39;zip&#39;]) .merge(wards, on=&#39;ward&#39;, suffixes=(&#39;_bus&#39;,&#39;_ward&#39;)) # Plot Results import matplotlib.pyplot as plt grant_licenses_ward.groupby(&#39;ward&#39;).agg(&#39;sum&#39;).plot(kind=&#39;bar&#39;, y=&#39;grant&#39;) . Merging Tables With Different Join Types . Left join . # Merge with left join movies_taglines = movies.merge(taglines, on=&#39;id&#39;, how=&#39;left&#39;) . Other joins . # Merge with right join tv_movies = movies.merge(tv_genre, how=&#39;right&#39;, left_on=&#39;id&#39;, right_on=&#39;movie_id&#39;) # Merge with outer join family_comedy = family.merge(comedy, on=&#39;movie_id&#39;, how=&#39;outer&#39;, suffixes=(&#39;_fam&#39;, &#39;_com&#39;)) . Merging a table to itself . # Merging a table to itself original_sequels = sequels.merge(sequels, left_on=&#39;sequel&#39;, right_on=&#39;id&#39;, suffixes=(&#39;_org&#39;,&#39;_seq&#39;)) . Merging on indexes . # Setting an index movies = pd.read_csv(&#39;tmdb_movies.csv&#39;, index_col=[&#39;id&#39;]) # Merging on index movies_taglines = movies.merge(taglines, on=&#39;id&#39;, how=&#39;left&#39;) # MultiIndex merge samuel_casts = samuel.merge(casts, on=[&#39;movie_id&#39;,&#39;cast_id&#39;]) # Index merge with left_on and right_on movies_genres = movies.merge(movie_to_genres, left_on=&#39;id&#39;, left_index=True, right_on=&#39;movie_id&#39;, right_index=True) . Advanced Merging and Concatenating . Filtering joins . ########### # semi-join # Step 1 - semi-join genres_tracks = genres.merge(top_tracks, on=&#39;gid&#39;) # Step 2 - semi-join genres[&#39;gid&#39;].isin(genres_tracks[&#39;gid&#39;]) # Step 3 - semi-join genres_tracks = genres.merge(top_tracks, on=&#39;gid&#39;) top_genres = genres[genres[&#39;gid&#39;].isin(genres_tracks[&#39;gid&#39;])] ########### # anti-join # Step 1 - anti-join genres_tracks = genres.merge(top_tracks, on=&#39;gid&#39;, how=&#39;left&#39;, indicator=True) # Step 2 - anti-join gid_list = genres_tracks.loc[genres_tracks[&#39;_merge&#39;] == &#39;left_only&#39;, &#39;gid&#39;] # Step 3 - anti-join genres_tracks = genres.merge(top_tracks, on=&#39;gid&#39;, how=&#39;left&#39;, indicator=True) gid_list = genres_tracks.loc[genres_tracks[&#39;_merge&#39;] == &#39;left_only&#39;,&#39;gid&#39;] non_top_genres = genres[genres[&#39;gid&#39;].isin(gid_list)] . Concatenate DataFrames together vertically . # Basic concatenation pd.concat([inv_jan, inv_feb, inv_mar]) # Ignoring the index pd.concat([inv_jan, inv_feb, inv_mar], ignore_index=True) # Setting labels to original tables pd.concat([inv_jan, inv_feb, inv_mar], ignore_index=False, keys=[&#39;jan&#39;,&#39;feb&#39;,&#39;mar&#39;]) # Concatenate tables with different column names pd.concat([inv_jan, inv_feb], sort=True) # Concatenate tables with different column names pd.concat([inv_jan, inv_feb], join=&#39;inner&#39;) # Append the tables inv_jan.append([inv_feb, inv_mar], ignore_index=True, sort=True) . Verifying integrity . # Validating merges .merge(validate=None) : Checks if merge is of specified type &#39;one_to_one&#39; &#39;one_to_many&#39; &#39;many_to_one&#39; &#39;many_to_many&#39; # Merge validate: one_to_one tracks.merge(specs, on=&#39;tid&#39;, validate=&#39;one_to_one&#39;) # Merge validate: one_to_many albums.merge(tracks, on=&#39;aid&#39;, validate=&#39;one_to_many&#39;) # Verifying concatenations .concat(verify_integrity=False) : Check whether the new concatenated index contains duplicates Default value is False . Merging Ordered and Time-Series Data . Using merge_ordered() . # Merging stock data import pandas as pd pd.merge_ordered(appl, mcd, on=&#39;date&#39;, suffixes=(&#39;_aapl&#39;,&#39;_mcd&#39;)) # Forward fill example pd.merge_ordered(appl, mcd, on=&#39;date&#39;, suffixes=(&#39;_aapl&#39;,&#39;_mcd&#39;), fill_method=&#39;ffill&#39;) . Using merge_asof() . # merge_asof() example pd.merge_asof(visa, ibm, on=&#39;date_time&#39;, suffixes=(&#39;_visa&#39;,&#39;_ibm&#39;)) # merge_asof() example with direction pd.merge_asof(visa, ibm, on=[&#39;date_time&#39;], suffixes=(&#39;_visa&#39;,&#39;_ibm&#39;), direction=&#39;forward&#39;) . Selecting data with .query() . # Querying on a single condition stocks.query(&#39;nike &gt;= 90&#39;) # Querying on a multiple conditions, &quot;and&quot;, &quot;or&quot; stocks.query(&#39;nike &gt; 90 and disney &lt; 140&#39;) stocks.query(&#39;nike &gt; 96 or disney &lt; 98&#39;) # Using .query() to select text stocks_long.query(&#39;stock==&quot;disney&quot; or (stock==&quot;nike&quot; and close &lt; 90)&#39;) . Reshaping data with .melt() . # Example of .melt() social_fin_tall = social_fin.melt(id_vars=[&#39;financial&#39;,&#39;company&#39;]) # Melting with value_vars social_fin_tall = social_fin.melt(id_vars=[&#39;financial&#39;,&#39;company&#39;], value_vars=[&#39;2018&#39;,&#39;2017&#39;]) # Melting with column names social_fin_tall = social_fin.melt(id_vars=[&#39;financial&#39;,&#39;company&#39;], value_vars=[&#39;2018&#39;,&#39;2017&#39;], var_name=[&#39;year&#39;], value_name=&#39;dollars&#39;) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/pandas-cheatsheet.html",
            "relUrl": "/blog/pandas-cheatsheet.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "matplotlib cheatsheet",
            "content": "Introduction to Data Visualization with Matplotlib . matplotlib cheatsheet in pdf . pdf lecture in github . Introduction to Matplotlib . Introduction to data visualization with Matplotlib . # Introducing the pyplot interface import matplotlib.pyplot as plt fig, ax = plt.subplots() plt.show() # Adding data to axes ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;]) plt.show() . Customizing your plots . # Adding markers ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-NORMAL&quot;], marker=&quot;o&quot;) plt.show() # Choosing markers ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-NORMAL&quot;], marker=&quot;v&quot;) plt.show() . markers . # Setting the linestyle fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;], marker=&quot;v&quot;, linestyle=&quot;--&quot;) plt.show() . line style . # Eliminating lines with linestyle fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;], marker=&quot;v&quot;, linestyle=&quot;None&quot;) plt.show() # Choosing color fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;], marker=&quot;v&quot;, linestyle=&quot;--&quot;, color=&quot;r&quot;) plt.show() # Customizing the axes labels ax.set_xlabel(&quot;Time (months)&quot;) plt.show() # Setting the y axis label ax.set_xlabel(&quot;Time (months)&quot;) ax.set_ylabel(&quot;Average temperature (Fahrenheit degrees)&quot;) plt.show() # Adding a title ax.set_title(&quot;Weather in Seattle&quot;) plt.show() . Small multiples . # Small multiples with plt.subplots fig, ax = plt.subplots(3, 2) plt.show() # Adding data to subplots ax.shape (3, 2) ax[0, 0].plot(seattle_weather[&quot;MONTH&quot;],seattle_weather[&quot;MLY-PRCP-NORMAL&quot;],color=&#39;b&#39;) plt.show() # Subplots with data fig, ax = plt.subplots(2, 1) ax[0].plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-NORMAL&quot;],color=&#39;b&#39;) ax[0].plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-25PCTL&quot;],linestyle=&#39;--&#39;, color=&#39;b&#39;) ax[0].plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-75PCTL&quot;],linestyle=&#39;--&#39;, color=&#39;b&#39;) ax[1].plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-PRCP-NORMAL&quot;],color=&#39;r&#39;) ax[1].plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-PRCP-25PCTL&quot;],linestyle=&#39;--&#39;, color=&#39;r&#39;) ax[1].plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-PRCP-75PCTL&quot;],linestyle=&#39;--&#39;, color=&#39;r&#39;) ax[0].set_ylabel(&quot;Precipitation (inches)&quot;) ax[1].set_ylabel(&quot;Precipitation (inches)&quot;) ax[1].set_xlabel(&quot;Time (months)&quot;) plt.show() # Sharing the y-axis range fig, ax = plt.subplots(2, 1, sharey=True) . Plotting time-series . Plotting time-series data . # DateTimeIndex climate_change.index DatetimeIndex([&#39;1958-03-06&#39;, &#39;1958-04-06&#39;, &#39;1958-05-06&#39;, &#39;1958-06-06&#39;, dtype=&#39;datetime64[ns]&#39;, name=&#39;date&#39;, length=706, freq=None) # Plotting time-series data import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&#39;co2&#39;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;) plt.show() # Zooming in on a decade sixties = climate_change[&quot;1960-01-01&quot;:&quot;1969-12-31&quot;] fig, ax = plt.subplots() ax.plot(sixties.index, sixties[&#39;co2&#39;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;) plt.show() # Zooming in on one year sixty_nine = climate_change[&quot;1969-01-01&quot;:&quot;1969-12-31&quot;] fig, ax = plt.subplots() ax.plot(sixty_nine.index, sixty_nine[&#39;co2&#39;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;) plt.show() . Plotting time-series with different variables . # Plotting two time-series together import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&quot;co2&quot;]) ax.plot(climate_change.index, climate_change[&quot;relative_temp&quot;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm) / Relative temperature&#39;) plt.show() # Using twin axes fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&quot;co2&quot;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[&quot;relative_temp&quot;]) ax2.set_ylabel(&#39;Relative temperature (Celsius)&#39;) plt.show() # Separating variables by color fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&quot;co2&quot;], color=&#39;blue&#39;) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;, color=&#39;blue&#39;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[&quot;relative_temp&quot;], color=&#39;red&#39;) ax2.set_ylabel(&#39;Relative temperature (Celsius)&#39;, color=&#39;red&#39;) plt.show() # Coloring the ticks fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&quot;co2&quot;], color=&#39;blue&#39;) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;, color=&#39;blue&#39;) ax.tick_params(&#39;y&#39;, colors=&#39;blue&#39;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[&quot;relative_temp&quot;], color=&#39;red&#39;) ax2.set_ylabel(&#39;Relative temperature (Celsius)&#39;, color=&#39;red&#39;) ax2.tick_params(&#39;y&#39;, colors=&#39;red&#39;) plt.show() # A function that plots time-series def plot_timeseries(axes, x, y, color, xlabel, ylabel): axes.plot(x, y, color=color) axes.set_xlabel(xlabel) axes.set_ylabel(ylabel, color=color) axes.tick_params(&#39;y&#39;, colors=color) # Using our function fig, ax = plt.subplots() plot_timeseries(ax, climate_change.index, climate_change[&#39;co2&#39;],&#39;blue&#39;, &#39;Time&#39;, &#39;CO2 (ppm)&#39;) ax2 = ax.twinx() plot_timeseries(ax, climate_change.index,climate_change[&#39;relative_temp&#39;],&#39;red&#39;, &#39;Time&#39;, &#39;Relative temperature (Celsius)&#39;) plt.show() . Annotating time-series data . # Annotation fig, ax = plt.subplots() plot_timeseries(ax, climate_change.index, climate_change[&#39;co2&#39;], &#39;blue&#39;, &#39;Time&#39;, &#39;CO2 (ppm)&#39;) ax2 = ax.twinx() plot_timeseries(ax2, climate_change.index, climate_change[&#39;relative_temp&#39;], &#39;red&#39;, &#39;Time&#39;, &#39;Relative temperature (Celsius)&#39;) ax2.annotate(&quot;&gt;1 degree&quot;, xy=[pd.TimeStamp(&quot;2015-10-06&quot;), 1]) plt.show() # Positioning the text ax2.annotate(&quot;&gt;1 degree&quot;, xy=(pd.Timestamp(&#39;2015-10-06&#39;), 1), xytext=(pd.Timestamp(&#39;2008-10-06&#39;), -0.2)) # Adding arrows to annotation ax2.annotate(&quot;&gt;1 degree&quot;, xy=(pd.Timestamp(&#39;2015-10-06&#39;), 1), xytext=(pd.Timestamp(&#39;2008-10-06&#39;), -0.2), arrowprops={}) # Customizing arrow properties ax2.annotate(&quot;&gt;1 degree&quot;, xy=(pd.Timestamp(&#39;2015-10-06&#39;), 1), xytext=(pd.Timestamp(&#39;2008-10-06&#39;), -0.2), arrowprops={&quot;arrowstyle&quot;:&quot;-&gt;&quot;, &quot;color&quot;:&quot;gray&quot;}) . Customizing annotations . Quantitative comparisons and statistical visualizations . Quantitative comparisons: bar-charts . # Olympic medals: visualizing the data medals = pd.read_csv(&#39;medals_by_country_2016.csv&#39;, index_col=0) fig, ax = plt.subplots() ax.bar(medals.index, medals[&quot;Gold&quot;]) plt.show() # Interlude: rotate the tick labels fig, ax = plt.subplots() ax.bar(medals.index, medals[&quot;Gold&quot;]) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) plt.show() # Olympic medals: visualizing the other medals : stacked bar chart fig, ax = plt.subplots ax.bar(medals.index, medals[&quot;Gold&quot;]) ax.bar(medals.index, medals[&quot;Silver&quot;], bottom=medals[&quot;Gold&quot;]) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) plt.show() # Olympic medals: visualizing all three fig, ax = plt.subplots ax.bar(medals.index, medals[&quot;Gold&quot;]) ax.bar(medals.index, medals[&quot;Silver&quot;], bottom=medals[&quot;Gold&quot;]) ax.bar(medals.index, medals[&quot;Bronze&quot;], bottom=medals[&quot;Gold&quot;] + medals[&quot;Silver&quot;]) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) plt.show() # Adding a legend fig, ax = plt.subplots ax.bar(medals.index, medals[&quot;Gold&quot;], label=&quot;Gold&quot;) ax.bar(medals.index, medals[&quot;Silver&quot;], bottom=medals[&quot;Gold&quot;], label=&quot;Silver&quot;) ax.bar(medals.index, medals[&quot;Bronze&quot;], bottom=medals[&quot;Gold&quot;] + medals[&quot;Silver&quot;], label=&quot;Bronze&quot;) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) ax.legend() plt.show() . Quantitative comparisons: histograms . # Introducing histograms fig, ax = plt.subplots() ax.hist(mens_rowing[&quot;Height&quot;]) ax.hist(mens_gymnastic[&quot;Height&quot;]) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) plt.show() # Labels are needed ax.hist(mens_rowing[&quot;Height&quot;], label=&quot;Rowing&quot;) ax.hist(mens_gymnastic[&quot;Height&quot;], label=&quot;Gymnastics&quot;) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) ax.legend() plt.show() # Customizing histograms: setting the number of bins ax.hist(mens_rowing[&quot;Height&quot;], label=&quot;Rowing&quot;, bins=5) ax.hist(mens_gymnastic[&quot;Height&quot;], label=&quot;Gymnastics&quot;, bins=5) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) ax.legend() plt.show() # Customizing histograms: setting bin boundaries ax.hist(mens_rowing[&quot;Height&quot;], label=&quot;Rowing&quot;, bins=[150, 160, 170, 180, 190, 200, 210]) ax.hist(mens_gymnastic[&quot;Height&quot;], label=&quot;Gymnastics&quot;, bins=[150, 160, 170, 180, 190, 200, 210]) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) ax.legend() plt.show() # Customizing histograms: transparency ax.hist(mens_rowing[&quot;Height&quot;], label=&quot;Rowing&quot;, bins=[150, 160, 170, 180, 190, 200, 210], histtype=&quot;step&quot;) ax.hist(mens_gymnastic[&quot;Height&quot;], label=&quot;Gymnastics&quot;, bins=[150, 160, 170, 180, 190, 200, 210], histtype=&quot;step&quot;) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) ax.legend() plt.show() . Statistical plotting . # Adding error bars to bar charts fig, ax = plt.subplots() ax.bar(&quot;Rowing&quot;,mens_rowing[&quot;Height&quot;].mean(), yerr=mens_rowing[&quot;Height&quot;].std()) ax.bar(&quot;Gymnastics&quot;,mens_gymnastics[&quot;Height&quot;].mean(), yerr=mens_gymnastics[&quot;Height&quot;].std()) ax.set_ylabel(&quot;Height (cm)&quot;) plt.show() # Adding error bars to plots fig, ax = plt.subplots() ax.errorbar(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;], yerr=seattle_weather[&quot;MLY-TAVG-STDDEV&quot;]) ax.errorbar(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-TAVG-NORMAL&quot;], yerr=austin_weather[&quot;MLY-TAVG-STDDEV&quot;]) ax.set_ylabel(&quot;Temperature (Fahrenheit)&quot;) plt.show() # Adding boxplots fig, ax = plt.subplots() ax.boxplot([mens_rowing[&quot;Height&quot;], mens_gymnastics[&quot;Height&quot;]]) ax.set_xticklabels([&quot;Rowing&quot;, &quot;Gymnastics&quot;]) ax.set_ylabel(&quot;Height (cm)&quot;) plt.show() . Quantitative comparisons: scatter plots . # Introducing scatter plots fig, ax = plt.subplots() ax.scatter(climate_change[&quot;co2&quot;], climate_change[&quot;relative_temp&quot;]) ax.set_xlabel(&quot;CO2 (ppm)&quot;) ax.set_ylabel(&quot;Relative temperature (Celsius)&quot;) plt.show() # Customizing scatter plots eighties = climate_change[&quot;1980-01-01&quot;:&quot;1989-12-31&quot;] nineties = climate_change[&quot;1990-01-01&quot;:&quot;1999-12-31&quot;] fig, ax = plt.subplots() ax.scatter(eighties[&quot;co2&quot;], eighties[&quot;relative_temp&quot;], color=&quot;red&quot;, label=&quot;eighties&quot;) ax.scatter(nineties[&quot;co2&quot;], nineties[&quot;relative_temp&quot;], color=&quot;blue&quot;, label=&quot;nineties&quot;) ax.legend() ax.set_xlabel(&quot;CO2 (ppm)&quot;) ax.set_ylabel(&quot;Relative temperature (Celsius)&quot;) plt.show() # Encoding a third variable by color fig, ax = plt.subplots() ax.scatter(climate_change[&quot;co2&quot;], climate_change[&quot;relative_temp&quot;], c=climate_change.index) ax.set_xlabel(&quot;CO2 (ppm)&quot;) ax.set_ylabel(&quot;Relative temperature (Celsius)&quot;) plt.show() . Sharing visualizations with others . Preparing your figures to share with others . # Choosing a style plt.style.use(&quot;ggplot&quot;) fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot; ax.plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-TAVG-NORMAL&quot;]) ax.set_xlabel(&quot;Time (months)&quot;) ax.set_ylabel(&quot;Average temperature (Fahrenheit degrees)&quot;) plt.show() # Back to the default plt.style.use(&quot;default&quot;) . available styles . # The &quot;bmh&quot; style plt.style.use(&quot;bmh&quot;) fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot; ax.plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-TAVG-NORMAL&quot;]) ax.set_xlabel(&quot;Time (months)&quot;) ax.set_ylabel(&quot;Average temperature (Fahrenheit degrees)&quot;) plt.show() # Seaborn styles plt.style.use(&quot;seaborn-colorblind&quot;) fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot; ax.plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-TAVG-NORMAL&quot;]) ax.set_xlabel(&quot;Time (months)&quot;) ax.set_ylabel(&quot;Average temperature (Fahrenheit degrees)&quot;) plt.show() . Saving your visualizations . # Saving the figure to file fig, ax = plt.subplots() ax.bar(medals.index, medals[&quot;Gold&quot;]) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) fig.savefig(&quot;gold_medals.png&quot;) # Different file formats fig.savefig(&quot;gold_medals.jpg&quot;) fig.savefig(&quot;gold_medals.jpg&quot;, quality=50) fig.savefig(&quot;gold_medals.svg&quot;) # Resolution fig.savefig(&quot;gold_medals.png&quot;, dpi=300) # Size fig.set_size_inches([5, 3]) # Another aspect ratio fig.set_size_inches([3, 5]) . Automating figures from data . # Getting unique values of a column sports = summer_2016_medals[&quot;Sport&quot;].unique() # Bar-chart of heights for all sports fig, ax = plt.subplots() for sport in sports: sport_df = summer_2016_medals[summer_2016_medals[&quot;Sport&quot;] == spor ax.bar(sport, sport_df[&quot;Height&quot;].mean(), yerr=sport_df[&quot;Height&quot;].std()) ax.set_ylabel(&quot;Height (cm)&quot;) ax.set_xticklabels(sports, rotation=90) plt.show() .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/matplotlib-cheatsheet.html",
            "relUrl": "/blog/matplotlib-cheatsheet.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "How To Install Packages from the Jupyter Notebook",
            "content": "This is directly from https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/. . Here are my own experimentations following this article detailed explanations. . Quick Fix: How To Install Packages from the Jupyter Notebook . . import sys !conda install --yes --prefix {sys.prefix} matplotlib . Collecting package metadata (current_repodata.json): done Solving environment: done # All requested packages already installed. . The Details: Why is Installation from Jupyter so Messy? . How your operating system locates executables . !echo $PATH . /home/explore/gems/bin:/home/explore/miniconda3/envs/pytorch/bin:/home/explore/miniconda3/condabin:/home/explore/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin . !type python . python is /home/explore/miniconda3/envs/pytorch/bin/python . You can optionally add the -a tag to see all available versions of the command in your current shell environment; for example: . !type -a python . python is /home/explore/miniconda3/envs/pytorch/bin/python python is /usr/bin/python . !type -a conda . conda is /home/explore/miniconda3/condabin/conda . How Python locates packages . import sys sys.path . [&#39;/home/explore/git/guillaume/blog/_notebooks&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python38.zip&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/lib-dynload&#39;, &#39;&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages/IPython/extensions&#39;, &#39;/home/explore/.ipython&#39;] . By default, the first place Python looks for a module is an empty path, meaning the current working directory. If the module is not found there, it goes down the list of locations until the module is found. You can find out which location has been used using the __path__ attribute of an imported module: . import numpy numpy.__path__ . [&#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages/numpy&#39;] . by printing the sys.path variables for each of the available python executables in my path, using Jupyter&#39;s delightful ability to mix Python and bash commands in a single code block: . paths = !type -a python for path in set(paths): path = path.split()[-1] print(path) !{path} -c &quot;import sys; print(sys.path)&quot; print() . /usr/bin/python [&#39;&#39;, &#39;/usr/lib/python2.7&#39;, &#39;/usr/lib/python2.7/plat-x86_64-linux-gnu&#39;, &#39;/usr/lib/python2.7/lib-tk&#39;, &#39;/usr/lib/python2.7/lib-old&#39;, &#39;/usr/lib/python2.7/lib-dynload&#39;, &#39;/home/explore/.local/lib/python2.7/site-packages&#39;, &#39;/usr/local/lib/python2.7/dist-packages&#39;, &#39;/usr/local/lib/python2.7/dist-packages/PyCapture2-0.0.0-py2.7-linux-x86_64.egg&#39;, &#39;/usr/lib/python2.7/dist-packages&#39;] /home/explore/miniconda3/envs/pytorch/bin/python [&#39;&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python38.zip&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/lib-dynload&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages&#39;] . pip install will install in the Python in the same path: . !type pip . pip is /home/explore/miniconda3/envs/pytorch/bin/pip . conda install will install in the active conda envt . !conda env list . # conda environments: # base /home/explore/miniconda3 d059 /home/explore/miniconda3/envs/d059 datacamp /home/explore/miniconda3/envs/datacamp deeplearning_specialization /home/explore/miniconda3/envs/deeplearning_specialization deeplearning_specialization_keras /home/explore/miniconda3/envs/deeplearning_specialization_keras deeplearning_specialization_tf1 /home/explore/miniconda3/envs/deeplearning_specialization_tf1 drl_handson /home/explore/miniconda3/envs/drl_handson fastai /home/explore/miniconda3/envs/fastai gan /home/explore/miniconda3/envs/gan gan_tensorflow /home/explore/miniconda3/envs/gan_tensorflow mit_6002x /home/explore/miniconda3/envs/mit_6002x pytorch * /home/explore/miniconda3/envs/pytorch squeezebox /home/explore/miniconda3/envs/squeezebox . The reason both pip and conda default to the conda pytorch environment is that this is the Python environment I used to launch the notebook. . How Jupyter executes code: Jupyter Kernels . !jupyter kernelspec list . Available kernels: python2 /home/explore/.local/share/jupyter/kernels/python2 python3 /home/explore/miniconda3/envs/pytorch/share/jupyter/kernels/python3 . !cat /home/explore/miniconda3/envs/pytorch/share/jupyter/kernels/python3/kernel.json . { &#34;argv&#34;: [ &#34;/home/explore/miniconda3/envs/pytorch/bin/python&#34;, &#34;-m&#34;, &#34;ipykernel_launcher&#34;, &#34;-f&#34;, &#34;{connection_file}&#34; ], &#34;display_name&#34;: &#34;Python 3&#34;, &#34;language&#34;: &#34;python&#34; } . If you&#39;d like to create a new kernel, you can do so using the jupyter ipykernel command; for example, I created the above kernels for my primary conda environments using the following as a template: . $ source activate myenv $ python -m ipykernel install --user --name myenv --display-name &quot;Python (myenv)&quot; . The Root of the Issue . The root of the issue is this: the shell environment is determined when the Jupyter notebook is launched, while the Python executable is determined by the kernel, and the two do not necessarily match. In other words, there is no guarantee that the python, pip, and conda in your $PATH will be compatible with the python executable used by the notebook. . Recall that the python in your path can be determined using . !type python . python is /home/explore/miniconda3/envs/pytorch/bin/python . The Python executable being used in the notebook can be determined using . sys.executable . &#39;/home/explore/miniconda3/envs/pytorch/bin/python&#39; . Notewhen the 2 differs, boom! .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/installing-python-packages-from-jupyter.html",
            "relUrl": "/blog/installing-python-packages-from-jupyter.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Hello nbdev",
            "content": "Resources . Everything is under nbdev website. . 3 resources worth to be mentioning: . nbdev tutorial video on youtube; 1 year old but seems still valid | nbdev tutorial page | nbdev github repo | . What I plan to do is to watch the video part, and keep note of my progress in this blog entry. . Walkthrough tutorial . repo creation . As suggested by Jeremy, I start by creating a github repo named hello_nbdev from a nbdev template. . It is just about clicking this link: https://github.com/fastai/nbdev_template/generate. If I am logged in github it will show the proper page. . github pages . Documentation will be hosted at github (can be hosted anywhere but github seems a straightforward option) and to do that we have to setup github pages: . Settings &gt; Options &gt; Github pages &gt; Source &gt; Master (branch) &gt; /docs (folder) &gt; Save . And when done . Now we can insert this doc url as our repo website setting: . repo home &gt; &lt;&gt; code &gt; about (edit repo details) &gt; Website . Edit settings.ini . Everything is in this file. . Just edit directly from github. . lib_name = nbdev_template # For Enterprise Git add variable repo_name and company name # repo_name = analytics # company_name = nike user = fastai # description = A description of your project # keywords = some keywords # author = Your Name # author_email = email@example.com # copyright = Your Name or Company Name . to . lib_name = hello_nbdev user = castorfou description = A tutorial walkthrough with nbdev keywords = fastai nbdev tutorial author = Guillaume Ramelet author_email = guillaume.ramelet@gmail.com copyright = Guillaume R. . and commit changes . Clone repo . ~/git/guillaume$ git clone git@github.com:castorfou/hello_nbdev.git Cloning into &#39;hello_nbdev&#39;... remote: Enumerating objects: 106, done. remote: Counting objects: 100% (106/106), done. remote: Compressing objects: 100% (94/94), done. remote: Total 106 (delta 7), reused 81 (delta 4), pack-reused 0 Receiving objects: 100% (106/106), 1.02 MiB | 2.45 MiB/s, done. Resolving deltas: 100% (7/7), done. . Setup nbdev python environment . It is not specifically mentionned in the video. For this walkthrough I will use my existing fastai environment. . ~/git/guillaume$ conda activate fastai ~/git/guillaume$ nbdev_ nbdev_build_docs nbdev_diff_nbs nbdev_test_nbs nbdev_build_lib nbdev_fix_merge nbdev_trust_nbs nbdev_bump_version nbdev_install_git_hooks nbdev_update_lib nbdev_clean_nbs nbdev_nb2md nbdev_upgrade nbdev_conda_package nbdev_new nbdev_detach nbdev_read_nbs . As expected nbdev is already integrated in it. . Otherwise my guess is that I have to run conda install -c fastai nbdev under my python env. . Install git hooks . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_install_git_hooks Executing: git config --local include.path ../.gitconfig Success: hooks are installed and repo&#39;s .gitconfig is now trusted . deal with conflicts . If needed in case of conflict, Jeremy explains one can call nbdev_fix_merge filename.ipynb and it will use the standard conflict marker to help you identify and fix the conflict. . Open 00_core.ipynb . create lib (we start with a core module) . Just following Jeremy&#39;s instructions. . Create say_hello function | Use it (example) | Test it (assert) | . build_lib . We can call nbdev_build_lib from anywhere in the repo. . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_build_lib Converted 00_core.ipynb. Converted index.ipynb. . and it creates files, under hello_nbdev . hello_nbdev$ ls hello_nbdev/ core.py __init__.py _nbdev.py __pycache__ . Module Documentation . There are 2 levels of documentation. Documentation for your library that will be in index.ipynb and documentation for your modules that will be directly created from your code/notebooks 00_core.ipynb, etc . And to generate this documentation it will be just a matter of calling nbdev_build_docs. . Library documentation into index.ipynb . create doc . Documentation (what will be puclished) is in index.ipynb. . This is an actual documentation. Documentation won&#39;t be written in markdown. It will be executed as code and rendered as such. How great is that. . To make it happen we have to import our lib just freshly generated. . And now we can use all the part of our lib to explain how it works and why it is great. . say_hello(&quot;Guillaume&quot;) . &#39;Hello Guillaume!&#39; . build_docs . We have to call nbdev_build_docs from our repo root. . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_build_docs converting: /home/explore/git/guillaume/hello_nbdev/00_core.ipynb converting /home/explore/git/guillaume/hello_nbdev/index.ipynb to README.md . commit to publish docs . Here is the list of files to be pushed: . git status Changes to be committed: modified: 00_core.ipynb new file: 00_core.py new file: Makefile modified: README.md new file: docs/_config.yml modified: docs/_data/sidebars/home_sidebar.yml new file: docs/_data/topnav.yml new file: docs/core.html new file: docs/index.html modified: docs/sidebar.json new file: hello_nbdev/__init__.py new file: hello_nbdev/_nbdev.py new file: hello_nbdev/core.py modified: index.ipynb new file: index.py . init.py . Just add from .core import * to __init__.py . !cat /home/explore/git/guillaume/hello_nbdev/hello_nbdev/__init__.py . __version__ = &#34;0.0.1&#34; from .core import * . So that we can easily use hello_nbdev without mentioning core module . commit and push . (fastai) ~/git/guillaume/hello_nbdev$ git commit -m &#39;initial commit&#39; [master 3484db7] initial commit 15 files changed, 520 insertions(+), 31 deletions(-) create mode 100644 00_core.py create mode 100644 Makefile create mode 100644 docs/_config.yml create mode 100644 docs/_data/topnav.yml create mode 100644 docs/core.html create mode 100644 docs/index.html create mode 100644 hello_nbdev/__init__.py create mode 100644 hello_nbdev/_nbdev.py create mode 100644 hello_nbdev/core.py create mode 100644 index.py (fastai) ~/git/guillaume/hello_nbdev$ git push Enumerating objects: 30, done. Counting objects: 100% (30/30), done. Delta compression using up to 12 threads Compressing objects: 100% (19/19), done. Writing objects: 100% (21/21), 4.87 KiB | 2.44 MiB/s, done. Total 21 (delta 7), reused 0 (delta 0) remote: Resolving deltas: 100% (7/7), completed with 5 local objects. remote: remote: GitHub found 1 vulnerability on castorfou/hello_nbdev&#39;s default branch (1 low). To find out more, visit: remote: https://github.com/castorfou/hello_nbdev/security/dependabot/docs/Gemfile.lock/nokogiri/open remote: To github.com:castorfou/hello_nbdev.git 3aec9f4..3484db7 master -&gt; master . And documentation is ready . https://castorfou.github.io/hello_nbdev/ . . Going further . Now that we have a 1st simple example up and running, we can go further with: . classes | autoreload tip | launch nbdev_build_lib from jupyter | run tests in parallel | . Classes . Following tutorial, we can create class HelloSayer and document our methods by calling show_doc(HelloSayer.say). . We can decide to add entries into index.ipynb if this is something worth having at the library level. . autoreload . By adding these lines . %load_ext autoreload %autoreload 2 . your notebook automatically reads in the new modules as soon as the python file changes . launch nbdev scripts directly from jupyter . Make it your last cell . from nbdev.export import notebook2script; notebook2script() . Converted 00_core.ipynb. Converted index.ipynb. . run tests in parallel . Just run nbdev_test_nbs . If your notebook starts with _, it will be excluded from the test list. . Jekyll to view documentation locally . Jekyll is the web server to properly render documentation. This is what is used at github pages. . Installation and setup . From https://jekyllrb.com/docs/installation/ubuntu/, . sudo apt-get install ruby-full build-essential zlib1g-dev . Install variables to use gem: . echo &#39;# Install Ruby Gems to ~/gems&#39; &gt;&gt; ~/.bashrc echo &#39;export GEM_HOME=&quot;$HOME/gems&quot;&#39; &gt;&gt; ~/.bashrc echo &#39;export PATH=&quot;$HOME/gems/bin:$PATH&quot;&#39; &gt;&gt; ~/.bashrc source ~/.bashrc . Install Jekyll and Builder: . gem install jekyll bundler . Setup our lib to use Jekyll . From our docs folder, launch bundle install . (fastai) ~/git/guillaume/hello_nbdev/docs$ bundle install Fetching gem metadata from https://rubygems.org/......... Using concurrent-ruby 1.1.7 .... Bundle complete! 4 Gemfile dependencies, 90 gems now installed. Use `bundle info [gemname]` to see where a bundled gem is installed. . Use it . From repo root, launch make docs_serve . (fastai) ~/git/guillaume/hello_nbdev$ make docs_serve cd docs &amp;&amp; bundle exec jekyll serve Configuration file: /home/explore/git/guillaume/hello_nbdev/docs/_config.yml Source: /home/explore/git/guillaume/hello_nbdev/docs Destination: /home/explore/git/guillaume/hello_nbdev/docs/_site Incremental build: disabled. Enable with --incremental Generating... GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data. done in 0.098 seconds. /home/explore/gems/gems/pathutil-0.16.2/lib/pathutil.rb:502: warning: Using the last argument as keyword parameters is deprecated Auto-regeneration: enabled for &#39;/home/explore/git/guillaume/hello_nbdev/docs&#39; Server address: http://127.0.0.1:4000/hello_nbdev// Server running... press ctrl-c to stop. . It is available locally at http://127.0.0.1:4000/hello_nbdev/ . . Skipped . I have not gone through pypi publication and console_scripts. . I don&#39;t have the need for the moment, if I need that I will add an entry here. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/nbdev_tutorial.html",
            "relUrl": "/blog/nbdev_tutorial.html",
            "date": " • Jan 12, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "About my datacamp learning process",
            "content": "Datacamp . I started learning with Datacamp in March 2019. This is a great resource and I recommend all datascience newcomers to give it a shot. . What I like are the consistent courses content. There is an overall logic between all courses. And content is just incredible: more than 300 interactive courses. OK maybe you won&#39;t find all of them super useful but at least you can pick what is of interest for you. Following my learning process it takes me about 8 hours to complete a course. . . Career tracks are a smart way to help you build a 1st tour in your datascience journey. I followed python programmer (old version), data scientist with python (old version) and machine learning scientist with python tracks. Mileage may vary but it is about 20 courses per track. Updated versions of tracks are now online and this is a mix between courses, projects and skills assessments. I have tested one project but it is a little bit too basic for me. . . There is a nice and smooth progress tracking system, and as in a game you earn XP for each achivement. . . Selecting courses . A natural way to select courses is to browse through courses from career tracks. And I will complete courses from new version of career tracks. Or when I need to learn on a new domain, I just search for relevant courses (search engine is very good). . I have 2 ways to track these courses: . bookmarks in Datacamp | . entries in ITP (individual training plan, a big excel list of learning items I plan to follow) | . Learning process . Starting a project . As an example I will use . . which is a project from the new Data Scientist career track and which is in my ITP: . . Git repo - data-scientist-skills . In my data-scientist-skills github repo, I have 2 folders: . Other datacamp courses - where I keep lectures (pdf slides) from datacamp courses | python-sandbox - where I keep notebooks and data from datacamp exercises | . creation of Data Manipulation with pandas folder under Other datacamp courses | creation of data-manipulation-with-pandas folder under python-sandbox | copy of python-sandbox/_1project-template/ into python-sandbox/data-manipulation-with-pandas | . Datacamp project template . In this project template, . data_from_datacamp will store all data needed to launch datacamp exercises | exports_py will contain exports of notebooks in txt/py format (usefull to search on code patterns) | start_env.sh start_env.bat to launch jupyter notebook from the right conda env | downloadfromFileIO.py to download data files from my local notebooks (using in the background file.io) | uploadfromdatacamp.py to upload data files from datacamp | uploadfromdatacamp_examples.py some examples to transfer dataframes, dataseries, lists, ... | . Projects structure . After initialisation, I have the following structure and content: . . On your left lectures (one per chapter) and final certificate. . On your right notebooks. . Notebooks for exercises . Just run the jupyter notebook environment by calling start_env.sh. . Get the chapter title: . . And name the notebook accordingly: . . Then enter interactive instructions. I copy paste instructions using copy selection as markdown firefox add-on. . . Here in this example, if I want to follow instructions locally I need to have homelessness dataframe. . I can use the following code from uploadfromdatacamp_examples.py . ##### Dataframe ################### #upload and download from downloadfromFileIO import saveFromFileIO &quot;&quot;&quot; à executer sur datacamp: (apres copie du code uploadfromdatacamp.py) uploadToFileIO(homelessness) &quot;&quot;&quot; tobedownloaded=&quot;&quot;&quot; {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} &quot;&quot;&quot; prefixToc=&#39;1.1&#39; prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=&quot;&quot;) #initialisation import pandas as pd homelessness = pd.read_csv(prefix+&#39;homelessness.csv&#39;,index_col=0) . Before executing this cell, I have to copy/paste/execute uploadfromdatacamp.py content on datacamp server. And call . uploadToFileIO(homelessness) . Then get the results last line . In [2]: uploadToFileIO(homelessness) {&quot;success&quot;:true,&quot;key&quot;:&quot;vTM1t2ehXds4&quot;,&quot;link&quot;:&quot;https://file.io/vTM1t2ehXds4&quot;,&quot;expiry&quot;:&quot;14 days&quot;} {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} . and copy it in tobedownloaded variable. . Update prefixTOC to the good value (exercise 1.1 is the 1st one in first chapter) which is used as a prefix in data files. And update local variable name and csv file. . Run the cell . Here is the result . Téléchargements à lancer {&#39;pandas.core.frame.DataFrame&#39;: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2528 0 2528 0 0 4870 0 --:--:-- --:--:-- --:--:-- 4870 . And homelessness is available to be used. . Files downloaded are in data_from_datacamp folder. . . And running again the cell won&#39;t download file from file.io, but will read the cached file. (delete file to force download) . Full content of this notebook example at the bottom . keep content in git . ~/git/guillaume/data-scientist-skills$ git add . ~/git/guillaume/data-scientist-skills$ git commit -m &#39;start of data manipulation in pandas course&#39; [master c8696ce] start of data manipulation in pandas course 45 files changed, 9010 insertions(+) create mode 100644 Other datacamp courses/Data Manipulation with pandas/chapter1.pdf create mode 100644 python-sandbox/data-manipulation-with-pandas/.ipynb_checkpoints/chapter1 - Transforming Data-checkpoint.ipynb create mode 100644 python-sandbox/data-manipulation-with-pandas/__pycache__/downloadfromFileIO.cpython-37.pyc create mode 100644 python-sandbox/data-manipulation-with-pandas/chapter1 - Transforming Data.ipynb create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/.empty_dir.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/chapter1 - Transforming Data-Exercise1.1_3277903540843719836.lock create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/chapter1 - Transforming Data-Exercise1.1_homelessness.csv create mode 100644 python-sandbox/data-manipulation-with-pandas/downloadfromFileIO.py create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/.empty_dir.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/Untitled.py create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/Untitled.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/chapter1 - Transforming Data.py create mode 100644 python-sandbox/data-manipulation-with-pandas/start_env.bat create mode 100755 python-sandbox/data-manipulation-with-pandas/start_env.sh create mode 100644 python-sandbox/data-manipulation-with-pandas/uploadfromdatacamp.py create mode 100644 python-sandbox/data-manipulation-with-pandas/uploadfromdatacamp_examples.py ~/git/guillaume/data-scientist-skills$ git push Enumerating objects: 43, done. Counting objects: 100% (43/43), done. Delta compression using up to 12 threads Compressing objects: 100% (38/38), done. Writing objects: 100% (40/40), 5.75 MiB | 3.85 MiB/s, done. Total 40 (delta 8), reused 1 (delta 0) remote: Resolving deltas: 100% (8/8), completed with 3 local objects. To github.com:castorfou/data-scientist-skills.git 89f60e5..c8696ce master -&gt; master . Update progress in ITP . Datacamp is giving instant progress . . So I regularly report this progress (here 0.18/4=5%) in ITP. . keep certificates . I download and keep certificates with lectures. . . Notebook example : Introducing DataFrames . Inspecting a DataFrame | Python . Inspecting a DataFrame . When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this. . .head() returns the first few rows (the “head” of the DataFrame). | .info() shows information on each of the columns, such as the data type and number of missing values. | .shape returns the number of rows and columns of the DataFrame. | .describe() calculates a few summary statistics for each column. | . homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state&#39;s total population. . pandas is imported for you. . init . ##### Dataframe ################### #upload and download from downloadfromFileIO import saveFromFileIO &quot;&quot;&quot; à executer sur datacamp: (apres copie du code uploadfromdatacamp.py) uploadToFileIO(homelessness) &quot;&quot;&quot; tobedownloaded=&quot;&quot;&quot; {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} &quot;&quot;&quot; prefixToc=&#39;1.1&#39; prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=&quot;&quot;) #initialisation import pandas as pd homelessness = pd.read_csv(prefix+&#39;homelessness.csv&#39;,index_col=0) . Téléchargements à lancer {&#39;pandas.core.frame.DataFrame&#39;: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2528 0 2528 0 0 4870 0 --:--:-- --:--:-- --:--:-- 4870 . code . Print the head of the homelessness DataFrame. . print(homelessness.head()) . region state individuals family_members state_pop 0 East South Central Alabama 2570.0 864.0 4887681 1 Pacific Alaska 1434.0 582.0 735139 2 Mountain Arizona 7259.0 2606.0 7158024 3 West South Central Arkansas 2280.0 432.0 3009733 4 Pacific California 109008.0 20964.0 39461588 . Print information about the column types and missing values in homelessness. . print(homelessness.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 51 entries, 0 to 50 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 region 51 non-null object 1 state 51 non-null object 2 individuals 51 non-null float64 3 family_members 51 non-null float64 4 state_pop 51 non-null int64 dtypes: float64(2), int64(1), object(2) memory usage: 2.4+ KB None . Print the number of rows and columns in homelessness. . print(homelessness.shape) . (51, 5) . Print some summary statistics that describe the homelessness DataFrame. . print(homelessness.describe()) . individuals family_members state_pop count 51.000000 51.000000 5.100000e+01 mean 7225.784314 3504.882353 6.405637e+06 std 15991.025083 7805.411811 7.327258e+06 min 434.000000 75.000000 5.776010e+05 25% 1446.500000 592.000000 1.777414e+06 50% 3082.000000 1482.000000 4.461153e+06 75% 6781.500000 3196.000000 7.340946e+06 max 109008.000000 52070.000000 3.946159e+07 . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Datacamp.html",
            "relUrl": "/blog/Datacamp.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Auto export python code from jupyter notebooks",
            "content": "This hack comes from https://github.com/jupyter/notebook/blob/master/docs/source/extending/savehooks.rst. . jupyter_notebook_config.py . Here is the code: . # Based off of https://github.com/jupyter/notebook/blob/master/docs/source/extending/savehooks.rst import io import os from notebook.utils import to_api_path _script_exporter = None _html_exporter = None def script_post_save(model, os_path, contents_manager, **kwargs): &quot;&quot;&quot;convert notebooks to Python script after save with nbconvert replaces `ipython notebook --script` &quot;&quot;&quot; from nbconvert.exporters.script import ScriptExporter from nbconvert.exporters.html import HTMLExporter if model[&#39;type&#39;] != &#39;notebook&#39;: return global _script_exporter if _script_exporter is None: _script_exporter = ScriptExporter(parent=contents_manager) log = contents_manager.log global _html_exporter if _html_exporter is None: _html_exporter = HTMLExporter(parent=contents_manager) log = contents_manager.log # save .py file base, ext = os.path.splitext(os_path) script, resources = _script_exporter.from_filename(os_path) # si le sous rep eports_py existe, on ecrit dedans, sinon on ecrit à la racine sous_rep=&#39;&#39; repertoire=os.path.dirname(base) if os.path.exists(repertoire+&#39;/exports_py&#39;): sous_rep=&#39;/exports_py&#39; basename = os.path.basename(base) script_fname = repertoire+ sous_rep+&#39;/&#39;+basename+resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;base: {}, basename: {}, sous_rep: {}, repertoire: {}&quot;.format(base, basename, sous_rep, repertoire)) log.info(&quot;script_fname: {}&quot;.format(script_fname)) #script_fname = base + resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;Saving script /%s&quot;, to_api_path(script_fname, contents_manager.root_dir)) with io.open(script_fname, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(script) &quot;&quot;&quot; # save html base, ext = os.path.splitext(os_path) script, resources = _html_exporter.from_filename(os_path) script_fname = base + resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;Saving html /%s&quot;, to_api_path(script_fname, contents_manager.root_dir)) with io.open(script_fname, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(script) &quot;&quot;&quot; c.FileContentsManager.post_save_hook = script_post_save . In this version, if a subfolder exports_py exists, .py version will be exported in it. Oherwise it will be exported in the notebook folder. . Maybe in a later version it would be good to export only of this subfolder exists. (for example I don&#39;t need these py files when creating such a blog entry, even if my .gitignore won&#39;t publish .py files) . And to remove the creation of Untitled.txt files when notebooks are just being created (and not yet named). . deployment . Just save/merge this jupyter_notebook_config.py file (download) to your jupyter home directory. . According to Config file and command line options in jupyter documentation, it is located at ~/.jupyter . And in windows it is at C: Users &lt;yourID&gt; .jupyter . This will be valid for all your conda environments. . test . ~/.jupyter$ cp ~/git/guillaume/blog/files/jupyter_notebook_config.py . . Restart Jupyter notebook server and click save on any notebook: . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/jupyter-export-notebook-as-py.html",
            "relUrl": "/blog/jupyter-export-notebook-as-py.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "From cron to anacron",
            "content": "Current config . I run generate_plots.sh daily at 9:30 AM. However what happens if my PC is off at that time, will have to wait another uptime at 9:30 AM. . Solution is to move from cron to anacron. . From https://www.putorius.net/cron-vs-anacron.html: . . Anacron as user . Anacron is already setup on ubuntu. Actually cron.daily is managed by anacron therefore everything under /etc/cron.daily is run even if your system was off at the time by crontab. . But it is true for root, and has to be setup for users. . I will follow these recommandations: https://askubuntu.com/a/235090 . .anacron folders . Create a .anacron folder in your home directory and in it two subfolders, etc and spool . !mkdir -p ~/.anacron/{etc,spool} . anacrontab . Create a new file ~/.anacron/etc/anacrontab with the following content: . # ~/.anacron/etc/anacrontab: configuration file for anacron # See anacron(8) and anacrontab(5) for details. SHELL=/bin/bash PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/home/explore/miniconda3/bin:/home/explore/miniconda3/condabin:/home/explore/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin # period delay job-identifier command 1 10 squeezebox ~/git/guillaume/squeezebox/generate_plots.sh . start anacron . Add the following line to your crontab using crontab -e: . @hourly /usr/sbin/anacron -s -t $HOME/.anacron/etc/anacrontab -S $HOME/.anacron/spool . And remove squeezebox entry from crontab. . !crontab -l . # NVIDIA SDK Manager updater # NVIDIA SDK Manager updater 0 12 */7 * * /bin/bash /home/explore/.nvsdkm/.updater/updater.sh #30 9 * * * ~/git/guillaume/squeezebox/generate_plots.sh @hourly /usr/sbin/anacron -s -t $HOME/.anacron/etc/anacrontab -S $HOME/.anacron/spool .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/from-cron-to-anacron.html",
            "relUrl": "/blog/from-cron-to-anacron.html",
            "date": " • Dec 13, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Repo with 2 remote-urls",
            "content": "from github . I have just created this empty repo: https://github.com/castorfou/data-scientist-skills . from existing local repo connected to gitlab . Add the new remote url to github, name it github . (base) guillaume@LL11LPC0PQARQ:~/git/data-scientist-skills$ git remote add github https://github.com/castorfou/data-scientist-skills.git . Or this is possible to use ssh protocol: git remote add origin git@github.com:castorfou/data-scientist-skills.git . Push repo to this new remote url: git push -u github . (base) guillaume@LL11LPC0PQARQ:~/git/data-scientist-skills$ git push -u github Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Counting objects: 4736, done. Delta compression using up to 8 threads. Compressing objects: 100% (3171/3171), done. Writing objects: 100% (4736/4736), 630.97 MiB | 9.08 MiB/s, done. Total 4736 (delta 1549), reused 4538 (delta 1458) remote: Resolving deltas: 100% (1549/1549), done. To https://github.com/castorfou/data-scientist-skills.git * [new branch] master -&gt; master Branch &#39;master&#39; set up to track remote branch &#39;master&#39; from &#39;github&#39;. . for new local repo . Clone the new repo: git clone git@github.com:castorfou/data-scientist-skills.git . And I want to have same names for same remotes: git remote rename origin github . So now it is quite easy to update from different remote repo: . cat refresh_from_github.sh #!/bin/bash git fetch github git pull .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/share-github-repo-on-2-pc.html",
            "relUrl": "/blog/share-github-repo-on-2-pc.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Push large files to github: git-lfs",
            "content": "Where is the problem . I am currently following deep learning specialization from Andrew Ng on coursera. . In the course 4 about CNNs, there are some pre-trained yolo models that we use to do object detection. And these models come as large .h5 files. . Because I run all programming assignments locally and keep everything (lectures + codes) on my local repo, when I pushed to github I got this error: . (base) explore@explore-ThinkPad-P53:~/git/guillaume/deeplearning_specialization$ git push Enumerating objects: 247, done. Counting objects: 100% (247/247), done. Delta compression using up to 12 threads Compressing objects: 100% (239/239), done. Writing objects: 100% (242/242), 707.06 MiB | 4.74 MiB/s, done. Total 242 (delta 6), reused 0 (delta 0) remote: Resolving deltas: 100% (6/6), completed with 3 local objects. remote: warning: File notebooks/C4W3/nb_images/pred_video.mp4 is 85.44 MB; this is larger than GitHub&#39;s recommended maximum file size of 50.00 MB remote: warning: File notebooks/C4W3/nb_images/road_video.mp4 is 81.71 MB; this is larger than GitHub&#39;s recommended maximum file size of 50.00 MB remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com. remote: error: Trace: 2d1944991c30279b831124b51e4aac57a17a860f2ef789b4e32801fb65282244 remote: error: See http://git.io/iEPt8g for more information. remote: error: File notebooks/C4W2/ResNet50.h5 is 270.32 MB; this exceeds GitHub&#39;s file size limit of 100.00 MB remote: error: File notebooks/C4W3/model_data/yolo.h5 is 194.69 MB; this exceeds GitHub&#39;s file size limit of 100.00 MB To github.com:castorfou/deeplearning_specialization.git ! [remote rejected] master -&gt; master (pre-receive hook declined) . Solution: git-lfs . As explained in https://github.com/git-lfs/git-lfs/wiki/Tutorial, there is (always) a way to do it properly. . First it is a matter of installing git-lfs: . sudo apt-get install git-lfs . Then to setup git lfs . git lfs install . And then to &quot;migrate&quot; big files to lfs: . git lfs migrate import --include=&quot;*.mp4&quot; . git lfs migrate import --include=&quot;*.h5&quot; . And now to git push . (base) explore@explore-ThinkPad-P53:~/git/guillaume/deeplearning_specialization$ git push Uploading LFS objects: 100% (25/25), 954 MB | 37 MB/s, done. Enumerating objects: 311, done. Counting objects: 100% (311/311), done. Delta compression using up to 12 threads Compressing objects: 100% (273/273), done. Writing objects: 100% (276/276), 60.49 MiB | 5.59 MiB/s, done. Total 276 (delta 18), reused 0 (delta 0) remote: Resolving deltas: 100% (18/18), completed with 16 local objects. To github.com:castorfou/deeplearning_specialization.git d0d2dc2..004fa09 master -&gt; master .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/push-big-files-to-github.html",
            "relUrl": "/blog/push-big-files-to-github.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Open Jupyter Notebook with http launch instead of redirect file",
            "content": "Where is the problem? . Default configuration when launching jupyter notebook is to create a redirect file. . Here is the explanation from config file ~/.jupyter/jupyter_notebook_config.py. . ## Disable launching browser by redirect file # # For versions of notebook &gt; 5.7.2, a security feature measure was added that # prevented the authentication token used to launch the browser from being # visible. This feature makes it difficult for other users on a multi-user # system from running code in your Jupyter session as you. # # However, some environments (like Windows Subsystem for Linux (WSL) and # Chromebooks), launching a browser using a redirect file can lead the browser # failing to load. This is because of the difference in file structures/paths # between the runtime and the browser. # # Disabling this setting to False will disable this behavior, allowing the # browser to launch by using a URL and visible token (as before). #c.NotebookApp.use_redirect_file = True . And when launching jupyter notebook from WSL . (xgboost) guillaume@LL11LPC0PQARQ:~/git/d059-vld-ic$ jupyter notebook [I 13:09:57.346 NotebookApp] The port 8888 is already in use, trying another port. [I 13:09:57.370 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1 [I 13:09:57.371 NotebookApp] Serving notebooks from local directory: /mnt/d/git/d059-vld-ic [I 13:09:57.372 NotebookApp] The Jupyter Notebook is running at: [I 13:09:57.373 NotebookApp] http://localhost:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 [I 13:09:57.373 NotebookApp] or http://127.0.0.1:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 [I 13:09:57.374 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 13:10:00.384 NotebookApp] To access the notebook, open this file in a browser: file:///home/guillaume/.local/share/jupyter/runtime/nbserver-828-open.html Or copy and paste one of these URLs: http://localhost:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 or http://127.0.0.1:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 . . Solution . As given in https://stackoverflow.com/questions/57679894/how-to-change-jupyter-launch-from-file-to-url, . update jupyter config file to change #c.NotebookApp.use_redirect_file = True to c.NotebookApp.use_redirect_file = False . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/open-jupyter-from-http-link.html",
            "relUrl": "/blog/open-jupyter-from-http-link.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Use git with github (ssh) behind corporate proxy",
            "content": "Configuration . . I use 2 kinds of repo. gitlab for internal/corporate projects, hosted inside my company. github for public/pet projects and as a blogging platform. 3 days a week I am inside company, 4 days a week outside. . Green lines are the natural path to collaborate. . When outside I don&#39;t have proxy configuration or firewall, and I can directly access github. I cannot access to gitlab but I don&#39;t want to address it now, this is why it is set as a black line. (if this is really needed I have a vpn access and this is as being inside) . When inside, I use internal proxy. I can directly access gitlab. But I want to access github in a transparent way. And yes from both Windows and Linux (WSL). This is the red line. . Setup the red line . Here is the https://stackoverflow.com/questions/21318535/how-to-setup-corkscrew-to-connect-to-github-through-draconian-proxy discussion. . 1st step is to install workscrew: sudo apt install corkscrew . Then I create 2 ssh config files: . (base) guillaume@LL11LPC0PQARQ:~/proxy_files$ cat ssh_config_noproxy Host github.com IdentityFile ~/.ssh/id_rsa_gmail Host gitlab.&lt;mycompany&gt;.com IdentityFile ~/.ssh/id_rsa (base) guillaume@LL11LPC0PQARQ:~/proxy_files$ cat ssh_config_proxy Host github.com ProxyCommand /usr/bin/corkscrew &lt;my_proxy_hostname&gt; &lt;my_proxy_port&gt; %h %p IdentityFile ~/.ssh/id_rsa_gmail Host gitlab.&lt;mycompany&gt;.com IdentityFile ~/.ssh/id_rsa . It is just a matter of linking appropriate files when I am in or out of corporate network. . As in my_ip.sh: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf #proxy for conda ln -sf ~/proxy_files/.condarc_proxy ~/.condarc #proxy for git git config --global http.proxy http://proxy_ip:80 ln -sf ~/proxy_files/ssh_config_proxy ~/.ssh/config } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf #no proxy for conda ln -sf ~/proxy_files/.condarc_noproxy ~/.condarc #no proxy for git git config --global --unset http.proxy ln -sf ~/proxy_files/ssh_config_noproxy ~/.ssh/config } .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/github-ssh-behind-proxy.html",
            "relUrl": "/blog/github-ssh-behind-proxy.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "edX MIT 6.00.2x Introduction to Computational Thinking and Data Science",
            "content": "MIT courses at edX . Last summer I have been following a 1st MIT course on python programming. Not that I would need this knowledge but as for Polytechnique courses, I like their way to explain knowledge foundations. Teachers from these schools tend to go back to deep roots, and provide clear and somtimes illuminating examples to help us understand concepts. . About 6 years ago I have completed a Probability introduction from Ecole Polythechnique. That was great. I had always been hermeticly closed to probability and statistics. For a reason I don&#39;t understand, it is not being teached in CPGE (which is a two-or-three-year intensive full-time course preparing top high school graduates for the entrance examination of French engineering and business schools, this is just after high school). It means last time I was exposed to probability was in high school, and probably in engineering school as well but on a light way. . That would be great to give back a look to these courses. . MIT 6.00.1x - Introduction to Computer Science and Programming Using Python . Unfortunately I registered in September when only a couple of weeks were left to complete this 9-week course. And because I didn&#39;t upgrade to a Verified Certificate, I lost access to materials and progress. It cut when my progress was about 38%. . I like Eric Grimson&#39;s style. He is calm and has his own way to explain some advanced subjects. . Next session is planned on Jan 27, 2021. That would be a good idea to complete this course. . MIT 6.00.2x - Introduction to Computational Thinking and Data Science . For this one I have registered on time. And I have purchased the Verified Certificate. . Here is the full course program and dates: . . 1st lectures are interesting. As said before I like to be back to roots of problems. And on that matter I expect to get a full overview. . Lecture 4 should be released in the coming at the end of October. cannot wait to resume these sessions. . As a matter of comparaison with gan specialization from coursera+openAI, I like better the interactions with students offered by openAI. They use slack as a platform to support these interactions and I think it is a smart move. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/MIT-edx-Introduction-Computational-Thinking-and-Data-Science.html",
            "relUrl": "/blog/MIT-edx-Introduction-Computational-Thinking-and-Data-Science.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "GAN Specialization course 2 week 3 - Apply and certificate",
            "content": "Notes . tbd later . Certificate . . About next steps . For the moment I am hesitating to follow last course for gans. This is a deep dive into gan, with lots of theory (papers) associated to it. My 1st goal was to know better about it and this is met. My 2nd one was to figure out how to use this kind of generative networks for other area such as tabular data + prescription issues. I am not sure this is applicable (or not yet). . Maybe it is better for now to resume my learning sessions with Jeremy Howard on fastai v2. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-course2-week3-certificate.html",
            "relUrl": "/blog/gan-course2-week3-certificate.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "GAN Specialization course 2 week 2 - Disadvantages and Bias",
            "content": "Notes . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-course2-week2-disadventages-bias.html",
            "relUrl": "/blog/gan-course2-week2-disadventages-bias.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "GAN Specialization course 2 week 1 - evaluations on GANs",
            "content": "Features extraction, inception v3, embeddings, FID (Fr&#233;chet Inception Distance), Sampling and Truncation, Precision and Recall . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-course2-week1-evaluations-on-gans.html",
            "relUrl": "/blog/gan-course2-week1-evaluations-on-gans.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "GAN Specialization course 1 week 4 - conditional generation, controllable generation",
            "content": "Conditional generation, controllable generation, disentanglement of Z-space . . End of course . Next one is a 3 week course named: Build Better Generative Adversarial Networks (GANs) . Here is my certificate . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-specialization-week4-conditional_generation-controllable-generation.html",
            "relUrl": "/blog/gan-specialization-week4-conditional_generation-controllable-generation.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "GAN Specialization course 1 week 3 - mode collapse, vanishing gradient, wasserstein loss",
            "content": "Mode collapse, Vanishing gradient . Very good explanation about why it happens. Flat region when discriminator is learning faster (it has an easier job) than generator. . . Earth mover&#39;s distance. Wasserstein loss. 1-L continuous condition . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-specialization-week3-mode_collapse-W_loss.html",
            "relUrl": "/blog/gan-specialization-week3-mode_collapse-W_loss.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Variables traces using show_guts decorator",
            "content": "show_guts decorator . Adaptaton from https://stackoverflow.com/questions/24165374/printing-a-functions-local-variable-names-and-values . Update to python 3. . import sys import threading def show_guts(f): sentinel = object() gutsdata = threading.local() gutsdata.captured_locals = None gutsdata.tracing = False def trace_locals(frame, event, arg): if event.startswith(&#39;c_&#39;): # C code traces, no new hook return if event == &#39;call&#39;: # start tracing only the first call if gutsdata.tracing: return None gutsdata.tracing = True return trace_locals if event == &#39;line&#39;: # continue tracing return trace_locals # event is either exception or return, capture locals, end tracing gutsdata.captured_locals = frame.f_locals.copy() return None def wrapper(*args, **kw): # preserve existing tracer, start our trace old_trace = sys.gettrace() sys.settrace(trace_locals) retval = sentinel try: retval = f(*args, **kw) finally: # reinstate existing tracer, report, clean up sys.settrace(old_trace) for key, val in gutsdata.captured_locals.items(): print(&#39;{}: {!r}&#39;.format(key, val)) if retval is not sentinel: print(&#39;Returned: {!r}&#39;.format(retval)) gutsdata.captured_locals = None gutsdata.tracing = False return retval return wrapper . use example . import torch from torch import nn from tqdm.auto import tqdm from torchvision import transforms from torchvision.utils import make_grid from torchvision.datasets import CelebA from torch.utils.data import DataLoader import matplotlib.pyplot as plt . @show_guts def get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight): &#39;&#39;&#39; Function to return the score of the current classifications, penalizing changes to other classes with an L2 norm. Parameters: current_classifications: the classifications associated with the current noise original_classifications: the classifications associated with the original noise target_indices: the index of the target class other_indices: the indices of the other classes penalty_weight: the amount that the penalty should be weighted in the overall score &#39;&#39;&#39; # Steps: 1) Calculate the change between the original and current classifications (as a tensor) # by indexing into the other_indices you&#39;re trying to preserve, like in x[:, features]. # 2) Calculate the norm (magnitude) of changes per example. # 3) Multiply the mean of the example norms by the penalty weight. # This will be your other_class_penalty. # Make sure to negate the value since it&#39;s a penalty! # 4) Take the mean of the current classifications for the target feature over all the examples. # This mean will be your target_score. #### START CODE HERE #### change_original_classification = (current_classifications[:,other_indices] - original_classifications[:,other_indices]) # Calculate the norm (magnitude) of changes per example and multiply by penalty weight other_class_penalty = - torch.mean(torch.norm(change_original_classification, dim=1) * penalty_weight) # Take the mean of the current classifications for the target feature target_score = torch.mean(current_classifications) #### END CODE HERE #### return target_score + other_class_penalty . rows = 10 current_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float() original_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float() # Must be 3 assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3 . current_classifications: tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.]]) original_classifications: tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.]]) target_indices: [1, 3] other_indices: [0, 2] penalty_weight: 0.2 change_original_classification: tensor([[0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.]]) other_class_penalty: tensor(-0.) target_score: tensor(2.5000) Returned: tensor(2.5000) . AssertionError Traceback (most recent call last) &lt;ipython-input-5-c7e77f2e4ae1&gt; in &lt;module&gt; 4 5 # Must be 3 -&gt; 6 assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3 AssertionError: .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/decorator-trace-variables.html",
            "relUrl": "/blog/decorator-trace-variables.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Conda activate from bash scripts",
            "content": "Can&#39;t execute conda activate from bash script . Good description of the problem in conda github. . Calling conda activate from a bash script will raise some errors: . CommandNotFoundError: Your shell has not been properly configured to use &#39;conda activate&#39;. To initialize your shell, run $ conda init &lt;SHELL_NAME&gt; Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See &#39;conda init --help&#39; for more information and options. . source ~/your_conda/etc/profile.d/conda.sh . It is just a matter of sourcing the conda bash settings before calling conda activate. . In m case I have installed conda in ~/miniconda3, I just have to call source ~/miniconda3/etc/profile.d/conda.sh . Example to run my blogging environment . #!/bin/bash source ~/miniconda3/etc/profile.d/conda.sh cd ~/git/guillaume/guillaume_blog/_notebooks conda activate fastai jupyter notebook .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/conda-activate-from-bash-script.html",
            "relUrl": "/blog/conda-activate-from-bash-script.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "GAN Specialization course 1 week 2 - Deep Convolutional GAN",
            "content": "My notes . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-specialization-course1-week2-Deep_convolutional_GAN.html",
            "relUrl": "/blog/gan-specialization-course1-week2-Deep_convolutional_GAN.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "GAN Specialization course 1 week 1 - intro to GAN",
            "content": "My notes . . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-specialization-course1-week1-intro_to_gan.html",
            "relUrl": "/blog/gan-specialization-course1-week1-intro_to_gan.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "Generative Adversarial Networks (GANs) Specialization from Coursera",
            "content": "Coursera . This specialization comes in 3 courses. . . Build Basic Generative Adversarial Networks . Build Better Generative Adversarial Networks . Apply Generative Adversarial Networks . env installation . I am just getting the version from coursera to be sure I have the same behaviour. . import torch . print(torch.__version__) . 1.4.0 . So I can now create a gan environment with appropriate lib versions. . conda create -n gan python=3.7 conda activate gan conda install -c pytorch pytorch=1.4.0 conda install jupyter matplotlib conda install -c conda-forge tqdm conda install -c pytorch torchvision . git settings . echo &quot;# gan_specialization from coursera&quot; &gt;&gt; README.md git init git add README.md git commit -m &quot;first commit&quot; git branch -M master git remote add origin git@github.com:castorfou/gan_specialization.git git push -u origin master . I am pushing notebooks to github . Intro to PyTorch . I have exported Intro to Pytorch notebook from coursera lab. . To run it on my machine. . Intro to GAN using tensorflow . Versions of python seems incompatible between each other. (3.7 for pytorch, 3.6 for tensorflow=1.10) . I create a new python environment: . conda create -n gan_tensorflow python=3.6 conda activate gan_tensorflow conda install jupyter conda install -c conda-forge requests pip install tensorflow-gpu==1.15 .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-pytorch-coursera.html",
            "relUrl": "/blog/gan-pytorch-coursera.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Use fingerprint to authenticate on Ubuntu, and passwordless on some apps",
            "content": "Fingerprint authentication . . Just by activating Fingerprint login, quite surprisingly it has been working directly. . Passwordless commands . Because I have changed my password for a quite complex one, I am interested to launch some sudo commands without prompt of password. . How to run sudo commands without password . Use visudo to update /etc/sudoers. I understand there is some syntax check to avoid mistake when editing this file. You don&#39;t want to be left with a defective sudo system. . I have just added this line. explore is my username. I can add additional commands after a comma (e.g. /bin/systemctl restart httpd.service, /bin/kill) . explore ALL = NOPASSWD: /usr/bin/apt .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/fingerprint-authentication-sudoers.html",
            "relUrl": "/blog/fingerprint-authentication-sudoers.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "Upgrade ubuntu LTS 18.04 to 20.04",
            "content": "Standard upgrade process . As a LTS user, I want to keep using these long term support version. . !cat /etc/issue . Ubuntu 18.04.5 LTS n l . A good way to do it is by using do-release-upgrade tool. Full explanation at: 18.04 to 20.04. . sudo do-release-upgrade Checking for a new Ubuntu release There is no development version of an LTS available. To upgrade to the latest non-LTS develoment release set Prompt=normal in /etc/update-manager/release-upgrades. . Waiting for blockers to be fixed . There is a last blocker before releasing Ubuntu 20.04.1 LTS. . . Expected around 1st of October 2020. . (2020-09-28) blockers are fixed, upgrade in progress . Unfortunately the upgrade process went uneventful. Nothing broke, nothing to learn ;) . It took minutes to do the upgrade. . . Ubuntu releases-code names .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/upgrade-ubuntu-18.04-to-20.04.html",
            "relUrl": "/blog/upgrade-ubuntu-18.04-to-20.04.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post44": {
            "title": "Multiple subplots and animations with matplotlib",
            "content": "Subplots . What I want it to display multiple plots, with a given max rows. And to display my plots depending only on these parameters. . from fastai.tabular.all import * %matplotlib inline # fastai v1 backward compatibility import matplotlib.pyplot as plt import torch import torch.nn as nn import numpy as np . def my_hidden_f(x): return 4*x**3+2*x**2-12*x+5+10*torch.rand(x.shape) n=100 time = torch.ones(n,1) time[:,0].uniform_(-3.14,3.14) speed=my_hidden_f(time) plt.scatter(time[:,0], speed) plt.scatter(tensor(-1.5), my_hidden_f(tensor([-1.5])), color=&#39;red&#39;) def f(t, params): a,b,c,d = params return a*(t**3) + (b*t**2) + c*t + d def mse(preds, targets): return ((preds-targets)**2).mean() def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-50,150) lr = 1e-4 def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . params = torch.randn(4).requires_grad_() #nbr of iterations max_iter = 1000 #nbr of curves visible nbr_graph = 4 #max number of curves on one row max_columns = 5 #nbr of rows max_rows = (nbr_graph-1) // max_columns + 1 #nbr of iter per plot graph_iteration = max_iter //(nbr_graph-1) _,axs = plt.subplots(nrows=max_rows,ncols=max_columns,figsize=(3*max_columns,3*max_rows)) i=-1 ax_index= ((i+1) // graph_iteration ) // (max_columns), ((i+1) // graph_iteration ) % (max_columns) if (max_rows ==1): ax_index= ((i+1) // graph_iteration ) % (max_columns) show_preds(apply_step(params, prn=False), axs[ax_index]) axs[ax_index].set_title(&#39;iter 0&#39;) for i in range(max_iter): preds=apply_step(params, prn=False) if ((i+1) % graph_iteration == 0): ax_index= ((i+1) // graph_iteration ) // (max_columns), ((i+1) // graph_iteration ) % (max_columns) if (max_rows ==1): ax_index= ((i+1) // graph_iteration ) % (max_columns) show_preds(preds, axs[ax_index]) axs[ax_index].set_title(&#39;iter &#39;+str(i+1)) plt.tight_layout() . Animation . import . %matplotlib inline # fastai v1 backward compatibility import matplotlib.pyplot as plt import torch import torch.nn as nn import numpy as np def tensor(*argv): return torch.tensor(argv) # TEST assert torch.all(tensor(1,2) == torch.tensor([1,2])), &#39;Backward compatibility with fastai v1&#39; . function and plot . n=100 x = torch.ones(n,1) x.uniform_(-3.14,3.14) def my_function(x, a): return ((torch.cat((x**3, x**2, x, torch.ones(n,1) ), 1))@a).reshape((n)) a=tensor(4., 2., -12., 5.) y = my_function(x, a) a = tensor(-1.,-2., 6., -8) y_hat = my_function(x, a) plt.scatter(x[:,0], y) plt.scatter(x[:,0],y_hat); def mse(y_hat, y): return ((y_hat-y)**2).mean() . gradient descent . a = nn.Parameter(a); a def update(): y_hat = my_function(x, a) loss = mse(y, y_hat) if t % 10 == 0: print(loss) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() lr = 1e-3 for t in range(100): update() . tensor(1967.0251, grad_fn=&lt;MeanBackward0&gt;) tensor(559.2718, grad_fn=&lt;MeanBackward0&gt;) tensor(365.7207, grad_fn=&lt;MeanBackward0&gt;) tensor(282.6393, grad_fn=&lt;MeanBackward0&gt;) tensor(245.4054, grad_fn=&lt;MeanBackward0&gt;) tensor(227.3450, grad_fn=&lt;MeanBackward0&gt;) tensor(217.3324, grad_fn=&lt;MeanBackward0&gt;) tensor(210.7267, grad_fn=&lt;MeanBackward0&gt;) tensor(205.5912, grad_fn=&lt;MeanBackward0&gt;) tensor(201.1171, grad_fn=&lt;MeanBackward0&gt;) . animation . from matplotlib import animation, rc rc(&#39;animation&#39;, html=&#39;jshtml&#39;) a = nn.Parameter(tensor(-1.,1)) a=tensor(4., 2., -12., 5.) y = my_function(x, a) a = tensor(-1.,-2., 6., -8) y_hat = my_function(x, a) a = nn.Parameter(a); a fig = plt.figure() plt.scatter(x[:,0], y, c=&#39;orange&#39;) line = plt.scatter(x[:,0], y_hat.detach()) plt.close() def animate(i): line.set_offsets(np.c_[x[:,0], (my_function(x,a)).detach()]) update() return line, animation.FuncAnimation(fig, animate, np.arange(0, 300), interval=5) . &lt;/input&gt; Once Loop Reflect",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Matplotlib-multiple-subplots-and-animations.html",
            "relUrl": "/blog/Matplotlib-multiple-subplots-and-animations.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post45": {
            "title": "Fastai book Deep Learning for Coders with fastai and Pytorch",
            "content": "Paper version of fastai book . Of course the 1st tep is to purchase this great book: . . I have liked what Jeremy Howard said about why this is important to purchase it (in video 1). Fastai is offering full access to the book as notebooks. So that we can run all codes from them. . Get notebook version of fastai book . ~/git/guillaume$ git clone https://github.com/fastai/fastbook.git . . I have now a perfect combo between paper book and notebooks. . Video courses based on fastai book . Rachel Thomas and Jeremy Howard have done some great videos about learning fastai (and pytorch). . They tend to do it every year. But this year is quite special due to fastai book. . Here are all the 1st 7 videos: https://course.fast.ai/videos/?lesson=1 . Fastai forums . This is the natural source of information and interactions with other students. . There is a category Part 1 (2020) which seems perfect: https://forums.fast.ai/c/part1-v4/46 . Personal git organization . ~/git/guillaume$ ll fastai/ &lt;-- from https://github.com/fastai/fastai fastai_experiments/ &lt;-- I will keep all experiments I will do here fastbook/ &lt;-- from https://github.com/fastai/fastbook.git guillaume_blog/ &lt;-- from git@github.com:castorfou/guillaume_blog.git . fastai_experiments likely to have 1 notebook per chapter or video. . update jupyter to include extensions (toc, ...) . conda install -c conda-forge jupyter_contrib_nbextensions . I like table of content, others are quite usefull as well (scratchpad, ExecuteTime...). . Install some libraries to run book examples . conda install -c fastai fastbook . It will install graphviz, nbdev, and other libraries. . And most of them can be loaded by calling from utils import * . Launch jupyter notebook and start expermenting . cd ~/git/guillaume conda activate fastai jupyter notebook . And launch several tabs at: blog entries, fastai experiments, fastai courses and fastai videos. . And I keep track of progress with git. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/fastai-book.html",
            "relUrl": "/blog/fastai-book.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post46": {
            "title": "Setup ubuntu box with fastai",
            "content": "Install miniconda . Get miniconda Linux installer. . Check sha256sum: sha256sum Miniconda3-latest-Linux-x86_64.sh . Run install: ./Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3 . Install fastai . conda create -n fastai python=3.8 conda activate fastai conda install -c fastai -c pytorch fastai . Install jupyter within fastai environment . conda activate fastai conda install jupyter . Test fastai installation (valid for v1) . With fastai v1, there was an easy way to check installation: . conda activate fastai python -m fastai.utils.show_install . get git repo to learn from fastai . From git folder, . git clone https://github.com/fastai/fastai . Test fastai v2 installation . From python environment: . from fastai.vision.all import * . From jupyter notebook . from fastai.vision.all import * . Install nvidia drivers for ubuntu . I tried by downloading a driver from nvidia website. But I was unable to install it (nvidia-drm-drv.c:662:44: error: &#39;DRIVER_PRIME&#39; undeclared here (not in a function); did you mean &#39;DRIVER_PCI_DMA&#39;?) . sudo ubuntu-drivers autoinstall . then rebooting fixed the issue. . Run courses from fastai github repo . just run fastai/dev_nbs/course/lesson1-pets.ipynb . And everything is just fined ;) . install nbdev . This is for rendering reasons: To get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev . !pip install nbdev . Collecting nbdev Downloading nbdev-1.0.18-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 1.5 MB/s eta 0:00:01 Requirement already satisfied: fastcore&gt;=1.0.5 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (1.0.13) Requirement already satisfied: packaging in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (20.4) Requirement already satisfied: jupyter-client in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (6.1.6) Requirement already satisfied: nbconvert&lt;6 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.6.1) Requirement already satisfied: nbformat&gt;=4.4.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.0.7) Collecting fastscript&gt;=1.0.0 Downloading fastscript-1.0.0-py3-none-any.whl (11 kB) Requirement already satisfied: pyyaml in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.3.1) Requirement already satisfied: pip in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (20.2.2) Requirement already satisfied: ipykernel in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.3.4) Requirement already satisfied: pyparsing&gt;=2.0.2 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;nbdev) (2.4.7) Requirement already satisfied: six in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;nbdev) (1.15.0) Requirement already satisfied: pyzmq&gt;=13 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (19.0.2) Requirement already satisfied: tornado&gt;=4.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (6.0.4) Requirement already satisfied: python-dateutil&gt;=2.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (2.8.1) Requirement already satisfied: traitlets in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (4.3.3) Requirement already satisfied: jupyter-core&gt;=4.6.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (4.6.3) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (1.4.2) Requirement already satisfied: pygments in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (2.7.1) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.8.4) Requirement already satisfied: defusedxml in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.6.0) Requirement already satisfied: bleach in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (3.2.1) Requirement already satisfied: jinja2&gt;=2.4 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (2.11.2) Requirement already satisfied: testpath in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.4.4) Requirement already satisfied: entrypoints&gt;=0.2.2 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.3) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbformat&gt;=4.4.0-&gt;nbdev) (3.0.2) Requirement already satisfied: ipython-genutils in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbformat&gt;=4.4.0-&gt;nbdev) (0.2.0) Requirement already satisfied: ipython&gt;=5.0.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipykernel-&gt;nbdev) (7.18.1) Requirement already satisfied: decorator in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from traitlets-&gt;jupyter-client-&gt;nbdev) (4.4.2) Requirement already satisfied: webencodings in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from bleach-&gt;nbconvert&lt;6-&gt;nbdev) (0.5.1) Requirement already satisfied: MarkupSafe&gt;=0.23 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jinja2&gt;=2.4-&gt;nbconvert&lt;6-&gt;nbdev) (1.1.1) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (0.17.3) Requirement already satisfied: attrs&gt;=17.4.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (20.2.0) Requirement already satisfied: setuptools in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (49.6.0.post20200814) Requirement already satisfied: backcall in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.2.0) Requirement already satisfied: pexpect&gt;4.3; sys_platform != &#34;win32&#34; in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (4.8.0) Requirement already satisfied: pickleshare in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.7.5) Requirement already satisfied: jedi&gt;=0.10 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.17.2) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (3.0.7) Requirement already satisfied: ptyprocess&gt;=0.5 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from pexpect&gt;4.3; sys_platform != &#34;win32&#34;-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.6.0) Requirement already satisfied: parso&lt;0.8.0,&gt;=0.7.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jedi&gt;=0.10-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.7.0) Requirement already satisfied: wcwidth in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.2.5) Installing collected packages: fastscript, nbdev Successfully installed fastscript-1.0.0 nbdev-1.0.18 .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Setup-ubuntu-box-with-fastai.html",
            "relUrl": "/blog/Setup-ubuntu-box-with-fastai.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post47": {
            "title": "Fastai on WSL 2 with Cuda",
            "content": "This is based on what is explained in https://forums.fast.ai/t/fastai-on-wsl-2-ubuntu-0-7-0-or-any-version/76651 . install update of nvidia drivers . Based on Deep Learning Course Forums Platform: Windows 10 using WSL2 w/GPU fastai users . create nvidia account | download quadro driver from https://developer.nvidia.com/cuda/wsl/download (460.15) | install | . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ nvidia-smi.exe Mon Sep 21 16:00:46 2020 +--+ | NVIDIA-SMI 460.15 Driver Version: 460.15 CUDA Version: 11.1 | |-+-+-+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Quadro M1000M WDDM | 00000000:01:00.0 On | N/A | | N/A 59C P0 N/A / N/A | 905MiB / 4096MiB | 0% Default | +-+-+-+ . install of WSL2 and convert existing images . Open a PowerShell window as an Administrator . Run wsl --set-default-version 2 . update KB . . --set-default-version 2 is not a valid option. KB4566116 should be installed . This can be downloaded from Catalog Microsoft Update . update kernel version . If you see this message after running the command: WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel. You still need to install the MSI Linux kernel update package. . Download from https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4download-the-linux-kernel-update-package . set default WSL to be version 2 . PS C: WINDOWS system32&gt; wsl --set-default-version 2 . convert existing images . PS C: WINDOWS system32&gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Running 1 PS C: WINDOWS system32&gt; wsl --set-version Ubuntu-18.04 2 La conversion est en cours. Cette opération peut prendre quelques minutes... Pour plus d’informations sur les différences de clés avec WSL 2, visitez https://aka.ms/wsl2 La conversion est terminée. . It took a while (~1 hour) for my unique ubuntu image. . And at the end it has worked. . PS C: WINDOWS system32&gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Stopped 2 . install of nvidia drivers under ubuntu . [Installation instructions)(https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal) . wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda . !cat /usr/local/cuda/version.txt . CUDA Version 11.0.228 . !/usr/local/cuda/samples/4_Finance/BlackScholes/BlackScholes . [/usr/local/cuda/samples/4_Finance/BlackScholes/BlackScholes] - Starting... CUDA error at ../../common/inc/helper_cuda.h:777 code=35(cudaErrorInsufficientDriver) &#34;cudaGetDeviceCount(&amp;device_count)&#34; . There is an error when launching CUDA samples. Googling that error maybe my video card is running on low driver version? . I have posted on nvidia (cuda+wsl) forum: https://forums.developer.nvidia.com/t/cuda-sample-throwing-error/142537/18 . (update 09-22: it is not possible to have cuda on wsl2 if not in Windows Insider build from Dev Channel. (20145 or higher)) . . because I am in version 1909 (18363.1049), it won&#39;t work for me. ;( . cuda for WSL . Here is a link that could be interesting: https://docs.nvidia.com/cuda/wsl-user-guide/index.html . According to this, I should not have installed cuda but cuda-toolkit. Do not choose the cuda, cuda-11-0, or cuda-drivers meta-packages under WSL 2 since these packages will result in an attempt to install the Linux NVIDIA driver under WSL 2. . Is it causing my issue? . apt-get install -y cuda-toolkit-11-0 . !/usr/local/cuda/bin/nvcc --version . nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Wed_Jul_22_19:09:09_PDT_2020 Cuda compilation tools, release 11.0, V11.0.221 Build cuda_11.0_bu.TC445_37.28845127_0 . install a new distro (ubuntu 20.04) . Because I cannot use windows store, I have to manually install https://docs.microsoft.com/fr-fr/windows/wsl/install-manual . Installation by just launching Ubuntu_2004.2020.424.0_x64.appx. . I have now 2 distros, . PS C: WINDOWS system32&gt; wsl --list -v NAME STATE VERSION * Ubuntu-18.04 Running 2 Ubuntu-20.04 Running 2 . WSL2 and network . There is a change of network architecture between WSL 1 and WSL 2. In WSL 2, a new network interface is available: . Carte Ethernet vEthernet (WSL) : Suffixe DNS propre à la connexion. . . : Adresse IPv6 de liaison locale. . . . .: Adresse IPv4. . . . . . . . . . . . . .: 192.168.81.193 Masque de sous-réseau. . . . . . . . . : 255.255.255.240 Passerelle par défaut. . . . . . . . . : . Revert image to WSL1 to get back network access . PS C: WINDOWS system32&gt; wsl --set-version Ubuntu-18.04 1 La conversion est en cours. Cette opération peut prendre quelques minutes... La conversion est terminée. . PS C: WINDOWS system32&gt; wsl --list -v NAME STATE VERSION * Ubuntu-18.04 Stopped 1 Ubuntu-20.04 Stopped 2 . access to linux files from windows . For running state distros: . Files are available at wsl$ . . For stopped state distros: . Files are available at C: Users &lt;users&gt; AppData Local Packages CanonicalGroupLimited.Ubuntu* LocalState rootfs . some usefull wsl commands . List distros . !wsl --list --verbose . NAME STATE VERSION * Ubuntu-18.04 Running 1 Ubuntu-20.04 Stopped 2 . Stop a distro . !wsl --terminate Ubuntu-18.04 . Update dns settings . as explained here . just switch from generateResolvConf = true to generateResolvConf = false in /etc/wsl.conf and edit /etc/resolv.conf . But still have issues, mainly I think linked to Symantec Endpoint protection. . Workaround network issue with WSL2 . https://github.com/sakai135/wsl-vpnkit . installation setup . vpnkit . install docker for windows . install genisoimage in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/c/cdrkit/genisoimage_1.1.11-3.1ubuntu1_amd64.deb) . install vpnkit . isoinfo -i /mnt/c/Program Files/Docker/Docker/resources/wsl/docker-for-wsl.iso -R -x /containers/services/vpnkit-tap-vsockd/lower/sbin/vpnkit-tap-vsockd &gt; ./vpnkit-tap-vsockd chmod +x vpnkit-tap-vsockd sudo mv vpnkit-tap-vsockd /sbin/vpnkit-tap-vsockd sudo chown root:root /sbin/vpnkit-tap-vsockd . npiperelay . install unzip in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/u/unzip/unzip_6.0-25ubuntu1_amd64.deb) . download npiprelay (from https://github.com/jstarks/npiperelay/releases/download/v0.1.0/npiperelay_windows_amd64.zip ) . install npiprelay . unzip npiperelay_windows_amd64.zip npiperelay.exe rm npiperelay_windows_amd64.zip mkdir -p /mnt/c/bin mv npiperelay.exe /mnt/c/bin/ sudo ln -s /mnt/c/bin/npiperelay.exe /usr/local/bin/npiperelay.exe . socat . install socat in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/s/socat/socat_1.7.3.3-2_amd64.deb) . Configure DNS for WSL . Disable WSL from generating and overwriting /etc/resolv.conf. . sudo tee /etc/wsl.conf &lt;&lt;EOL [network] generateResolvConf = false EOL . Manually set DNS servers to use when not running this script. 1.1.1.1 is provided as an example. . sudo tee /etc/resolv.conf &lt;&lt;EOL nameserver 1.1.1.1 EOL . wsl-vpnkit . from https://github.com/sakai135/wsl-vpnkit/archive/refs/heads/main.zip . unzip ~/git/wsl-vpnkit-main.zip -d ~/Applications/wsl-vpnkit . execution . ~/Applications/wsl-vpnkit/wsl-vpnkit-main$ sudo ./wsl-vpnkit . proxychains . I can now use proxychains and everything works beautifully ;) . clean from local deb install to ubuntu repo . ~/git$ ls *.deb genisoimage_1.1.11-3.1ubuntu1_amd64.deb proxychains_3.1-8.1_all.deb libproxychains3_3.1-8.1_amd64.deb socat_1.7.3.3-2_amd64.deb net-tools_1.60+git20180626.aebd88e-1ubuntu1_amd64.deb unzip_6.0-25ubuntu1_amd64.deb . sudo apt remove genisoimage net-tools socat unzip sudo proxychains apt install genisoimage libproxychains3 net-tools proxychains socat unzip . I don&#39;t exactly see how to do it with proxychains. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Windows10-fastai-wsl2-cuda.html",
            "relUrl": "/blog/Windows10-fastai-wsl2-cuda.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post48": {
            "title": "Autodetect Home / Office network + Proxy",
            "content": "IP detection . Command to get IP address is as follow: . IP=`ifconfig | grep &#39;inet &#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;{ print $2}&#39;` . I can then check how IP is setup: . empty: no network attached, in that case nothing to do | HOME_IP=192.168.1.241: based on MAC I give fixed IP to my computers (out of DHCP scope) | S8_IP=192.168.: hotspot from samsung is using 192.168. addresses | OFFICE_IP=10.: office network uses 10. addresses | . Detect if variable IP is set: . if [ -z &quot;$IP&quot; ]; then echo &quot;Not connected to any network&quot; fi . Network detection and proxy settings . Depending on my network, I have to set or unset proxy. . Here is the 1st version: . (xgboost) guillaume@LL11LPC0PQARQ:~$ cat my_ip.sh #!/bin/bash IP=`ifconfig | grep &#39;inet &#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;{ print $2}&#39;` HOME_IP=192.168.1.241 OFFICE_IP=10. S8_IP=192.168. # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY } if [ -z &quot;$IP&quot; ]; then echo &quot;Not connected to any network&quot; else echo &quot;Connected and IP address is: $IP&quot; if [[ &quot;$IP&quot; == &quot;$HOME_IP&quot; ]]; then echo &quot;Connected at home from freebox pop --&gt; no proxy&quot; unsetproxy else if [[ &quot;$IP&quot; == &quot;$S8_IP&quot;* ]]; then echo &quot;Connected with mobile phone --&gt; no proxy&quot; unsetproxy fi if [[ &quot;$IP&quot; == &quot;$OFFICE_IP&quot;* ]]; then echo &quot;Connected from Office --&gt; proxy&quot; setproxy fi fi fi . Call this script: source . If I want these environment variables to be available from parent shell, I have to call my script with source. . (xgboost) guillaume@LL11LPC0PQARQ:~$ source my_ip.sh Connected and IP address is: 10.xxx.xxx.xxx 192.168.1.241 Connected from Office --&gt; proxy Calling setproxy . And I will auto launch this script each time I open a terminal by adding source my_ip.sh at the end of .bashrc . git and keep dot configuration files: config . Another great practice from Jeremy Howard: From https://developer.atlassian.com/blog/2016/02/best-way-to-store-dotfiles-git-bare-repo/ and https://www.atlassian.com/git/tutorials/dotfiles . I will create a blog entry about that later. . config add .bashrc my_ip.sh config commit -m &#39;detect network and set proxy&#39; config push . wget: proxy / no proxy . I store proxy conf files under ~/proxy_files/ . For wget: 2 files . $ cat proxy_files/.wgetrc_noproxy use_proxy=no $ cat proxy_files/.wgetrc_proxy use_proxy=yes http_proxy=proxy_ip:80 https_proxy=proxy_ip:80 . And enabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc . Disabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc . So the updated functions setproxy and unsetproxy are: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc } . apt-get: proxy / no proxy . I store proxy conf files under ~/proxy_files/ . For apt, 1 file . $ cat proxy_files/apt_proxy.conf Acquire { HTTP::proxy &quot;http://proxy_ip:80&quot;; HTTPS::proxy &quot;http://proxy_ip:80&quot;; } . And enabling proxy for apt: sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf . Disabling proxy for wget: sudo rm -f /etc/apt/apt.conf.d/proxy.conf . . Refactor to avoid password request each time it is launched . for the moment I have just commented out these lines . So the updated functions setproxy and unsetproxy are: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf } . Sept-21 2020: IP detection to be changed after WSL2 . With WSL2, IP address is from 172 network. . This looks like a virtual internal address. More detail at that address: https://github.com/microsoft/WSL/issues/4150. . . to update IP detection . Oct-21 2020: Use git with github (ssh) behind corporate proxy . Here is the new configuration explained in my blog entry Use git with github (ssh) behind corporate proxy . It is just a matter of linking appropriate files when I am in or out of corporate network. . As in my_ip.sh: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf #proxy for conda ln -sf ~/proxy_files/.condarc_proxy ~/.condarc #proxy for git git config --global http.proxy http://proxy_ip:80 ln -sf ~/proxy_files/ssh_config_proxy ~/.ssh/config } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf #no proxy for conda ln -sf ~/proxy_files/.condarc_noproxy ~/.condarc #no proxy for git git config --global --unset http.proxy ln -sf ~/proxy_files/ssh_config_noproxy ~/.ssh/config } .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/autodetect-home-office-network-and-proxy-settings.html",
            "relUrl": "/blog/autodetect-home-office-network-and-proxy-settings.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post49": {
            "title": "Fast read Excel files with pandas",
            "content": "Problem description . initial settings . measure_time decorator . from functools import wraps from time import time def measure_time(func): @wraps(func) def _time_it(*args, **kwargs): start = int(round(time() * 1000)) try: return func(*args, **kwargs) finally: end_ = int(round(time() * 1000)) - start print(f&quot;Total execution time: {end_ if end_ &gt; 0 else 0} ms&quot;) return _time_it . read big excel file with pandas . big_excel_file = root_data+&#39;/pandas-caching/big_excel_file.xlsx&#39; . @measure_time def load_excel(file): dataframe = pd.read_excel(file) return dataframe . dataframe = load_excel(big_excel_file) . Total execution time: 36196 ms . . Waouh, 36 sec to read this file! . read converted csv file (turned to csv from excel using excel) . csv_file = root_data+&#39;/pandas-caching/big_csv_file_turned_from_excel.csv&#39; . @measure_time def load_csv(file): dataframe = pd.read_csv(file, sep=&#39;;&#39;, decimal=&#39;,&#39;) return dataframe . df_csv = load_csv(csv_file) . Total execution time: 836 ms . . Much better, 0.8 sec! . Caching library . import os def read_CachedXLS(filename, forceReload = False, **options): &quot;&quot;&quot; Part d&#39;un fichier excel natif (filename). Si le dataframe caché correspondant n&#39;existe pas encore, alors sauve le dataframe caché au format csv dans le rep source. (s&#39;il existe et si forceReload==True, alors écrase le dataframe caché existant par une nouvelle version) Lit le dataframe caché correspondant avec les **options et retourne le dataframe. Examples -- &gt;&gt;&gt; filename = &#39;/mnt/z/data/Stam-CC/ExportData 25625.xlsx&#39; forceReload = False option={&#39;dayfirst&#39;:True, &#39;parse_dates&#39;:[&#39;Fecha de Medida&#39;, &#39;Fecha de Fabricacion&#39;], &#39;sheetname&#39;:0} getCachedXLSRaw(filename, forceRelead, **option).info() Parameters - filename : string Emplacement du fichier XLS. Avec l&#39;extension. Format complet Ex: &#39;/mnt/z/data/Stam-CC/ExportData 25625.xlsx&#39; forceReload : boolean, optional, default value = False Si forceReload == True, le fichier sera relu et sauvé même s&#39;il existe déjà en cache options : **keyword args, optional Arguments de lecture du fichier XLS : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html Ex: sheetname=1 Returns - dataframe Dataframe correspondant &quot;&quot;&quot; #split pour ne garder que le nom sans le chemin de filename : Stam-CC/ExportData 25625 --&gt; ExportData 25625 dataframe_filename = os.path.dirname(filename)+&#39;/&#39;+os.path.basename(filename)+&#39;.csv&#39; #bug de pandas.to_csv quand il y a des espaces ? dataframe_filename = dataframe_filename.replace(&quot; &quot;, &quot;_&quot;) dataframe=[] xls_toget = False #print(dataframe_filename) if (forceReload and os.path.exists(dataframe_filename)): print(&quot;Cached file &quot;+dataframe_filename+&quot; déjà existant mais forceReload=True - FORCE RELOAD&quot;) xls_toget = True if (not os.path.exists(dataframe_filename)): print(&quot;Cached file &quot;+dataframe_filename+&quot; inexistant - read_CachedXLS&quot;) xls_toget = True if (xls_toget): dataframe = pd.read_excel(filename, **options) dataframe.to_csv(dataframe_filename) else: print(&quot;Cached file &quot;+dataframe_filename+&quot; existe en cache, relecture&quot;) #index_col pour ignorer les n° de lignes excel options[&#39;sep&#39;]=&#39;,&#39; options[&#39;decimal&#39;]=&#39;.&#39; options[&#39;skiprows&#39;]=0 options.pop(&#39;sheet_name&#39;) dataframe = pd.read_csv(dataframe_filename,**options) return dataframe . option={&#39;sheet_name&#39;:0} read_CachedXLS(big_excel_file, **option) print(&quot;et voila&quot;) . Cached file /mnt/z/data//pandas-caching/big_excel_file.xlsx.csv existe en cache, relecture et voila .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/fast-read-excel-pandas.html",
            "relUrl": "/blog/fast-read-excel-pandas.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post50": {
            "title": "Git push to github without password",
            "content": "By default everytime I push to github, I have a prompt asking for password. . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Everything up-to-date . Would be great if I could leverage ssh keys to authenticate. . Update remote from https to ssh . From https://stackoverflow.com/questions/14762034/push-to-github-without-a-password-using-ssh-key, . For example, a GitHub project like Git will have an HTTPS URL: https://github.com/&lt;Username&gt;/&lt;Project&gt;.git And the SSH one: git@github.com:&lt;Username&gt;/&lt;Project&gt;.git You can do: git remote set-url origin git@github.com:&lt;Username&gt;/&lt;Project&gt;.git to change the URL. . In my case I have . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git remote -v origin https://github.com/castorfou/guillaume_blog.git (fetch) origin https://github.com/castorfou/guillaume_blog.git (push) . I have just to modify: . git remote set-url origin git@github.com:castorfou/guillaume_blog.git . Results . It looks like working: . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push Counting objects: 4, done. Delta compression using up to 8 threads. Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 1.59 KiB | 812.00 KiB/s, done. Total 4 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:castorfou/guillaume_blog.git 4013108..ae78b99 master -&gt; master . Drawback: doesn&#39;t work behing a firewall . . To find a solution to use a proxy . Here are 2 ways to be tested: https://stackoverflow.com/questions/1728934/accessing-a-git-repository-via-ssh-behind-a-firewall https://stackoverflow.com/questions/18604719/how-to-configure-git-to-clone-repo-from-github-behind-a-proxy-server?noredirect=1&amp;lq=1 .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/git-commit-without-password.html",
            "relUrl": "/blog/git-commit-without-password.html",
            "date": " • Sep 11, 2020"
        }
        
    
  
    
        ,"post51": {
            "title": "Blog from jupyter notebook",
            "content": "That will be great if I can simply write blog entries using Jupyter Notebook. . I usually paste inner images into jupyter cells. But this feature is not available yet into fastpages. So for the moment I won&#39;t include images into these posts. . That way I could simply use markdown and insert images . And directly see rendered impact before commiting and pushing to my blog. . get local repo from github . As I am behind a proxy most of my time when working from office, the easiest way for me is to work from WSL. . WSL . I won&#39;t detail how to install WSL on Windows. . I use ubuntu images (18.04) on my PC. . set unset proxy in WSL . I have just added some bash commands at the end of my .bashrc file. . # Set Proxy function setproxy() { export {http,https,ftp}_proxy=&quot;http://&lt;my proxy ip address&gt;:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://&lt;my proxy ip address&gt;:80&quot; } # Unset Proxy function unsetproxy() { unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY } . git clone castorfou.github.io . I keep most of my local repos under ~/git/ . cd ~/git setproxy git clone https://github.com/castorfou/castorfou.github.io.git . create a blog entry with Jupyter Notebook . commit and push to github . (base) guillaume@LL11LPC0PQARQ:~$ cd git (base) guillaume@LL11LPC0PQARQ:~/git$ cd castorfou.github.io/ (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git add . (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git commit -m &#39;new blog entry: blog from jupyter&#39; [master 6b7460a] new blog entry: blog from jupyter 4 files changed, 516 insertions(+) create mode 100644 _posts/.ipynb_checkpoints/2020-09-10-blog-from-jupyter-checkpoint.ipynb create mode 100644 _posts/2020-09-10-blog-from-jupyter.ipynb create mode 100644 _posts/2020-09-10-blog-from-jupyter.py create mode 100644 _posts/Untitled.txt (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push fatal: unable to access &#39;https://github.com/castorfou/castorfou.github.io.git/&#39;: gnutls_handshake() failed: The TLS connection was non-properly terminated. . error: gnutls_handshake() failed: The TLS connection was non-properly terminated. . Just googling this error gives some insight: https://github.community/t/unable-to-push-to-repo-gnutls-handshake-failed/885 . It is likely some local firewell issue. . . To be fixed later . switch to mobile wifi without need of proxy . (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ unsetproxy (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Counting objects: 7, done. Delta compression using up to 8 threads. Compressing objects: 100% (6/6), done. Writing objects: 100% (7/7), 141.26 KiB | 10.09 MiB/s, done. Total 7 (delta 1), reused 0 (delta 0) remote: Resolving deltas: 100% (1/1), completed with 1 local object. remote: remote: GitHub found 3 vulnerabilities on castorfou/castorfou.github.io&#39;s default branch (2 high, 1 moderate). To find out more, visit: remote: https://github.com/castorfou/castorfou.github.io/network/alerts remote: To https://github.com/castorfou/castorfou.github.io.git 6adeb02..6b7460a master -&gt; master . check entries into blog . double entries . Double entries: one for the notebook (.ipynb) and one for the auto python export (.py). I will have to update my jupyter settings to avoid this python file creation. In the meantime I can just delete the python file, and commit. . . Change settings of jupyter + .gitignore to avoid these double entries . cannot open notebook into browser . Clicking just ask me to download the notebook, it doesn&#39;t display it into the browser. . checking .gitignore . Just by looking into .gitignore, there is an interesting entry: . *.swp ~* *~ _site .sass-cache .jekyll-cache .jekyll-metadata vendor _notebooks/.ipynb_checkpoints . Wait what is in this last line. . Let&#39;s create _notebooks directory and move my notebook in that directory. . notebooks from _notebooks not rendered . No entries, I guess there is some additional settings to do... . . Why notebooks are not rendered by Jekyl . test entry from md using local repo . There is no problem with that. . Creating a local md file in _poststhen pushing to github is creating the right entry blog. . following fastpages troubleshooting guide . upgrade fastpages . Try the automated upgrade as described in https://github.com/fastai/fastpages/blob/master/_fastpages_docs/UPGRADE.md . Unfortunately I don&#39;t see . I have to follow the manual upgrade. . manual fastpages upgrade . I am surprised because the 1st step from manual upgrade is to copy the fastpages repo. It is what I did 2 days ago. I doubt having an outdated version of fastpages. . fastai forum: fastpages category . I will browse through nbdev &amp; faspages category in fastai forums. I should see people with the same issue. . I have created an entry, into fastai forums: Fastpages - cannot see build process of GitHub Actions . And quite immediately Hamel Hussain answered guiding to the write direction: . I misread the Settings instruction: my github repo should explicitely NOT include my github username and I did exactly the opposite. . . I have to create a new repo: guillaume_blog . nothing visible from Actions tab . And another surprising subject: at github in Actions tab. I have a kind of default page. I expect something like an execution journal of Actions. . Page build failure . Received a notification by email: . The page build failed for the master branch with the following error: . Page build failed. For more information, see https://docs.github.com/github/working-with-github-pages/troubleshooting-jekyll-build-errors-for-github-pages-sites#troubleshooting-build-errors. . For information on troubleshooting Jekyll see: . https://docs.github.com/articles/troubleshooting-jekyll-builds . If you have any questions you can submit a request on the Contact GitHub page at https://support.github.com/contact?repo_id=293820308&amp;page_build_id=202240535 . Move to another repo . repo creation . It was just a matter of creating a new repo: . actions monitoring . Monitoring is effective . merge pull request . actions around ssh keys . Following the steps: . Create keys using ssh utility | Enter Secret Key | Enter Deploy Key | . merge PR . There are conflicts to be fixed before that. . And it works: https://castorfou.github.io/guillaume_blog/ . Get local repo . cd ~/git unsetproxy git clone https://github.com/castorfou/guillaume_blog.git .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/blog-from-jupyter-sans-images.html",
            "relUrl": "/blog/blog-from-jupyter-sans-images.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post52": {
            "title": "Blogging from github",
            "content": "Blogging from github . fastai and fastpages . I am a big fan of fastai’s spirit and even more of their leaders: . Jeremy Howards | Rachel Thomas | Sylvain Gugger who is know at huggingface. | . They are commited to beautiful ideas, and are inspiring people. I like their courses. I like their softwares. For sure I will discuss about fastai. They have created fastpages. It turns github into a blogging platform. I don’t have the full detail but it is explained in fastpages github repo It is based on github actions, and by just creating a repo from a fastpages template https://github.com/fastai/fast_template/generate and giving a couple of settings, you are ready to go. . And here I have to thank Hamel Husain. He is from github company and I think he is behing github actions and helped fastai to release fastpages. I don’t know Hamel but he looks like a humble, terribly skilled guy, with tons of energy. Thanks Hamel. . my blog . My main audience is the future me. (maybe not entirely true otherwise I would have written in French) In 1 year, I want to turn back to this blog and I would like to see all the learning peaces I went through. I want this platform to be as easy as possible. . fastpages . For the moment it cannot be easier. I have setup the about page. And each blog entry is just a new markdown page into _posts. github _posts: . . By commiting this page, there are internal actions being run automatically (through github actions magic) and after a couple of minutes the new blog pages are generated (using Jekyl and ruby if I am not wrong). For the moment I use github web interface. But I guess it is easier to have a local repo of my blog, create new entries and when satisfied git push to github. (to be tested later) . github accounts . For a reason I used my personal github account (guillaume.ramelet@gmail.com) and not my professional one (guillaume.ramelet@michelin.com). I will see later if I have to move to another account. I had some troubles to setup actions into github. For a reason I thought it was available only for organization account. So I have turned my michelin github account to an organization, and I cannot login anymore. To be fixed later. . markdown . Ok I am not a huge fan of markdown. I use it as a basic text system specially within notebooks. But it is not as easy to insert images. Currently I screenshot what I want to share, insert into images folder of my repo and reference this image from my blog post using markdown language. I definitely have to improve my practice of markdown, and there are multiple cheatsheets to be used. . jupyter . There are options within fastpages to blog from jupyter notebooks. I have to do it. My intent will be to use this place to share my knowledge. Today most of my knowledge comes from experiences I make within jupyter. If I could directly blog from that it will be great. . comments . OK as the sole reader this is maybe a minor concern but there is no commenting system associated with fastpages. I cannot get any feedback from these entries. Would love to get advices, create discussions within that blog. Not for today. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/blogging-from-github.html",
            "relUrl": "/blog/blogging-from-github.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post53": {
            "title": "Becoming a datascientist",
            "content": "March 2019 . This journey has started about 1 year ago. . No wait, that was dormant for a long time before that. I guess I have to go back to my studying time: at that time my days were full of maths and computers. And my days were flying as crazy. It happens to me (to you?) when you’re just in a middle of something you like very much. 10 hours looks like 1. And the opposite is true as well. . 2000 - 2004: software development . Most of my days and weekends at that time were dedicated to code in Java and bash. Java mainly for server-side developpement in J2EE at Unilog Management. Bash from time to time to automate some tasks on my personal PC. At that time it was mainly about learning what is an operating system. I had started with LFS (Linux From Scratch). And in 2002 with Gentoo which was a much more powerful way to mimic LFS. . 2004 - 2009: project management . Strange period. I don’t remember exactly why but I had a shift in my professional orientation. I moved away from software development and turned into a project manager. In 2005 I entered into Michelin company. And sofwtare technical matters at that time were considered as unimportant (and embarrassing subjects) Fortunately in 2009 I have been started my agile journey. A lot to learn, and it was less about software than human relations and empathy. It was like a start from scratch. . 2009 - 2015: agile journey . Quite a new world for me. I had some basic knowledge by following Jono Bacon. At that time he was a community manager|release leader at Ubuntu. And was reporting progress using burndown charts. In 2009 I launched a project to create an employee portal (closed to what netvibes and igoogle were at that time). Using standard java portal technologies and more importantly using agile approach. A lot to learn about Agile, Scrum, and endless discussions about how to introduce Agile into a non-Agile organization. In 2010 I started another more ambitious project, with many colleagues (~30 persons) and a vague vision. It was about to create a product lifecycle management solution for semi-finished products. . 2015 - 2017: lean journey . In 2015, I met lean approaches for office. I was immediately convinced there was powerful and deep roots within lean. And it could bring a lot to people and organizations. I turned into a lean coach, to work with teams identifying what they could improve, how they could work better, with more pleasure. . 2017 - 2019: Welcome to USA! . Nice opportunity at that time to move from Clermont-Ferrand (France) to Greenville, South Carolina (USA). I have loved every part of it. Except maybe that 2 years were too short to make a full tour of this amazing country. It is crazy to think how different we are when we look like the same. . 2019 - : back to France and turning as a datascientist . Sept 2019 - back to France and for 4 months to prepare for a complete new position: datascientist for Manufacturing within Michelin. I spent many days to learn from various sources specially datacamp and Andrew Ng. That was just the beginning. My intent was to move away from project management, team leadership and focus about what I can do by myself. I wanted to return to math domains without giving up an IT landscape. My colleague Francois Deheeger told me about data science and Artificial Intelligence. That looked as interesting as terrifying. I was in. I was not afraid to learn a new language, and to restart my career from scratch. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/becoming-datascientist.html",
            "relUrl": "/blog/becoming-datascientist.html",
            "date": " • Sep 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About . This blog . This is more a journal where I am adding entries about my (baby steps) learnings. It is likely to be centered around python, git, data-science, … I have been strongly inspired by Rachel Thomas explaining why I should blog. Specially when starting such a journey to turn a datascientist. . My intent would be to regularly add entries to this blog. Ideally at least once a week. Maybe only short ones, the point being to stick on this frequent activity. If it takes days to write posts I am pretty sure I won’t do it. Those entries are personnal thoughts and not those of my employer Michelin. . Me . I am Guillaume Ramelet. I am 44 (in 2020). Father of 3. And have been working for a French tire company for 15+ years. I am French and as you can read English is not my mother tongue (but using English is a good exercise, isn’t it) .",
          "url": "https://castorfou.github.io/guillaume_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://castorfou.github.io/guillaume_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}