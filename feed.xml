<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://castorfou.github.io/guillaume_blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://castorfou.github.io/guillaume_blog/" rel="alternate" type="text/html" /><updated>2021-03-25T08:39:47-05:00</updated><id>https://castorfou.github.io/guillaume_blog/feed.xml</id><title type="html">Guillaume’s blog</title><subtitle>Journey for a datascientist</subtitle><entry><title type="html">Headless raspberry pi: create a wifi to ethernet bridge</title><link href="https://castorfou.github.io/guillaume_blog/blog/headless-raspberry-pi-bridge-network.html" rel="alternate" type="text/html" title="Headless raspberry pi: create a wifi to ethernet bridge" /><published>2021-03-25T00:00:00-05:00</published><updated>2021-03-25T00:00:00-05:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/headless-raspberry-pi-bridge-network</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/headless-raspberry-pi-bridge-network.html">&lt;p&gt;After my internet provider router stopped unexpectedly yesterday, I had to find a solution with internet access from phones and raspberry pi to broadcast internet to full home devices.&lt;/p&gt;

&lt;h2 id=&quot;headless-raspberry-pi&quot;&gt;Headless raspberry pi&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Installation on SD from ubuntu&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;for a reason, &lt;em&gt;raspberry pi imager&lt;/em&gt; snap doesn’t work (due to a bug linked to QT+wayland).&lt;/p&gt;

&lt;p&gt;I download deb ubuntu version (imager_1.6_amd64.deb) from &lt;a href=&quot;https://www.raspberry.org/software&quot;&gt;https://www.raspberry.org/software&lt;/a&gt; and install with dpkg. (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo dpkg -i imager_1.6_amd64.deb&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rpi-imager&lt;/code&gt;, I can install by selecting the default OS (raspberry Pi OS 32-bit), and SD card as storage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Headless wifi&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As explained in &lt;a href=&quot;https://www.raspberrypi.org/documentation/configuration/wireless/headless.md&quot;&gt;https://www.raspberrypi.org/documentation/configuration/wireless/headless.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Create (touch) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wpa_supplicant.conf&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/boot&lt;/code&gt; of SD card and paste this content:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1
country=FR

network={
 ssid=&quot;AndroidAP&quot;
 psk=&quot;&amp;lt;Password for your wireless LAN&amp;gt;&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Headless ssh&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As explained in &lt;a href=&quot;https://www.raspberrypi.org/documentation/remote-access/ssh/README.md&quot;&gt;https://www.raspberrypi.org/documentation/remote-access/ssh/README.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Create (touch) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/boot&lt;/code&gt; of SD card&lt;/p&gt;

&lt;p&gt;If it is found, SSH is enabled and the file is deleted. The content of  the file does not matter; it could contain text, or nothing at all.&lt;/p&gt;

&lt;h2 id=&quot;test-installation&quot;&gt;Test installation&lt;/h2&gt;

&lt;p&gt;Boot. After a couple of minutes, I have a notification on phone saying a device is connected on my phone hotspot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/home/explore/git/guillaume/blog/images/raspberrypi_hotspot.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And ssh raspberry (default username/password are pi/raspberry)&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; pi 192.168.43.179
pi@192.168.43.179&lt;span class=&quot;s1&quot;&gt;'s password: 
Linux raspberrypi 5.4.83-v7+ #1379 SMP Mon Dec 14 13:08:57 GMT 2020 armv7l

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Thu Mar 25 06:23:17 2021

SSH is enabled and the default password for the '&lt;/span&gt;pi&lt;span class=&quot;s1&quot;&gt;' user has not been changed.
This is a security risk - please login as the '&lt;/span&gt;pi&lt;span class=&quot;s1&quot;&gt;' user and type '&lt;/span&gt;passwd&lt;span class=&quot;s1&quot;&gt;' to set a new password.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Headless raspberry is ready to be used.&lt;/p&gt;

&lt;h2 id=&quot;wifi-to-ethernet-bridge&quot;&gt;Wifi to ethernet bridge&lt;/h2&gt;

&lt;p&gt;I will use &lt;a href=&quot;https://willhaley.com/blog/raspberry-pi-wifi-ethernet-bridge/&quot;&gt;https://willhaley.com/blog/raspberry-pi-wifi-ethernet-bridge/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The only  package that is needed is dnsmasq however from a clean install it is a  good idea to make sure everything is up-to-date:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;get up-to-date system&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get upgrade &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;rpi-update dnsmasq &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;rpi-update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Option 1 - Same Subnet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Save this script as a file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bridge.sh&lt;/code&gt; on your Pi.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env bash&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$EUID&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-ne&lt;/span&gt; 0 &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;run as root&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&amp;amp;2 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;1

&lt;span class=&quot;c&quot;&gt;##########################################################&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# You should not need to update anything below this line #&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;##########################################################&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# parprouted  - Proxy ARP IP bridging daemon&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# dhcp-helper - DHCP/BOOTP relay agent&lt;/span&gt;

apt update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; parprouted dhcp-helper

systemctl stop dhcp-helper
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;dhcp-helper

&lt;span class=&quot;c&quot;&gt;# Enable ipv4 forwarding.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt; s/#net.ipv4.ip_forward&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1/net.ipv4.ip_forward&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1/ /etc/sysctl.conf

&lt;span class=&quot;c&quot;&gt;# Service configuration for standard WiFi connection. Connectivity will&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# be lost if the username and password are incorrect.&lt;/span&gt;
systemctl restart wpa_supplicant.service

&lt;span class=&quot;c&quot;&gt;# Enable IP forwarding for wlan0 if it's not already enabled.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'^option ip-forwarding 1$'&lt;/span&gt; /etc/dhcpcd.conf &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;option ip-forwarding 1&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /etc/dhcpcd.conf

&lt;span class=&quot;c&quot;&gt;# Disable dhcpcd control of eth0.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'^denyinterfaces eth0$'&lt;/span&gt; /etc/dhcpcd.conf &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;denyinterfaces eth0&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /etc/dhcpcd.conf

&lt;span class=&quot;c&quot;&gt;# Configure dhcp-helper.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /etc/default/dhcp-helper &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
DHCPHELPER_OPTS=&quot;-b wlan0&quot;
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF

&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Enable avahi reflector if it's not already enabled.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/#enable-reflector=no/enable-reflector=yes/'&lt;/span&gt; /etc/avahi/avahi-daemon.conf
&lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'^enable-reflector=yes$'&lt;/span&gt; /etc/avahi/avahi-daemon.conf &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;something went wrong...&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Manually set 'enable-reflector=yes in /etc/avahi/avahi-daemon.conf'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# I have to admit, I do not understand ARP and IP forwarding enough to explain&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# exactly what is happening here. I am building off the work of others. In short&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# this is a service to forward traffic from WiFi to Ethernet.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;' &amp;gt;/usr/lib/systemd/system/parprouted.service
[Unit]
Description=proxy arp routing service
Documentation=https://raspberrypi.stackexchange.com/q/88954/79866
Requires=sys-subsystem-net-devices-wlan0.device dhcpcd.service
After=sys-subsystem-net-devices-wlan0.device dhcpcd.service

[Service]
Type=forking
# Restart until wlan0 gained carrier
Restart=on-failure
RestartSec=5
TimeoutStartSec=30
# clone the dhcp-allocated IP to eth0 so dhcp-helper will relay for the correct subnet
ExecStartPre=/bin/bash -c '/sbin/ip addr add &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;/sbin/ip &lt;span class=&quot;nt&quot;&gt;-4&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-br&lt;/span&gt; addr show wlan0 | /bin/grep &lt;span class=&quot;nt&quot;&gt;-Po&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;/32 dev eth0'
ExecStartPre=/sbin/ip link set dev eth0 up
ExecStartPre=/sbin/ip link set wlan0 promisc on
ExecStart=-/usr/sbin/parprouted eth0 wlan0
ExecStopPost=/sbin/ip link set wlan0 promisc off
ExecStopPost=/sbin/ip link set dev eth0 down
ExecStopPost=/bin/bash -c '/sbin/ip addr del &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;/sbin/ip &lt;span class=&quot;nt&quot;&gt;-4&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-br&lt;/span&gt; addr show wlan0 | /bin/grep &lt;span class=&quot;nt&quot;&gt;-Po&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;/32 dev eth0'

[Install]
WantedBy=wpa_supplicant.service
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF

&lt;/span&gt;systemctl daemon-reload
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;parprouted
systemctl start parprouted dhcp-helper
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Execute the script on your Pi like so.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;bash bridge.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Reboot.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;reboot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Done!&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="raspberry pi" /><summary type="html">After my internet provider router stopped unexpectedly yesterday, I had to find a solution with internet access from phones and raspberry pi to broadcast internet to full home devices.</summary></entry><entry><title type="html">Stable baselines 3 - 1st steps</title><link href="https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html" rel="alternate" type="text/html" title="Stable baselines 3 - 1st steps" /><published>2021-03-24T00:00:00-05:00</published><updated>2021-03-24T00:00:00-05:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-03-24-stable-baselines-3.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;What-is-stable-baselines-3-(sb3)&quot;&gt;What is stable baselines 3 (sb3)&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-stable-baselines-3-(sb3)&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;I have just read about this new release. This is a complete rewrite of stable baselines 2, without any reference to tensorflow, and based on pytorch (&amp;gt;1.4+).&lt;/p&gt;
&lt;p&gt;There is a lot of running implementations of RL algorithms, based on gym.
A very good introduction in this &lt;a href=&quot;https://araffin.github.io/post/sb3/&quot;&gt;blog entry&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://araffin.github.io/post/sb3/&quot;&gt;Stable-Baselines3: Reliable Reinforcement Learning Implementations | Antonin Raffin | Homepage&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;h2 id=&quot;Links&quot;&gt;Links&lt;a class=&quot;anchor-link&quot; href=&quot;#Links&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;GitHub repository:&lt;a href=&quot;https://github.com/DLR-RM/stable-baselines3&quot;&gt;https://github.com/DLR-RM/stable-baselines3&lt;/a&gt;&amp;gt; Documentation:&lt;a href=&quot;https://stable-baselines3.readthedocs.io/&quot;&gt;https://stable-baselines3.readthedocs.io/&lt;/a&gt;&amp;gt; RL Baselines3 Zoo:&lt;a href=&quot;https://github.com/DLR-RM/rl-baselines3-zoo&quot;&gt;https://github.com/DLR-RM/rl-baselines3-zoo&lt;/a&gt;&amp;gt; Contrib:&lt;a href=&quot;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&quot;&gt;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&lt;/a&gt;&amp;gt; RL Tutorial:&lt;a href=&quot;https://github.com/araffin/rl-tutorial-jnrr19&quot;&gt;https://github.com/araffin/rl-tutorial-jnrr19&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;My-installation&quot;&gt;My installation&lt;a class=&quot;anchor-link&quot; href=&quot;#My-installation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Standard installation&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda create --name stablebaselines3 &lt;span class=&quot;nv&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;.7
conda activate stablebaselines3
pip install stable-baselines3&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;extra&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
conda install -c conda-forge jupyter_contrib_nbextensions
conda install nb_conda
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;conda list
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;# packages in environment at /home/explore/miniconda3/envs/stablebaselines3:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main  
absl-py                   0.12.0                   pypi_0    pypi
atari-py                  0.2.6                    pypi_0    pypi
attrs                     20.3.0             pyhd3deb0d_0    conda-forge
backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
backports                 1.0                        py_2    conda-forge
backports.functools_lru_cache 1.6.1                      py_0    conda-forge
bleach                    3.3.0              pyh44b312d_0    conda-forge
box2d                     2.3.10                   pypi_0    pypi
box2d-py                  2.3.8                    pypi_0    pypi
ca-certificates           2021.1.19            h06a4308_1  
cachetools                4.2.1                    pypi_0    pypi
certifi                   2020.12.5        py37h06a4308_0  
chardet                   4.0.0                    pypi_0    pypi
cloudpickle               1.6.0                    pypi_0    pypi
cycler                    0.10.0                   pypi_0    pypi
decorator                 4.4.2                      py_0    conda-forge
defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
future                    0.18.2                   pypi_0    pypi
google-auth               1.28.0                   pypi_0    pypi
google-auth-oauthlib      0.4.3                    pypi_0    pypi
grpcio                    1.36.1                   pypi_0    pypi
gym                       0.18.0                   pypi_0    pypi
icu                       58.2              hf484d3e_1000    conda-forge
idna                      2.10                     pypi_0    pypi
importlib-metadata        3.7.3            py37h89c1867_0    conda-forge
ipykernel                 5.5.0            py37h888b3d9_1    conda-forge
ipython                   7.21.0           py37h888b3d9_0    conda-forge
ipython_genutils          0.2.0                      py_1    conda-forge
jedi                      0.18.0           py37h89c1867_2    conda-forge
jinja2                    2.11.3             pyh44b312d_0    conda-forge
jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
jupyter_client            6.1.12             pyhd8ed1ab_0    conda-forge
jupyter_contrib_core      0.3.3                      py_2    conda-forge
jupyter_contrib_nbextensions 0.5.1              pyhd8ed1ab_2    conda-forge
jupyter_core              4.7.1            py37h89c1867_0    conda-forge
jupyter_highlight_selected_word 0.2.0           py37h89c1867_1002    conda-forge
jupyter_latex_envs        1.4.6           pyhd8ed1ab_1002    conda-forge
jupyter_nbextensions_configurator 0.4.1            py37h89c1867_2    conda-forge
kiwisolver                1.3.1                    pypi_0    pypi
ld_impl_linux-64          2.33.1               h53a641e_7  
libffi                    3.3                  he6710b0_2  
libgcc-ng                 9.1.0                hdf63c60_0  
libsodium                 1.0.18               h36c2ea0_1    conda-forge
libstdcxx-ng              9.1.0                hdf63c60_0  
libxml2                   2.9.10               hb55368b_3  
libxslt                   1.1.34               hc22bd24_0  
lxml                      4.6.3            py37h9120a33_0  
markdown                  3.3.4                    pypi_0    pypi
markupsafe                1.1.1            py37hb5d75c8_2    conda-forge
matplotlib                3.3.4                    pypi_0    pypi
mistune                   0.8.4           py37h4abf009_1002    conda-forge
nb_conda                  2.2.1                    py37_0  
nb_conda_kernels          2.3.1            py37h06a4308_0  
nbconvert                 5.6.1            py37hc8dfbb8_1    conda-forge
nbformat                  5.1.2              pyhd8ed1ab_1    conda-forge
ncurses                   6.2                  he6710b0_1  
notebook                  5.7.10           py37hc8dfbb8_0    conda-forge
numpy                     1.20.1                   pypi_0    pypi
oauthlib                  3.1.0                    pypi_0    pypi
opencv-python             4.5.1.48                 pypi_0    pypi
openssl                   1.1.1j               h27cfd23_0  
packaging                 20.9               pyh44b312d_0    conda-forge
pandas                    1.2.3                    pypi_0    pypi
pandoc                    2.12                 h7f98852_0    conda-forge
pandocfilters             1.4.2                      py_1    conda-forge
parso                     0.8.1              pyhd8ed1ab_0    conda-forge
pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
pickleshare               0.7.5                   py_1003    conda-forge
pillow                    7.2.0                    pypi_0    pypi
pip                       21.0.1           py37h06a4308_0  
prometheus_client         0.9.0              pyhd3deb0d_0    conda-forge
prompt-toolkit            3.0.18             pyha770c72_0    conda-forge
protobuf                  3.15.6                   pypi_0    pypi
psutil                    5.8.0                    pypi_0    pypi
ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
pyasn1                    0.4.8                    pypi_0    pypi
pyasn1-modules            0.2.8                    pypi_0    pypi
pyglet                    1.5.0                    pypi_0    pypi
pygments                  2.8.1              pyhd8ed1ab_0    conda-forge
pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
pyrsistent                0.17.3           py37h4abf009_1    conda-forge
python                    3.7.10               hdb3f193_0  
python-dateutil           2.8.1                      py_0    conda-forge
python_abi                3.7                     1_cp37m    conda-forge
pytz                      2021.1                   pypi_0    pypi
pyyaml                    5.3.1            py37hb5d75c8_1    conda-forge
pyzmq                     19.0.2           py37hac76be4_2    conda-forge
readline                  8.1                  h27cfd23_0  
requests                  2.25.1                   pypi_0    pypi
requests-oauthlib         1.3.0                    pypi_0    pypi
rsa                       4.7.2                    pypi_0    pypi
scipy                     1.6.1                    pypi_0    pypi
send2trash                1.5.0                      py_0    conda-forge
setuptools                52.0.0           py37h06a4308_0  
six                       1.15.0             pyh9f0ad1d_0    conda-forge
sqlite                    3.35.2               hdfb4753_0  
stable-baselines3         1.0                      pypi_0    pypi
tensorboard               2.4.1                    pypi_0    pypi
tensorboard-plugin-wit    1.8.0                    pypi_0    pypi
terminado                 0.9.3            py37h89c1867_0    conda-forge
testpath                  0.4.4                      py_0    conda-forge
tk                        8.6.10               hbc83047_0  
torch                     1.8.0                    pypi_0    pypi
tornado                   6.1              py37h4abf009_0    conda-forge
traitlets                 5.0.5                      py_0    conda-forge
typing_extensions         3.7.4.3                    py_0    conda-forge
urllib3                   1.26.4                   pypi_0    pypi
wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
webencodings              0.5.1                      py_1    conda-forge
werkzeug                  1.0.1                    pypi_0    pypi
wheel                     0.36.2             pyhd3eb1b0_0  
xz                        5.2.5                h7b6447c_0  
yaml                      0.2.5                h516909a_0    conda-forge
zeromq                    4.3.4                h2531618_0  
zipp                      3.4.1              pyhd8ed1ab_0    conda-forge
zlib                      1.2.11               h7b6447c_3  
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;SB3-tutorials&quot;&gt;SB3 tutorials&lt;a class=&quot;anchor-link&quot; href=&quot;#SB3-tutorials&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gym&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;stable_baselines3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2C&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;stable_baselines3.common.monitor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Monitor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;stable_baselines3.common.callbacks&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CheckpointCallback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EvalCallback&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Save a checkpoint every 1000 steps&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;checkpoint_callback&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CheckpointCallback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/home/explore/git/guillaume/stable_baselines_3/logs/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                         &lt;span class=&quot;n&quot;&gt;name_prefix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;rl_model&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Evaluate the model periodically&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# and auto-save the best model and evaluations&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Use a monitor wrapper to properly report episode stats&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eval_env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;LunarLander-v2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Use deterministic actions for evaluation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eval_callback&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EvalCallback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_model_save_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/home/explore/git/guillaume/stable_baselines_3/logs/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;log_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/home/explore/git/guillaume/stable_baselines_3/logs/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Train an agent using A2C on LunarLander-v2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;MlpPolicy&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;LunarLander-v2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_timesteps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callback&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval_callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Retrieve and reset the environment&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Query the agent (stochastic action here)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Using cuda device
Creating environment from the given name &amp;#39;LunarLander-v2&amp;#39;
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 97.6     |
|    ep_rew_mean        | -265     |
| time/                 |          |
|    fps                | 484      |
|    iterations         | 100      |
|    time_elapsed       | 1        |
|    total_timesteps    | 500      |
| train/                |          |
|    entropy_loss       | -1.28    |
|    explained_variance | -0.0391  |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | -5.3     |
|    value_loss         | 17.3     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 107      |
|    ep_rew_mean        | -249     |
| time/                 |          |
|    fps                | 499      |
|    iterations         | 200      |
|    time_elapsed       | 2        |
|    total_timesteps    | 1000     |
| train/                |          |
|    entropy_loss       | -1.19    |
|    explained_variance | 0.00593  |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | 2.79     |
|    value_loss         | 16.6     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 108      |
|    ep_rew_mean        | -277     |
| time/                 |          |
|    fps                | 502      |
|    iterations         | 300      |
|    time_elapsed       | 2        |
|    total_timesteps    | 1500     |
| train/                |          |
|    entropy_loss       | -1.22    |
|    explained_variance | -0.00474 |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | -2.79    |
|    value_loss         | 9.45     |
------------------------------------
Eval num_timesteps=2000, episode_reward=-1534.45 +/- 783.96
Episode length: 184.80 +/- 74.13
New best mean reward!
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 185       |
|    mean_reward        | -1.53e+03 |
| rollout/              |           |
|    ep_len_mean        | 117       |
|    ep_rew_mean        | -296      |
| time/                 |           |
|    fps                | 414       |
|    iterations         | 400       |
|    time_elapsed       | 4         |
|    total_timesteps    | 2000      |
| train/                |           |
|    entropy_loss       | -1.1      |
|    explained_variance | 0.0118    |
|    learning_rate      | 0.0007    |
|    n_updates          | 399       |
|    policy_loss        | -1.07     |
|    value_loss         | 5.93      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 131      |
|    ep_rew_mean        | -299     |
| time/                 |          |
|    fps                | 426      |
|    iterations         | 500      |
|    time_elapsed       | 5        |
|    total_timesteps    | 2500     |
| train/                |          |
|    entropy_loss       | -1.17    |
|    explained_variance | 0.00582  |
|    learning_rate      | 0.0007   |
|    n_updates          | 499      |
|    policy_loss        | 2.68     |
|    value_loss         | 14       |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 133       |
|    ep_rew_mean        | -309      |
| time/                 |           |
|    fps                | 428       |
|    iterations         | 600       |
|    time_elapsed       | 6         |
|    total_timesteps    | 3000      |
| train/                |           |
|    entropy_loss       | -0.902    |
|    explained_variance | -1.94e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 599       |
|    policy_loss        | -5.9      |
|    value_loss         | 38.7      |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 133       |
|    ep_rew_mean        | -286      |
| time/                 |           |
|    fps                | 437       |
|    iterations         | 700       |
|    time_elapsed       | 7         |
|    total_timesteps    | 3500      |
| train/                |           |
|    entropy_loss       | -1.13     |
|    explained_variance | -0.000943 |
|    learning_rate      | 0.0007    |
|    n_updates          | 699       |
|    policy_loss        | 1.34      |
|    value_loss         | 5.43      |
-------------------------------------
Eval num_timesteps=4000, episode_reward=-2461.14 +/- 815.61
Episode length: 450.40 +/- 58.92
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 450       |
|    mean_reward        | -2.46e+03 |
| rollout/              |           |
|    ep_len_mean        | 137       |
|    ep_rew_mean        | -276      |
| time/                 |           |
|    fps                | 343       |
|    iterations         | 800       |
|    time_elapsed       | 11        |
|    total_timesteps    | 4000      |
| train/                |           |
|    entropy_loss       | -0.763    |
|    explained_variance | -0.00105  |
|    learning_rate      | 0.0007    |
|    n_updates          | 799       |
|    policy_loss        | 1.13      |
|    value_loss         | 24.1      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 140      |
|    ep_rew_mean        | -274     |
| time/                 |          |
|    fps                | 354      |
|    iterations         | 900      |
|    time_elapsed       | 12       |
|    total_timesteps    | 4500     |
| train/                |          |
|    entropy_loss       | -0.87    |
|    explained_variance | -0.00024 |
|    learning_rate      | 0.0007   |
|    n_updates          | 899      |
|    policy_loss        | 11.7     |
|    value_loss         | 349      |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 151       |
|    ep_rew_mean        | -270      |
| time/                 |           |
|    fps                | 363       |
|    iterations         | 1000      |
|    time_elapsed       | 13        |
|    total_timesteps    | 5000      |
| train/                |           |
|    entropy_loss       | -0.949    |
|    explained_variance | -0.000288 |
|    learning_rate      | 0.0007    |
|    n_updates          | 999       |
|    policy_loss        | -3.85     |
|    value_loss         | 6.77      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 152      |
|    ep_rew_mean        | -262     |
| time/                 |          |
|    fps                | 371      |
|    iterations         | 1100     |
|    time_elapsed       | 14       |
|    total_timesteps    | 5500     |
| train/                |          |
|    entropy_loss       | -0.836   |
|    explained_variance | 0.000152 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1099     |
|    policy_loss        | -2.77    |
|    value_loss         | 4.58     |
------------------------------------
Eval num_timesteps=6000, episode_reward=-1441.51 +/- 344.51
Episode length: 470.00 +/- 216.72
New best mean reward!
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 470       |
|    mean_reward        | -1.44e+03 |
| rollout/              |           |
|    ep_len_mean        | 154       |
|    ep_rew_mean        | -246      |
| time/                 |           |
|    fps                | 315       |
|    iterations         | 1200      |
|    time_elapsed       | 19        |
|    total_timesteps    | 6000      |
| train/                |           |
|    entropy_loss       | -0.655    |
|    explained_variance | 4.6e-05   |
|    learning_rate      | 0.0007    |
|    n_updates          | 1199      |
|    policy_loss        | -3.96     |
|    value_loss         | 10        |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 158       |
|    ep_rew_mean        | -242      |
| time/                 |           |
|    fps                | 323       |
|    iterations         | 1300      |
|    time_elapsed       | 20        |
|    total_timesteps    | 6500      |
| train/                |           |
|    entropy_loss       | -0.799    |
|    explained_variance | -0.000516 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1299      |
|    policy_loss        | -4.78     |
|    value_loss         | 91.6      |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 162       |
|    ep_rew_mean        | -238      |
| time/                 |           |
|    fps                | 330       |
|    iterations         | 1400      |
|    time_elapsed       | 21        |
|    total_timesteps    | 7000      |
| train/                |           |
|    entropy_loss       | -0.623    |
|    explained_variance | -0.000611 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1399      |
|    policy_loss        | 8.3       |
|    value_loss         | 88.8      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 171      |
|    ep_rew_mean        | -227     |
| time/                 |          |
|    fps                | 333      |
|    iterations         | 1500     |
|    time_elapsed       | 22       |
|    total_timesteps    | 7500     |
| train/                |          |
|    entropy_loss       | -0.956   |
|    explained_variance | 1.63e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1499     |
|    policy_loss        | -3.67    |
|    value_loss         | 41.2     |
------------------------------------
Eval num_timesteps=8000, episode_reward=-684.75 +/- 100.33
Episode length: 490.60 +/- 85.08
New best mean reward!
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 491      |
|    mean_reward        | -685     |
| rollout/              |          |
|    ep_len_mean        | 174      |
|    ep_rew_mean        | -223     |
| time/                 |          |
|    fps                | 302      |
|    iterations         | 1600     |
|    time_elapsed       | 26       |
|    total_timesteps    | 8000     |
| train/                |          |
|    entropy_loss       | -0.144   |
|    explained_variance | 0.00724  |
|    learning_rate      | 0.0007   |
|    n_updates          | 1599     |
|    policy_loss        | -0.162   |
|    value_loss         | 61.2     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 176      |
|    ep_rew_mean        | -220     |
| time/                 |          |
|    fps                | 308      |
|    iterations         | 1700     |
|    time_elapsed       | 27       |
|    total_timesteps    | 8500     |
| train/                |          |
|    entropy_loss       | -0.528   |
|    explained_variance | 9.72e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 1699     |
|    policy_loss        | 2        |
|    value_loss         | 7.58     |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 175       |
|    ep_rew_mean        | -219      |
| time/                 |           |
|    fps                | 314       |
|    iterations         | 1800      |
|    time_elapsed       | 28        |
|    total_timesteps    | 9000      |
| train/                |           |
|    entropy_loss       | -0.726    |
|    explained_variance | -0.000773 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1799      |
|    policy_loss        | 9.47      |
|    value_loss         | 243       |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 179       |
|    ep_rew_mean        | -218      |
| time/                 |           |
|    fps                | 320       |
|    iterations         | 1900      |
|    time_elapsed       | 29        |
|    total_timesteps    | 9500      |
| train/                |           |
|    entropy_loss       | -0.609    |
|    explained_variance | -0.000227 |
|    learning_rate      | 0.0007    |
|    n_updates          | 1899      |
|    policy_loss        | 2.69      |
|    value_loss         | 145       |
-------------------------------------
Eval num_timesteps=10000, episode_reward=-1105.99 +/- 697.41
Episode length: 859.40 +/- 72.91
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 859       |
|    mean_reward        | -1.11e+03 |
| rollout/              |           |
|    ep_len_mean        | 181       |
|    ep_rew_mean        | -218      |
| time/                 |           |
|    fps                | 270       |
|    iterations         | 2000      |
|    time_elapsed       | 37        |
|    total_timesteps    | 10000     |
| train/                |           |
|    entropy_loss       | -1.03     |
|    explained_variance | -0.11     |
|    learning_rate      | 0.0007    |
|    n_updates          | 1999      |
|    policy_loss        | -3.99     |
|    value_loss         | 29.9      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 181      |
|    ep_rew_mean        | -218     |
| time/                 |          |
|    fps                | 273      |
|    iterations         | 2100     |
|    time_elapsed       | 38       |
|    total_timesteps    | 10500    |
| train/                |          |
|    entropy_loss       | -0.895   |
|    explained_variance | 0.191    |
|    learning_rate      | 0.0007   |
|    n_updates          | 2099     |
|    policy_loss        | 0.0962   |
|    value_loss         | 0.102    |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 198       |
|    ep_rew_mean        | -212      |
| time/                 |           |
|    fps                | 278       |
|    iterations         | 2200      |
|    time_elapsed       | 39        |
|    total_timesteps    | 11000     |
| train/                |           |
|    entropy_loss       | -0.706    |
|    explained_variance | -5.46e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2199      |
|    policy_loss        | 7.79      |
|    value_loss         | 206       |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 200       |
|    ep_rew_mean        | -213      |
| time/                 |           |
|    fps                | 283       |
|    iterations         | 2300      |
|    time_elapsed       | 40        |
|    total_timesteps    | 11500     |
| train/                |           |
|    entropy_loss       | -0.797    |
|    explained_variance | -0.000233 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2299      |
|    policy_loss        | -5.37     |
|    value_loss         | 22.2      |
-------------------------------------
Eval num_timesteps=12000, episode_reward=-175.07 +/- 257.78
Episode length: 767.20 +/- 287.16
New best mean reward!
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 767       |
|    mean_reward        | -175      |
| rollout/              |           |
|    ep_len_mean        | 202       |
|    ep_rew_mean        | -210      |
| time/                 |           |
|    fps                | 253       |
|    iterations         | 2400      |
|    time_elapsed       | 47        |
|    total_timesteps    | 12000     |
| train/                |           |
|    entropy_loss       | -0.999    |
|    explained_variance | -0.000878 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2399      |
|    policy_loss        | -0.454    |
|    value_loss         | 3.24      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 208      |
|    ep_rew_mean        | -210     |
| time/                 |          |
|    fps                | 256      |
|    iterations         | 2500     |
|    time_elapsed       | 48       |
|    total_timesteps    | 12500    |
| train/                |          |
|    entropy_loss       | -0.0409  |
|    explained_variance | -0.00201 |
|    learning_rate      | 0.0007   |
|    n_updates          | 2499     |
|    policy_loss        | -0.00709 |
|    value_loss         | 1.45     |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 206       |
|    ep_rew_mean        | -206      |
| time/                 |           |
|    fps                | 262       |
|    iterations         | 2600      |
|    time_elapsed       | 49        |
|    total_timesteps    | 13000     |
| train/                |           |
|    entropy_loss       | -0.394    |
|    explained_variance | -0.000404 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2599      |
|    policy_loss        | -3.02     |
|    value_loss         | 27.3      |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 209       |
|    ep_rew_mean        | -202      |
| time/                 |           |
|    fps                | 266       |
|    iterations         | 2700      |
|    time_elapsed       | 50        |
|    total_timesteps    | 13500     |
| train/                |           |
|    entropy_loss       | -0.727    |
|    explained_variance | -0.000365 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2699      |
|    policy_loss        | 4.28      |
|    value_loss         | 52.3      |
-------------------------------------
Eval num_timesteps=14000, episode_reward=-45.13 +/- 159.96
Episode length: 595.40 +/- 209.59
New best mean reward!
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 595       |
|    mean_reward        | -45.1     |
| rollout/              |           |
|    ep_len_mean        | 210       |
|    ep_rew_mean        | -200      |
| time/                 |           |
|    fps                | 251       |
|    iterations         | 2800      |
|    time_elapsed       | 55        |
|    total_timesteps    | 14000     |
| train/                |           |
|    entropy_loss       | -0.482    |
|    explained_variance | -3.78e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2799      |
|    policy_loss        | 8.53      |
|    value_loss         | 126       |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 213       |
|    ep_rew_mean        | -199      |
| time/                 |           |
|    fps                | 256       |
|    iterations         | 2900      |
|    time_elapsed       | 56        |
|    total_timesteps    | 14500     |
| train/                |           |
|    entropy_loss       | -0.692    |
|    explained_variance | -0.000127 |
|    learning_rate      | 0.0007    |
|    n_updates          | 2899      |
|    policy_loss        | -0.475    |
|    value_loss         | 3.51      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 213      |
|    ep_rew_mean        | -197     |
| time/                 |          |
|    fps                | 260      |
|    iterations         | 3000     |
|    time_elapsed       | 57       |
|    total_timesteps    | 15000    |
| train/                |          |
|    entropy_loss       | -0.373   |
|    explained_variance | 0.0614   |
|    learning_rate      | 0.0007   |
|    n_updates          | 2999     |
|    policy_loss        | 0.592    |
|    value_loss         | 61.1     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 217      |
|    ep_rew_mean        | -192     |
| time/                 |          |
|    fps                | 263      |
|    iterations         | 3100     |
|    time_elapsed       | 58       |
|    total_timesteps    | 15500    |
| train/                |          |
|    entropy_loss       | -0.827   |
|    explained_variance | 0.000454 |
|    learning_rate      | 0.0007   |
|    n_updates          | 3099     |
|    policy_loss        | -4.96    |
|    value_loss         | 167      |
------------------------------------
Eval num_timesteps=16000, episode_reward=-212.05 +/- 90.82
Episode length: 606.80 +/- 136.16
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 607      |
|    mean_reward        | -212     |
| rollout/              |          |
|    ep_len_mean        | 219      |
|    ep_rew_mean        | -192     |
| time/                 |          |
|    fps                | 250      |
|    iterations         | 3200     |
|    time_elapsed       | 63       |
|    total_timesteps    | 16000    |
| train/                |          |
|    entropy_loss       | -0.397   |
|    explained_variance | -0.0068  |
|    learning_rate      | 0.0007   |
|    n_updates          | 3199     |
|    policy_loss        | 0.713    |
|    value_loss         | 4.8      |
------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 222       |
|    ep_rew_mean        | -187      |
| time/                 |           |
|    fps                | 253       |
|    iterations         | 3300      |
|    time_elapsed       | 65        |
|    total_timesteps    | 16500     |
| train/                |           |
|    entropy_loss       | -0.691    |
|    explained_variance | -0.000298 |
|    learning_rate      | 0.0007    |
|    n_updates          | 3299      |
|    policy_loss        | -3.58     |
|    value_loss         | 29.9      |
-------------------------------------
-------------------------------------
| rollout/              |           |
|    ep_len_mean        | 222       |
|    ep_rew_mean        | -185      |
| time/                 |           |
|    fps                | 257       |
|    iterations         | 3400      |
|    time_elapsed       | 66        |
|    total_timesteps    | 17000     |
| train/                |           |
|    entropy_loss       | -0.678    |
|    explained_variance | -6.87e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 3399      |
|    policy_loss        | 4.88      |
|    value_loss         | 58.4      |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 222      |
|    ep_rew_mean        | -183     |
| time/                 |          |
|    fps                | 260      |
|    iterations         | 3500     |
|    time_elapsed       | 67       |
|    total_timesteps    | 17500    |
| train/                |          |
|    entropy_loss       | -0.973   |
|    explained_variance | -0.46    |
|    learning_rate      | 0.0007   |
|    n_updates          | 3499     |
|    policy_loss        | -1.88    |
|    value_loss         | 100      |
------------------------------------
Eval num_timesteps=18000, episode_reward=1.73 +/- 344.63
Episode length: 609.20 +/- 210.81
New best mean reward!
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 609      |
|    mean_reward        | 1.73     |
| rollout/              |          |
|    ep_len_mean        | 228      |
|    ep_rew_mean        | -177     |
| time/                 |          |
|    fps                | 245      |
|    iterations         | 3600     |
|    time_elapsed       | 73       |
|    total_timesteps    | 18000    |
| train/                |          |
|    entropy_loss       | -0.149   |
|    explained_variance | 0.0186   |
|    learning_rate      | 0.0007   |
|    n_updates          | 3599     |
|    policy_loss        | 0.198    |
|    value_loss         | 31.9     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 228      |
|    ep_rew_mean        | -173     |
| time/                 |          |
|    fps                | 248      |
|    iterations         | 3700     |
|    time_elapsed       | 74       |
|    total_timesteps    | 18500    |
| train/                |          |
|    entropy_loss       | -0.369   |
|    explained_variance | 0.0153   |
|    learning_rate      | 0.0007   |
|    n_updates          | 3699     |
|    policy_loss        | 0.175    |
|    value_loss         | 2.75     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 228      |
|    ep_rew_mean        | -171     |
| time/                 |          |
|    fps                | 252      |
|    iterations         | 3800     |
|    time_elapsed       | 75       |
|    total_timesteps    | 19000    |
| train/                |          |
|    entropy_loss       | -1.09    |
|    explained_variance | 0.00332  |
|    learning_rate      | 0.0007   |
|    n_updates          | 3799     |
|    policy_loss        | -5.61    |
|    value_loss         | 94.5     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 228      |
|    ep_rew_mean        | -172     |
| time/                 |          |
|    fps                | 255      |
|    iterations         | 3900     |
|    time_elapsed       | 76       |
|    total_timesteps    | 19500    |
| train/                |          |
|    entropy_loss       | -0.441   |
|    explained_variance | -17.2    |
|    learning_rate      | 0.0007   |
|    n_updates          | 3899     |
|    policy_loss        | 0.701    |
|    value_loss         | 23.8     |
------------------------------------
Eval num_timesteps=20000, episode_reward=2.77 +/- 147.98
Episode length: 394.00 +/- 204.93
New best mean reward!
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 394      |
|    mean_reward        | 2.77     |
| rollout/              |          |
|    ep_len_mean        | 227      |
|    ep_rew_mean        | -170     |
| time/                 |          |
|    fps                | 250      |
|    iterations         | 4000     |
|    time_elapsed       | 79       |
|    total_timesteps    | 20000    |
| train/                |          |
|    entropy_loss       | -0.353   |
|    explained_variance | 0.221    |
|    learning_rate      | 0.0007   |
|    n_updates          | 3999     |
|    policy_loss        | 0.00796  |
|    value_loss         | 0.0138   |
------------------------------------
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Issues&quot;&gt;Issues&lt;a class=&quot;anchor-link&quot; href=&quot;#Issues&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;CUDA-error:-CUBLAS_STATUS_INTERNAL_ERROR&quot;&gt;CUDA error: CUBLAS_STATUS_INTERNAL_ERROR&lt;a class=&quot;anchor-link&quot; href=&quot;#CUDA-error:-CUBLAS_STATUS_INTERNAL_ERROR&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Downgrade pytorch to 1.7.1&lt;/p&gt;
&lt;p&gt;to avoid &lt;code&gt;RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasCreate(handle)&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install &lt;span class=&quot;nv&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;.7.1
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;RuntimeError:-CUDA-error:-invalid-device-function&quot;&gt;RuntimeError: CUDA error: invalid device function&lt;a class=&quot;anchor-link&quot; href=&quot;#RuntimeError:-CUDA-error:-invalid-device-function&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;nvidia-smi
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Thu Mar 25 09:13:49 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 4000     Off  | 00000000:01:00.0  On |                  N/A |
| N/A   41C    P5    18W /  N/A |   2104MiB /  7982MiB |     32%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1153      G   /usr/lib/xorg/Xorg                162MiB |
|    0   N/A  N/A      1904      G   /usr/lib/xorg/Xorg                268MiB |
|    0   N/A  N/A      2076      G   /usr/bin/gnome-shell              403MiB |
|    0   N/A  N/A      2697      G   ...gAAAAAAAAA --shared-files       54MiB |
|    0   N/A  N/A      7220      G   ...AAAAAAAAA= --shared-files       84MiB |
|    0   N/A  N/A     57454      G   /usr/lib/firefox/firefox            2MiB |
|    0   N/A  N/A     59274      C   ...ablebaselines3/bin/python     1051MiB |
+-----------------------------------------------------------------------------+
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;CUDA version is 11.0 on my workstation.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;nvcc --version
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;conda install &lt;span class=&quot;nv&quot;&gt;pytorch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;.7.1 &lt;span class=&quot;nv&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;.8.2 &lt;span class=&quot;nv&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;.7.2 &lt;span class=&quot;nv&quot;&gt;cudatoolkit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;11&lt;/span&gt;.0 -c pytorch
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Collecting package metadata (current_repodata.json): done
Solving environment: done

# All requested packages already installed.

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Everything seems fine after these updates.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Stable-baselines-3-user-guide&quot;&gt;Stable baselines 3 user guide&lt;a class=&quot;anchor-link&quot; href=&quot;#Stable-baselines-3-user-guide&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is an impressive documentation associated with stable baselines 3.
&lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html&quot;&gt;Quickstart&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Resource-page&quot;&gt;Resource page&lt;a class=&quot;anchor-link&quot; href=&quot;#Resource-page&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/rl.html&quot;&gt;Reinforcement Learning Resources — Stable Baselines3 1.1.0a1 documentation&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Stable-Baselines3 assumes that you already understand the basic concepts of Reinforcement Learning (RL).&lt;/p&gt;
&lt;p&gt;However, if you want to learn about RL, there are several good resources to get started:&amp;gt; &amp;gt; -   &lt;a href=&quot;https://spinningup.openai.com/en/latest/&quot;&gt;OpenAI Spinning Up&lt;/a&gt;&amp;gt; &amp;gt; -   &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&quot;&gt;David Silver’s course&lt;/a&gt;&amp;gt; &amp;gt; -   &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html&quot;&gt;Lilian Weng’s blog&lt;/a&gt;&amp;gt; &amp;gt; -   &lt;a href=&quot;https://sites.google.com/view/deep-rl-bootcamp/lectures&quot;&gt;Berkeley’s Deep RL Bootcamp&lt;/a&gt;&amp;gt; &amp;gt; -   &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse/&quot;&gt;Berkeley’s Deep Reinforcement Learning course&lt;/a&gt;&amp;gt; &amp;gt; -   &lt;a href=&quot;https://github.com/dennybritz/reinforcement-learning&quot;&gt;More resources&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="reinforcement learning" /><category term="pytorch" /><category term="sb3" /><summary type="html"></summary></entry><entry><title type="html">Git - How to find all *unpushed* commits for all projects in a directory?</title><link href="https://castorfou.github.io/guillaume_blog/blog/clustergit.html" rel="alternate" type="text/html" title="Git - How to find all *unpushed* commits for all projects in a directory?" /><published>2021-03-09T00:00:00-06:00</published><updated>2021-03-09T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/clustergit</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/clustergit.html">&lt;p&gt;Very basic question to help keep my repo clean.&lt;/p&gt;

&lt;h2 id=&quot;installation-clustergit&quot;&gt;Installation clustergit&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mnagel/clustergit&quot;&gt;clustergit&lt;/a&gt; seems a good candidate&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/mnagel/clustergit/master/doc/clustergit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/Applications
git clone git@github.com:mnagel/clustergit.git
&lt;span class=&quot;c&quot;&gt;# add export PATH=&quot;$PATH:$HOME/Applications/clustergit&quot; to ~.bashrc&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;usage-clustergit&quot;&gt;Usage clustergit&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;clustergit status&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;clustergit 
Scanning sub directories of &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
./Deep-Reinforcement-Learning-Hands-On  : Changesn &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1/17&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
./Deep_reinforcement_learning_Course    : Changes
./ReinforcementLearning_references      : On branch main, Untracked files
./blog                                  : Untracked files
./d059                                  : On branch main, Changes
./data-scientist-skills                 : Clean
./deeplearning_specialization           : Clean
./fastai                                : Changes
./fastai_experiments                    : Changes
./fastbook                              : Changes
./gan_specialization                    : Clean
./hello_nbdev                           : Clean
./introduction-reinforcement-learning-david-silver: On branch main, Untracked files
./mit_600.2x Introduction to Computational Thinking and Data Science: Clean
./mit_6S191_Intro_to_deep_learning      : On branch main, No Changes
./pytorch_tutorial                      : On branch main, Changes
./squeezebox                            : On branch main, No Changes
Done

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;clustergit status (detailed)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;clustergit &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;...]
&lt;span class=&quot;nt&quot;&gt;----------------&lt;/span&gt; ./squeezebox &lt;span class=&quot;nt&quot;&gt;-----------------&lt;/span&gt;
running  &lt;span class=&quot;nv&quot;&gt;LC_ALL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;C git status
On branch main
Your branch is up to &lt;span class=&quot;nb&quot;&gt;date &lt;/span&gt;with &lt;span class=&quot;s1&quot;&gt;'origin/main'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

nothing to commit, working tree clean

./squeezebox                            : On branch main, No Changes
&lt;span class=&quot;nt&quot;&gt;----------------&lt;/span&gt; ./squeezebox &lt;span class=&quot;nt&quot;&gt;-----------------&lt;/span&gt;
Done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;clustergit status (less detailed: hide Clean)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;clustergit &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt;
Scanning sub directories of &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
./d059                                  : On branch main, Changes
./fastai                                : Changes
./fastai_experiments                    : Changes
./fastbook                              : Changes
./introduction-reinforcement-learning-david-silver: On branch main, Untracked files
./mit_6S191_Intro_to_deep_learning      : On branch main, No Changes
./pytorch_tutorial                      : On branch main, Changes
./squeezebox                            : On branch main, No Changes
Done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Clean&lt;/strong&gt; vs &lt;strong&gt;On branch main, No Changes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;seems related to branch name. If branch is named master, then clean is displayed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Git pull, push&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I am not sure I will use it. But allows to recursively launch pull commands to update repos (if no local changes)&lt;/p&gt;

&lt;h2 id=&quot;rename-branches-from-main-to-master&quot;&gt;Rename branches from &lt;em&gt;main&lt;/em&gt; to &lt;em&gt;master&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.github.com/en/github/administering-a-repository/renaming-a-branch&quot;&gt;Renaming a branch&lt;/a&gt; from github website.&lt;/p&gt;

&lt;p&gt;Rename branch main to master from github website&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.github.com/assets/images/help/branches/branches-link.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Update local clones&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; main master
git fetch origin
git branch &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; origin/master master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;rabbitvcs&quot;&gt;RabbitVCS&lt;/h2&gt;

&lt;p&gt;From this &lt;a href=&quot;https://www.addictivetips.com/ubuntu-linux-tips/integrate-git-with-gnome-file-manager-on-linux/&quot;&gt;page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Installation&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;rabbitvcs-nautilus
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.addictivetips.com/wp-content/uploads/2018/10/rvcs-update-e1540364222288.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These overlay icons are not automatically updated (have to hit Ctrl-F5, it is a cache issue?) Which is not a surprise: number of actions are fired based on file modifications, and here status (commited, pushed) is not at all linked to file modifications. The system doesn’t know that overlay icon should be changed because file was not touched.&lt;/p&gt;

&lt;h2 id=&quot;git-nautilus-icons&quot;&gt;git-nautilus-icons&lt;/h2&gt;

&lt;p&gt;Just to check if it works better than RabbitVCS regarding overlay icon cache issue.&lt;/p&gt;

&lt;p&gt;No I didn’t manage to make it work. Back to RabbitVCS.&lt;/p&gt;

&lt;h2 id=&quot;activate-git-with-globalprotect&quot;&gt;Activate git with GlobalProtect&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;move from ssh to https, keeping password&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git remote &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt;
origin  git@github.com:castorfou/guillaume_blog.git &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;fetch&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
origin  git@github.com:castorfou/guillaume_blog.git &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;push&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;move to https://github.com/castorfou/guillaume_blog.git&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git remote set-url origin https://github.com/castorfou/guillaume_blog.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Make Git store the username and password and it will never ask for them.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config &lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt; credential.helper store
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Save the username and password for a session (cache it);&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config &lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt; credential.helper cache
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and to activate trace&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ GIT_TRACE_PACKET&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;GIT_TRACE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;GIT_CURL_VERBOSE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 git fetch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we can enrich certificates with Global Protect CA&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/anaconda3/ssl&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;certPG.pem /etc/ssl/certs/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;add-a-ca-certificate-in-ubuntu&quot;&gt;Add a ca-certificate in ubuntu&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local/share/ca-certificates/&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Create a new folder, i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo mkdir school&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Copy the . crt file into the school folder.&lt;/li&gt;
  &lt;li&gt;Make sure the permissions are OK (755 for the folder, 644 for the file)&lt;/li&gt;
  &lt;li&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo update-ca-certificates&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We should see effects in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/ssl/certs&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/etc/ssl/certs&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ll &lt;span class=&quot;nt&quot;&gt;-tr&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;..]
lrwxrwxrwx 1 root root     86 mars  24 10:02  cert_M_X5C_sase-net-sslfwd-trust-ca.pem -&amp;gt; /usr/local/share/ca-certificates/globalprotect/cert_M_X5C_sase-net-sslfwd-trust-ca.crt
lrwxrwxrwx 1 root root     39 mars  24 10:02  0dc7de9e.0 -&amp;gt; cert_M_X5C_sase-net-sslfwd-trust-ca.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="git" /><summary type="html">Very basic question to help keep my repo clean.</summary></entry><entry><title type="html">Introduction to Reinforcement Learning with David Silver</title><link href="https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html" rel="alternate" type="text/html" title="Introduction to Reinforcement Learning with David Silver" /><published>2021-03-09T00:00:00-06:00</published><updated>2021-03-09T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/Introduction%20to%20Reinforcement%20Learning%20with%20David%20Silver</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html">&lt;p&gt;This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver&quot;&gt;Website with 10 lectures: videos and slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/castorfou/introduction-reinforcement-learning-david-silver&quot;&gt;My repo with slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_sylabus.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3921---lecture-1-introduction-to-reinforcement-learning&quot;&gt;3/9/21 - Lecture 1: Introduction to Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;This introduction is essentially about giving examples of RL to have a good intuition about this field and to provide definitions or context:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Definitions: rewards, actions, agent, environment, state (and history)&lt;/li&gt;
  &lt;li&gt;Major components: policy, value function, model&lt;/li&gt;
  &lt;li&gt;Categorizing RL agents (taxonomy): value based, policy based, actor critic, model free, model based&lt;/li&gt;
  &lt;li&gt;Learning and planning&lt;/li&gt;
  &lt;li&gt;Prediction and control&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And David gives 2 references:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;well known&lt;/a&gt; Introduction to Reinforcement Learning, Sutton and Barto, 1998&lt;/li&gt;
  &lt;li&gt;Algorithms for Reinforcement Learning, Szepesvari. Available &lt;a href=&quot;http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf&quot;&gt;online&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(s): essentially a map from state to action. Can be deterministic &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(s) or stochastic &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(a|s).&lt;/p&gt;

&lt;p&gt;Value function v&lt;sub&gt;$\pi$&lt;/sub&gt;(s): is a prediction of expected future reward.&lt;/p&gt;

&lt;p&gt;Model: it is not the environment itself but useful to predict what the environment will do next. 2 types of models: transitions model and rewards model. Transition model predicts the next state (e.g. based on dynamics). Reward model predicts the next immediate reward.&lt;/p&gt;

&lt;p&gt;A lot of algorithms are model-free and doesn’t require these models. It is a fundamental distinctions in RL.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec1_taxonomy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then David explains 2 fundamental different problems with Learning vs Planning.&lt;/p&gt;

&lt;p&gt;With Learning, environment is unknown, agent interacts directly with the environment and improves its policy.&lt;/p&gt;

&lt;p&gt;With Planning, a model of environment is known, and agent “plays” with this model and improves its policy.&lt;/p&gt;

&lt;p&gt;These 2 problems may be linked where you start to learn from the environment and apply planning then.&lt;/p&gt;

&lt;p&gt;2 examples based on atari games.&lt;/p&gt;

&lt;p&gt;Another topic is exploration vs exploitation then prediction and control.&lt;/p&gt;

&lt;h2 id=&quot;31021---lecture-2-markov-decision-processes&quot;&gt;3/10/21 - Lecture 2: Markov Decision Processes&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Markov decision processes&lt;/em&gt;&lt;/strong&gt; formally describe an environment for reinforcement learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Markov property&lt;/em&gt;&lt;/strong&gt;: the future is independent of the past given the present.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Markov Process&lt;/em&gt;&lt;/strong&gt; (or &lt;strong&gt;&lt;em&gt;Markov Chain&lt;/em&gt;&lt;/strong&gt;) is the tuple (S, P)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_markovchain.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can take sample episodes from this chain. (e.g. C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep)&lt;/p&gt;

&lt;p&gt;We can formalize the transition matrix from s to s’.&lt;/p&gt;

&lt;p&gt;When you add reward you get &lt;strong&gt;&lt;em&gt;Markov reward process&lt;/em&gt;&lt;/strong&gt; (S, P, R, &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;Reward here is a function to map for each state the immediate reward.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the discounted factor, &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; [0,1]. David explains why we could need such discount.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Return&lt;/em&gt;&lt;/strong&gt; Gt is the total discounted reward at time-step t for a given sample.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_return.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Value function&lt;/em&gt;&lt;/strong&gt; v(s) is really what we care about, it is the long-term value of state s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_valuefunction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Bellman Equation for MRPs&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The value function can be decomposed into two parts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;immediate reward R&lt;sub&gt;t+1&lt;/sub&gt;&lt;/li&gt;
  &lt;li&gt;discounted value of next state &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.v (S&lt;sub&gt;t+1&lt;/sub&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_bellman.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We use that to calculate value function with &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; $\neq$ 0.&lt;/p&gt;

&lt;p&gt;And calculating value function can be seen as the resolution of this linear equation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_bellman_solving.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And now we introduce actions and it gives &lt;strong&gt;&lt;em&gt;Markov Decision Process&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_mdp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we introduce policy&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_policy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then we can define the &lt;em&gt;state-value function&lt;/em&gt; v&lt;sub&gt;$\pi$&lt;/sub&gt;(s,a) for a given policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_statevaluefunction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and &lt;em&gt;action-value function&lt;/em&gt; q&lt;sub&gt;$\pi$&lt;/sub&gt;(s,a) for a given policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_actionvaluefunction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And impact on Bellman Equation ends like that:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_bellman_mdp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;v is giving us how good it is to be in a state. q is giving us how good is it to take an action.&lt;/p&gt;

&lt;p&gt;And then we have the Bellman equation expressed with v and q.&lt;/p&gt;

&lt;p&gt;We don’t care much about a given v&lt;sub&gt;$\pi$&lt;/sub&gt;, we want to get the best policy. And ultimately to get q&lt;sub&gt;*&lt;/sub&gt; which is the &lt;strong&gt;optimal action value function&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/home/explore/git/guillaume/blog/images/deepmind_lec2_optimal_value_function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The optimal value function specifies the best possible performance in the MDP.
A MDP is “solved” when we know the optimal value function q&lt;sub&gt;*&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;What we really care about is &lt;strong&gt;optimal policy&lt;/strong&gt; &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;sub&gt;*&lt;/sub&gt;. There is a partial ordering about policies. And a theorem saying that for any MDP, there exists at least one optimal policy.&lt;/p&gt;

&lt;p&gt;So the optimal value function calculation is similar to what we did earlier when we averaged the value of the next state but now we take the max instead of average.&lt;/p&gt;

&lt;p&gt;So no we can write the &lt;strong&gt;Bellman Optimality Equation&lt;/strong&gt;. Unfortunately this is non-linear.&lt;/p&gt;

&lt;p&gt;There are many approaches such as iterative ones.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Value Iteration&lt;/li&gt;
  &lt;li&gt;Policy Iteration&lt;/li&gt;
  &lt;li&gt;Q-learning&lt;/li&gt;
  &lt;li&gt;Sarsa&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;31221---lecture-3-planning-by-dynamic-programming&quot;&gt;3/12/21 - Lecture 3: Planning by Dynamic Programming&lt;/h2&gt;

&lt;p&gt;Will discuss from the agent side: how to solve these MDP problems.&lt;/p&gt;

&lt;p&gt;David starts with general ideas on dynamic programming. (programming in a sense of policy)&lt;/p&gt;

&lt;p&gt;Value function is an important idea for RL because it sotres valuable information that you can later reuse (it embeds solutions). And Bellman equation gives the recursive decomposition.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Planning by Dynamic Programming&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We assume full knowledge of the MDP. Dynamic programming is used for planning in an MDP. With 2 usages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;prediction: given MDP and policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, we predict the value of this policy v&lt;sub&gt;$\pi$&lt;/sub&gt;.&lt;/li&gt;
  &lt;li&gt;control: given MDP, we get optimal value function v&lt;sub&gt;&amp;amp;ast;&lt;/sub&gt; and optimal policy $\pi$&lt;sub&gt;&amp;amp;ast;&lt;/sub&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And by full MDP it would mean for an atari game to have access to internal code to calculate everything.&lt;/p&gt;

&lt;p&gt;We need the 2 aspects to solve MDP: prediction to value policy, and control to get the best one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy Evaluation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Problem: evaluate a given policy π
Solution: iterative application of Bellman expectation backup&lt;/p&gt;

&lt;p&gt;(Bellman expectation is used in prediction, Bellman optimality is used in control)&lt;/p&gt;

&lt;p&gt;David takes an example with a small grid-world and calculates iteratively (k=0, 1, 2, …) v(s) for a uniform random policy (north, south, east, west with prob 0.25) (left column). And then we follow policy greedily using v function. (right column)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy Iteration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In small grid-world example, just by evaluating the policy and act greedily were sufficient to get the optimal policy. This is not generally the case. In general, need more iterations of  evaluation (iterative policy evaluation) / improvement (greedy policy).
But this process of policy iteration always converges to π∗&lt;/p&gt;

&lt;p&gt;David uses Jack’s Car Rental where it needs 4 steps to get the optimal policy.  And explains why acting greedy improves the policy.       And if improvement stops, Bellman optimality equation is satisfied, we have our optimal policy.&lt;/p&gt;

&lt;p&gt;Some question then about convergence of v&lt;sub&gt;$\pi$&lt;/sub&gt; . Why not update policy at each step of evaluation -&amp;gt; this is value iteration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value Iteration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Problem: find optimal policy π
Solution: iterative application of Bellman optimality backup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Extensions to dynamic programming&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DP uses full-width backups. It is effective for medium-sized problems. Curse of dimensionality for large problems. Even one backup can be too expensive.&lt;/p&gt;

&lt;p&gt;One solution is to &lt;strong&gt;sample backups&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Advantages:
Model-free: no advance knowledge of MDP required
Breaks the curse of dimensionality through sampling
Cost of backup is constant, independent of n = |S|&lt;/p&gt;

&lt;h2 id=&quot;31521---lecture-4-model-free-prediction&quot;&gt;3/15/21 - Lecture 4: Model-Free Prediction&lt;/h2&gt;

&lt;p&gt;Model-Free: no-one gives us the MDP. And we still want to solve it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Monte-Carlo learning&lt;/strong&gt;: basically methods which goes all the way to the end of trajectory and estimates value by looking at sample returns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Temporal-Difference learning&lt;/strong&gt;: goes one step ahead and estimates after one step&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TD(&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\lambda&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;λ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;)&lt;/strong&gt;: unify both approaches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We give up the assumption giving how the environment works (which is highly unrealistic for interesting problems). We break it down in 2 pieces (as with previous lecture with planning):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;policy evaluation case (this lecture) - how much reward we get from that policy (in model-free envt)&lt;/li&gt;
  &lt;li&gt;control (next lecture) - find the optimum value function and then optimum policy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Monte-Carlo Reinforcement Learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We go all the way through the episodes and we take sample returns. So the estimated value function can be the average of all returns. You have to terminate to perform this mean.&lt;/p&gt;

&lt;p&gt;It means we use the &lt;em&gt;empirical mean return&lt;/em&gt; in place of &lt;em&gt;expected return&lt;/em&gt;. (by &lt;em&gt;law of large numbers&lt;/em&gt;, this average returns will converge to value function as the number of episodes for that state tends to infinity)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal-Difference Reinforcement Learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TD learns from incomplete episodes, by bootstrapping&lt;/p&gt;

&lt;p&gt;David takes an example from Sutton about predicting time to commute home, comparing MC and TD.&lt;/p&gt;

&lt;p&gt;TD target (R&lt;sub&gt;t+1&lt;/sub&gt;+&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;V&lt;sub&gt;t+1&lt;/sub&gt;) is biased estimate of v&lt;sub&gt;$\pi$&lt;/sub&gt;(S&lt;sub&gt;t&lt;/sub&gt;), but has lower variance than the return G&lt;sub&gt;t&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;David compares perf of MC, TD(0), … using Random Walk example and different values of &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;31821---lecture-5-model-free-control&quot;&gt;3/18/21 - Lecture 5: Model-Free Control&lt;/h2&gt;

&lt;p&gt;Distinction between on-policy (learning by doing the job) and off-policy (following someone else behavior)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;on-policy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Monte-Carlo approach, we have 2 issues. First is that we don’t have access to model so we should use Q(s, a) instead of v(s). Second is lack of exploration so we should use &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-greedy policy.&lt;/p&gt;

&lt;p&gt;With GLIE (Greedy in the Limit with Infinite Exploration), we can update Q after each episodes.&lt;/p&gt;

&lt;p&gt;We will now use TD:&lt;/p&gt;

&lt;p&gt;Natural idea: use TD instead of MC in our control loop&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apply TD to Q(S, A)&lt;/li&gt;
  &lt;li&gt;Use &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-greedy policy improvement&lt;/li&gt;
  &lt;li&gt;Update every time-step&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is SARSA update. Every single time-step we update our diagram.&lt;/p&gt;

&lt;p&gt;A generalisation is n-step Sarsa. n=1 is standard Sarsa. n=&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\infty&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∞&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is MC.&lt;/p&gt;

&lt;p&gt;To get the best of both worlds, we consider Sarsa(&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\lambda&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;λ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;). We have a forward version&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/home/explore/git/guillaume/blog/images/deepmind_lec5_sarsal.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And a backward version which allows online experience. Thanks to eligibility traces.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;off-policy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Why is this important?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Learn from observing humans or other agents&lt;/li&gt;
  &lt;li&gt;Re-use experience generated from old policies π 1 , π 2 , …, π t−1&lt;/li&gt;
  &lt;li&gt;Learn about optimal policy while following exploratory policy&lt;/li&gt;
  &lt;li&gt;Learn about multiple policies while following one policy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can apply it in importance sampling for off-policy. With Monte-Carlo it is however useless due to high variance. It is imperative to to TD.&lt;/p&gt;

&lt;p&gt;We can apply that to Q-learning. We can use greedy slection on target policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; greedy on behaviour policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\mu&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;μ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;</content><author><name></name></author><category term="reinforcement learning" /><category term="deepmind" /><summary type="html">This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/RL.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/RL.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Use of gpg under linux</title><link href="https://castorfou.github.io/guillaume_blog/blog/gpg-linux.html" rel="alternate" type="text/html" title="Use of gpg under linux" /><published>2021-03-03T00:00:00-06:00</published><updated>2021-03-03T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/gpg-linux</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/gpg-linux.html">&lt;p&gt;from &lt;a href=&quot;https://www.fosslinux.com/27018/best-ways-to-encrypt-files-in-linux.htm&quot;&gt;best ways to encrypt files on linux&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gpg&quot;&gt;gpg&lt;/h2&gt;

&lt;h4 id=&quot;setup-the-key&quot;&gt;setup the key&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--gen-key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and enter a strong passphrase.&lt;/p&gt;

&lt;h4 id=&quot;export-public-key&quot;&gt;export public key&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--armor&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; mypubkey.gpg &lt;span class=&quot;nt&quot;&gt;--export&lt;/span&gt; &amp;lt;E-mail that you registered&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;import-from-windows-box&quot;&gt;import from windows box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--import&lt;/span&gt; mypubkey.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;encrypt-files-from-windows-box&quot;&gt;encrypt files from windows box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; test.txt.gpg &lt;span class=&quot;nt&quot;&gt;--encrypt&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--recipient&lt;/span&gt; &amp;lt;Receiver&lt;span class=&quot;s1&quot;&gt;'s E-Mail ID&amp;gt; test.txt
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;decrypt-files-on-linux-box&quot;&gt;decrypt files on linux box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; test.txt &lt;span class=&quot;nt&quot;&gt;--decrypt&lt;/span&gt; test.txt.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;find--gpg--tmpfs&quot;&gt;find + gpg + tmpfs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;encrypt from Windows&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'df_76*.csv'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-exec&lt;/span&gt; gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;.gpg &lt;span class=&quot;nt&quot;&gt;--encrypt&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--recipient&lt;/span&gt; guillaume.ramelet@michelin.com &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;decrypt from Linux&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There should be better ways to do it.&lt;/p&gt;

&lt;p&gt;Here is my process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Before starting: call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mount_decrypt.sh&lt;/code&gt;. It mounts a tmpfs in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secured_data/data&lt;/code&gt;, and decrypt all gpg files to this directory&lt;/li&gt;
  &lt;li&gt;
    &lt;work&gt;
&lt;/work&gt;
  &lt;/li&gt;
  &lt;li&gt;After work is done: call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;umount_decrypt.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpg_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;gpg_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;src_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;gpg_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;%.*&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;gpg decrypt &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$gpg_name&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; -&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$src_name&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$src_name&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--decrypt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$gpg_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mount_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;GPG_DEC_CMD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/gpg_decrypt.sh
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mount &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; tmpfs &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1G tmpfs &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /media/explore/CHACLEF/janus
find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'df_76*.csv.gpg'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-exec&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$GPG_DEC_CMD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;umount_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;umount &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="gpg" /><summary type="html">from best ways to encrypt files on linux</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/secure.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/secure.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Logbook for March 21</title><link href="https://castorfou.github.io/guillaume_blog/blog/logbook-March.html" rel="alternate" type="text/html" title="Logbook for March 21" /><published>2021-03-01T00:00:00-06:00</published><updated>2021-03-01T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/logbook-March</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/logbook-March.html">&lt;h2 id=&quot;week-9---mar-21&quot;&gt;Week 9 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Deep Generative Modeling (lecture 4) - vaes and gans.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; De-biasing Facial Recognition Systems (lab 2): CNN, VAE, DB-VAE&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Approximations non linéaires et réseaux de neurones (lecture 4)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver&quot;&gt;RL Course by David Silver&lt;/a&gt; lecture 1 - intro (22’/88’)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ilp.mit.edu/attend/future-manufacturing-mit?utm_medium=email&amp;amp;utm_campaign=Future%20of%20Manu%2032-3-4%20day%20of&amp;amp;utm_content=Future%20of%20Manu%2032-3-4%20day%20of+CID_523e5a27df7d82b6ebf742ac50bdef62&amp;amp;utm_source=Email%20campaign&amp;amp;utm_term=SEE%20THE%20AGENDA&quot;&gt;Future of Manufacturing@MIT&lt;/a&gt; - interesting landscape about Manufacturing and AI&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt; by Christoph Molnar. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/lime.html#lime&quot;&gt;LIME&lt;/a&gt; reading to understand context of local surrogate models. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/shap.html&quot;&gt;SHAP&lt;/a&gt; chapter using Janus data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 3 v1) on DQN with temporal limitation using LSTM, and experience replay. (replay buffer)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thursday 3/4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt; by Christoph Molnar. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/pdp.html&quot;&gt;PDP&lt;/a&gt; chapter using Janus data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 3/5&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p220-223) - full vs sample backups, trajectory sampling, heuristic search&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p223+) - start of Approximate Solution Methods, why to use NN.&lt;/p&gt;

&lt;h2 id=&quot;week-10---mar-21&quot;&gt;Week 10 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/8&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Deep Reinforcement Learning. Q-learning vs Policy Gradient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/9&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Ondelettes et échantillonnage (lecture 5)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Introduction to Reinforcement Learning (lecture 1)&lt;/p&gt;

&lt;p&gt;Installation of &lt;a href=&quot;/guillaume_blog/blog/clustergit.html&quot;&gt;clustergit&lt;/a&gt; to detect local (=uncommited  or unpushed) changes in repo&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/10&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 4 v1) on four strategies to improve DQN (fixed Q-targets, double DQN, dueling DQN (DDQN), Prioritized Experience Replay (PER))&lt;/p&gt;

&lt;p&gt;t-SNE using Janus data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Markov Decision Processes (lecture 2)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 3/12&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p287-352) - Applications and case studies, end of the book&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Planning by Dynamic Programming (lecture 3)&lt;/p&gt;

&lt;h2 id=&quot;week-11---mar-21&quot;&gt;Week 11 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/15&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Limitations and New Frontiers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Pixels-to-Control Learning (lab 3): Cartpole and Pong&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Model-Free Prediction (lecture 4)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/16&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Multi-résolutions (lecture 6)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/17&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 5 v1)  - Policy Gradient&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thursday 3/18&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 5 v1)  - Policy Gradient notebooks&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Model-Free Control (lecture 5)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 3/19&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 6 v1)  - Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C)&lt;/p&gt;

&lt;p&gt;College de France - l’apprentissage profond par Yann Lecunn 2016 - &lt;a href=&quot;https://www.college-de-france.fr/site/yann-lecun/course-2016-02-12-14h30.htm&quot;&gt;Pourquoi l’apprentissage profond ?&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;week-12---mar-21&quot;&gt;Week 12 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/22&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Evidential Deep Learning and Uncertainty (lecture 7).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (v1 Part 5)  - Advantage Actor Critic (A2C)  - implementation and video&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/23&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Bases orthonormales d’ondelettes (lecture 7)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/24&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 7 v1)  - Proximal Policy Optimization PPO&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/stable-baselines-3.html&quot;&gt;Stable baselines 3&lt;/a&gt; - init and 1st tutorial&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thursday 3/25&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;setup &lt;a href=&quot;/guillaume_blog/blog/headless-raspberry-pi-bridge-network.html&quot;&gt;headless raspberry pi&lt;/a&gt; to bridge wifi (tethering from phone) to ethernet (to wifi-router)&lt;/p&gt;</content><author><name></name></author><category term="logbook" /><summary type="html">Week 9 - Mar 21</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Logbook for February 21</title><link href="https://castorfou.github.io/guillaume_blog/blog/logbook-Februrary.html" rel="alternate" type="text/html" title="Logbook for February 21" /><published>2021-02-26T00:00:00-06:00</published><updated>2021-02-26T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/logbook-Februrary</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/logbook-Februrary.html">&lt;p&gt;This is a test. I will try to keep words on a monthly (this page), weekly (per heading), daily basis. Just some short entries with possibly some links to more detailed materials.&lt;/p&gt;

&lt;h2 id=&quot;week-8---feb-21&quot;&gt;Week 8 - Feb 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 2/22&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To develop knowledge about RL, here is my learning process on a weekly basis.&lt;/p&gt;

&lt;p&gt;Monday &lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tuesday &lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Wednesday &lt;a href=&quot;/guillaume_blog/blog/an-introduction-to-deep-reinforcement-learning.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Friday &lt;a href=&quot;https://github.com/castorfou/datascience-papers&quot;&gt;RL readings&lt;/a&gt;: papers, books, …&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 2/26&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;blog fastpages - setup automated upgrade (instructions from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_fastpages_docs&lt;/code&gt;) v2.1.42&lt;/p&gt;

&lt;p&gt;blog fastpages - display image preview (update of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;RL - understood differences between Q-learning and Sarsa algorithms in &lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;end of step2 part2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p200-220) - eligibility traces, and start of planning vs learning&lt;/p&gt;</content><author><name></name></author><category term="logbook" /><summary type="html">This is a test. I will try to keep words on a monthly (this page), weekly (per heading), daily basis. Just some short entries with possibly some links to more detailed materials.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Practicing: Deep Reinforcement Learning Course by Thomas Simonini</title><link href="https://castorfou.github.io/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html" rel="alternate" type="text/html" title="Practicing: Deep Reinforcement Learning Course by Thomas Simonini" /><published>2021-02-19T00:00:00-06:00</published><updated>2021-02-19T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/Deep%20Reinforcement%20Learning%20Course%20by%20Thomas%20Simonini</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html">&lt;p&gt;A course by &lt;a href=&quot;https://www.simoninithomas.com/&quot;&gt;Thomas Simonini&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://simoninithomas.github.io/deep-rl-course/&quot;&gt;Syllabus (from 2018)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/deep-reinforcement-learning-course/launching-deep-reinforcement-learning-course-v2-0-38fa3c24bcbc&quot;&gt;Course introduction (from 2020)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Everything available in &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course&quot;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I appreciate the effort to update examples, and some 2018 implementations became obsolete. Historical Atari VC2600 games are now Starcraft 2 or minecraft, and news series on building AI for video games in Unity and Unreal Engine..&lt;/p&gt;

&lt;h2 id=&quot;21921---chapter-1---an-introduction-to-deep-reinforcement-learning&quot;&gt;(2/19/21) - &lt;a href=&quot;https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c&quot;&gt;Chapter 1&lt;/a&gt; - An Introduction to Deep Reinforcement Learning?&lt;/h2&gt;

&lt;p&gt;Previous version from &lt;a href=&quot;https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419&quot;&gt;2018: What is Deep Reinforcement Learning?&lt;/a&gt; is quite interesting. With 3 parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What Reinforcement Learning is, and how rewards are the central idea&lt;/li&gt;
  &lt;li&gt;The three approaches of Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;What the “Deep” in Deep Reinforcement Learning means&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*aKYFRoEmmKkybqJOvLt2JQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rewards, long-term future reward, discount rate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*zrzRTXt8rtWF5fX__kZ-yQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Episodic (starting and ending point) vs Continuous (e.g. stock trading) tasks.&lt;/p&gt;

&lt;p&gt;Way of learning: Monte Carlo (MC: rewards collected at the end of an episode) vs Temporal Difference (TD: estimate rewards at each step)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*LLfj11fivpkKZkwQ8uPi3A.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exploration/Exploitation trade off. Will see later different ways to handle that trade-off.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*APLmZ8CVgu0oY3sQBVYIuw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;three-approaches-to-reinforcement-learning&quot;&gt;Three approaches to Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;These are value-based, policy-based, and model-based.&lt;/p&gt;

&lt;h4 id=&quot;value-based&quot;&gt;Value Based&lt;/h4&gt;

&lt;p&gt;In value-based RL, the goal is to optimize the value function &lt;em&gt;V(s)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The value function is a function that tells us the maximum expected future reward the agent will get at each state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*2_JRk-4O523bcOcSy1u31g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;policy-based&quot;&gt;Policy Based&lt;/h4&gt;

&lt;p&gt;In policy-based RL, we want to directly optimize the policy function &lt;em&gt;π(s)&lt;/em&gt; without using a value function.&lt;/p&gt;

&lt;p&gt;The policy is what defines the agent behavior at a given time.&lt;/p&gt;

&lt;p&gt;We have two types of policy:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deterministic: a policy at a given state will always return the same action.&lt;/li&gt;
  &lt;li&gt;Stochastic: output a distribution probability over actions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*fii7Z01laRGateAJDvloAQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;model-based&quot;&gt;Model Based&lt;/h4&gt;

&lt;p&gt;In model-based RL, we model the environment. This means we create a model of the behavior of the environment. Not addressed in this course.&lt;/p&gt;

&lt;h3 id=&quot;deep-reinforcement-learning&quot;&gt;Deep Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;In Q-learning, we keep a table of actions to take for each state (based on reward). This can be huge.&lt;/p&gt;

&lt;p&gt;Deep Learning allows to approximate this Q function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c&quot;&gt;Updated version&lt;/a&gt; from 2020 (and &lt;a href=&quot;https://www.youtube.com/watch?v=q0BiUn5LiBc&quot;&gt;video&lt;/a&gt; version)&lt;/p&gt;

&lt;p&gt;This is a good starting point, well explained.&lt;/p&gt;

&lt;p&gt;Reinforcement Learning is just a &lt;strong&gt;computational approach of learning from action.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A formal definition&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that &lt;strong&gt;learn from the environment&lt;/strong&gt; by &lt;strong&gt;interacting with it&lt;/strong&gt; through trial and error and &lt;strong&gt;receiving rewards&lt;/strong&gt; (positive or negative) &lt;strong&gt;as unique feedback.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some explanations about &lt;strong&gt;observations&lt;/strong&gt; (partial description) vs &lt;strong&gt;states&lt;/strong&gt; (fully observed envt). Only differs in implementation, all theoretical background stays the same.&lt;/p&gt;

&lt;p&gt;Action space where we can distinguish &lt;strong&gt;discrete&lt;/strong&gt; (e.g. fire, up) actions from &lt;strong&gt;continuous&lt;/strong&gt; (e.g. turn 23deg) ones.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt; part is the same as the one from 2018. With cheese, mouse, maze example.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Episodic&lt;/strong&gt; and &lt;strong&gt;continuous&lt;/strong&gt; tasks part is the same as the one from 2018.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exploration/Exploitation trade-off&lt;/strong&gt; is explained the same way + an additional example taken from &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse-fa18/&quot;&gt;berkley - CS 294-112&lt;/a&gt; - Deep Reinforcement Learning course. I want to learn more about this course!&lt;/p&gt;

&lt;p&gt;About &lt;strong&gt;solving RL problems&lt;/strong&gt;, it is now presented as 2 main approaches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;policy-based&lt;/strong&gt; methods&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;value-based&lt;/strong&gt; methods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*Vujmmyswrg2wIjmpvSUBfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And bedore to explain that, nice presentation of what is a &lt;strong&gt;policy $\pi$&lt;/strong&gt;. Solving RL problem is to find that optimal policy: directly with policy-based method, indirectly (through value function) with value-based method.&lt;/p&gt;

&lt;p&gt;There is an explanation about different types of policy: &lt;strong&gt;deterministic&lt;/strong&gt; and &lt;strong&gt;stochastic&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;And that we use deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based). Thomas suggests to go further with deep learning with MIT 6.S191, which is the &lt;a href=&quot;https://castorfou.github.io/guillaume_blog/deep%20learning/mit/tensorflow/2021/02/05/learning-MIT-6.S191-2021.html&quot;&gt;one&lt;/a&gt; (version 2021) I follow these days.&lt;/p&gt;

&lt;h2 id=&quot;21921---chapter-2---part-1---q-learning-lets-create-an-autonomous-taxi&quot;&gt;(2/19/21) - &lt;a href=&quot;https://thomassimonini.medium.com/q-learning-lets-create-an-autonomous-taxi-part-1-2-3e8f5e764358&quot;&gt;Chapter 2 - part 1&lt;/a&gt; - Q-Learning, let’s create an autonomous Taxi&lt;/h2&gt;

&lt;p&gt;And in &lt;a href=&quot;https://www.youtube.com/watch?v=230bR2DrbdE&amp;amp;feature=emb_logo&quot;&gt;video&lt;/a&gt; (I like to read + watch the video at the same time)&lt;/p&gt;

&lt;p&gt;Here in Step 2 we focus on a value-based method: Q-learning. And what is seen in part 1 and 2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*2yYWVAXJh4FI2lpsL0ajwQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;value-based-method&quot;&gt;Value-based method&lt;/h4&gt;

&lt;p&gt;Remember what we mean in value-based method&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*jfUUaZuHUa1h61oD6O18KA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;you don’t train your policy, you define a simple function such as greedy function to select the best association State-Action, so the best action.&lt;/p&gt;

&lt;h4 id=&quot;bellman-equation&quot;&gt;&lt;strong&gt;Bellman equation&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;each value as the sum of the expected return, &lt;strong&gt;which is a long process.&lt;/strong&gt; This is equivalent &lt;strong&gt;to the sum of immediate reward + the discounted value of the state that follows.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*FMjoVEELvz0oKcIfmcvGPQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;monte-carlo-vs-temporal-difference&quot;&gt;Monte Carlo vs Temporal Difference&lt;/h4&gt;

&lt;p&gt;And then an explanation about 2 types of method to learn a policy or a value-function:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Monte Carlo&lt;/em&gt;: learning at the end of the episode. With &lt;em&gt;Monte Carlo&lt;/em&gt;, we update the value function from a complete episode and so we &lt;strong&gt;use the actual accurate discounted return of this episode.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;TD learning&lt;/em&gt;: learning at each step. With &lt;em&gt;TD learning&lt;/em&gt;, we update the value function from a step, so we replace Gt that we don’t have with &lt;strong&gt;an estimated return called TD target.&lt;/strong&gt; (chich is the immediate reward + the discounted value of the next state)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*c8nfnXRu8n1h78bWPEK5vg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was not clear to me that these methods could be used for policy-based approach. It is now!&lt;/p&gt;

&lt;h2 id=&quot;22421---chapter-2---part-2---q-learning-lets-create-an-autonomous-taxi&quot;&gt;(2/24/21) - &lt;a href=&quot;https://thomassimonini.medium.com/q-learning-lets-create-an-autonomous-taxi-part-2-2-8cbafa19d7f5&quot;&gt;Chapter 2 - part 2&lt;/a&gt; - Q-Learning, let’s create an autonomous Taxi&lt;/h2&gt;

&lt;p&gt;But the video is not yet available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is Q-Learning?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Q-Learning is an &lt;strong&gt;off-policy value-based method that uses a TD approach to train its action-value function:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;“Off-policy”&lt;/em&gt;: we’ll talk about that at the end of this chapter.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Value-based method”&lt;/em&gt;: it means that it finds its optimal policy indirectly by training a  value-function or action-value function that will tell us what’s &lt;strong&gt;the value of each state or each state-action pair.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Uses a TD approach”&lt;/em&gt;: &lt;strong&gt;updates its action-value function at each step.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Q stands for quality (quality of action). After training we’ll get the optimal Q-function.&lt;/p&gt;

&lt;p&gt;When choosing an action, we have to balance between exploration and exploitation with &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; - greedy:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*AYz65tJDERsWTg2DGEJ35g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But at beginning Q table is not trained yet so we have to increase exploitation. It is done with some decreasing &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*1J2lJN48gdjeuoRBqsO_CA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Q-learning algorithm is a 4-step process:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;step1: Q-Table init&lt;/li&gt;
  &lt;li&gt;step2: Choose action (&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; - greedy strategy)&lt;/li&gt;
  &lt;li&gt;step3: Perform action A&lt;sub&gt;t&lt;/sub&gt; and get R&lt;sub&gt;t+1&lt;/sub&gt; and S&lt;sub&gt;t+1&lt;/sub&gt;&lt;/li&gt;
  &lt;li&gt;step4: Update Q(S&lt;sub&gt;t&lt;/sub&gt;, A&lt;sub&gt;t&lt;/sub&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*teZ5KRfvYjMKZnmhaWTUXg.png&quot; alt=&quot;Update Q(S&amp;lt;sub&amp;gt;t&amp;lt;/sub&amp;gt;, A&amp;lt;sub&amp;gt;t&amp;lt;/sub&amp;gt;)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Why it is called &lt;strong&gt;off-policy&lt;/strong&gt;? Because we don’t have the same logic to select action (&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; - greedy) and update Q (greedy).&lt;/p&gt;

&lt;p&gt;With &lt;em&gt;On-policy:&lt;/em&gt; we use the &lt;strong&gt;same policy for acting and updating.&lt;/strong&gt; Sarsa is such an algorithm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*gVl6V-wbX_hOoNQATx081Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nice and simple manual example with mouse, cheese in a maze. We run Q-learning and make all calculation by hands.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/500/1*GMuThIF7aNj-V_d6hTRN8A.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;todo&gt; &lt;/todo&gt;

&lt;div class=&quot;alert alert-info&quot;&gt;implement with numpy+gym this algorithm should be a nice exercise.&lt;/div&gt;

&lt;p&gt;There is an exercise to implement a taxi, within this &lt;a href=&quot;https://colab.research.google.com/gist/simoninithomas/466c81aa1c2a07dd14793240c6d033c5/q-learning-with-taxi-v3.ipynb#scrollTo=20tSdDbxxK_H&quot;&gt;notebook&lt;/a&gt; at colab google. Taxi V3 is an env from opengym.&lt;/p&gt;

&lt;h2 id=&quot;3321---back-to-2018---chapter-3---deep-q-learning-with-doom&quot;&gt;(3/3/21) - back to 2018 - Chapter 3 - Deep Q-learning with Doom&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8&quot;&gt;Article&lt;/a&gt;, &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb&quot;&gt;Notebook&lt;/a&gt;, &lt;a href=&quot;https://youtu.be/gCJyVX98KJ4&quot;&gt;Video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’ll create an agent that learns to play Doom. Doom is a big  environment with a gigantic state space (millions of different states).  Creating and updating a Q-table for that environment would not be  efficient at all.&lt;/p&gt;

&lt;p&gt;The best idea in this case is to create a &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/&quot;&gt;neural network&lt;/a&gt; that will approximate, given a state, the different Q-values for each action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*LglEewHrVsuEGpBun8_KTg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Addresses pb of temporal limitation: get multiple frames to have sense of motion.&lt;/p&gt;

&lt;p&gt;Video is nice because it goes from start and follows closely all steps.&lt;/p&gt;

&lt;p&gt;I wil try to implement in my own by creating an environment and running under a clone of Deep_reinforcement_learning_Course &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course&quot;&gt;Thomas’s repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here at &lt;a href=&quot;https://github.com/castorfou/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb&quot;&gt;Deep Q learning with Doom.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I had to switch to tensorflow-gpu 1.13. Manage some cuda memory issue. But then was able to run it.&lt;/p&gt;

&lt;p&gt;However as Thomas says, I should do it step by step on my own.&lt;/p&gt;

&lt;h2 id=&quot;31021---chapter-4-improvements-in-deep-q-learning-v1&quot;&gt;(3/10/21) - Chapter 4: Improvements in Deep Q Learning V1&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682&quot;&gt;Article&lt;/a&gt;, &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/Dueling%20Deep%20Q%20Learning%20with%20Doom%20(%2B%20double%20DQNs%20and%20Prioritized%20Experience%20Replay).ipynb&quot;&gt;Notebook&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=-Ynjw0Vl3i4&amp;amp;feature=emb_title&quot;&gt;Video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;four strategies that improve — dramatically — the training and the results of our DQN agents:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fixed Q-targets&lt;/li&gt;
  &lt;li&gt;double DQNs&lt;/li&gt;
  &lt;li&gt;dueling DQN (aka DDQN)&lt;/li&gt;
  &lt;li&gt;Prioritized Experience Replay (aka PER)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;fixed Q-targets&lt;/strong&gt; to avoid chasing a moving target&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using a separate network with a fixed parameter (let’s call it w-) for estimating the TD target.&lt;/li&gt;
  &lt;li&gt;At every &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\Tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.68333em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathrm&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; step, we copy the parameters from our DQN network to update the target network.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*D9i0I2EO7LKL2aAb2HLfTg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/&quot;&gt;Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Implementation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Implementing fixed q-targets is pretty straightforward:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;First, we create two networks (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DQNetwork&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TargetNetwork&lt;/code&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then, we create a function that will take our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DQNetwork&lt;/code&gt; parameters and copy them to our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TargetNetwork&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Finally, during the training, we calculate the TD target using our target network. We update the target network with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DQNetwork&lt;/code&gt; every &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\Tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.68333em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathrm&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; step (&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\Tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.68333em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathrm&quot;&gt;T&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is an hyper-parameter that we define).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;double DQNs&lt;/strong&gt; to handle overestimating of Q-values (at the beginning of training, taking the maximum q value (which is noisy) as the best action to take can lead to false positives)&lt;/p&gt;

&lt;p&gt;we move from this TD target logic&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*KsQ46R8zyTQlKGv91xi6ww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;to the use of 2 networks&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use our DQN network to select what is the best action to take for the next state (the action with the highest Q value).&lt;/li&gt;
  &lt;li&gt;use our target network to calculate the target Q value of taking that action at the next state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*g5l4q162gDRZAAsFWtX7Nw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Implementation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*oyGR6gJ4WyqeKOfq0Cd8iQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dueling DQN (aka DDQN)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;based on this paper &lt;a href=&quot;https://arxiv.org/pdf/1511.06581.pdf&quot;&gt;Dueling Network Architectures for Deep Reinforcement Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With DDQN, we want to separate the estimator of these two elements, using two new streams:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;one that estimates the &lt;strong&gt;state value V(s)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;one that estimates the &lt;strong&gt;advantage for each action A(s,a)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*FkHqwA2eSGixdS-3dvVoMA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and this can be combined with &lt;strong&gt;Prioritized experience replay&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is nicely explained in this &lt;a href=&quot;https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/&quot;&gt;article&lt;/a&gt;. DDQN explanation is clearer than Thomas’.&lt;/p&gt;

&lt;p&gt;The key here is to deal efficiently with experiences. When treating all samples the same,  we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay (PER) is one strategy that tries to leverage this fact by changing the sampling distribution.&lt;/p&gt;

&lt;p&gt;I guess there are several options to manage this prioritization (we would prefer transitions that do not fit well to our current estimate of Q function). And a key aspect is the performance of this selection. One implementation is SumTree.&lt;/p&gt;

&lt;p&gt;I have to see full implementation in the notebook to fully understand the logic.&lt;/p&gt;

&lt;p&gt;About the &lt;strong&gt;video&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Thomas has insisted about the importance to master these architecture (DQN then DDQN, etc) before going further with state of the art architectures (Policy Gradient, PPO…)&lt;/p&gt;

&lt;p&gt;Approach in videos is now different. In previous videos it was about explaining articles. Now it is more turned to implementation details based on notebooks.&lt;/p&gt;

&lt;p&gt;Thomas has given a reference to Arthur Juliani who is a senior ML engineer at &lt;a href=&quot;https://unity.com/fr/products/machine-learning-agents&quot;&gt;Unity&lt;/a&gt;. I would like to browse though this reference and see what can be done.&lt;/p&gt;

&lt;p&gt;Should follow video and run/update notebook in //.&lt;/p&gt;

&lt;h2 id=&quot;31721---chapter-5-policy-gradients-v1&quot;&gt;(3/17/21) - Chapter 5: Policy Gradients V1&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/&quot;&gt;Article&lt;/a&gt;, &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/Policy%20Gradients&quot;&gt;Notebook&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=wLTQRuizVyE&quot;&gt;Video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In policy-based methods, instead of learning a value function that tells us what is the expected sum of rewards given a state and an action, we  learn directly the policy function that maps state to action (select  actions without using a value function).&lt;/p&gt;

&lt;p&gt;3 main advantages to use Policy Gradients vs Q learning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;convergence - have better convergence properties&lt;/li&gt;
  &lt;li&gt;effective in high dimension, or with continuous actions&lt;/li&gt;
  &lt;li&gt;stochastic policy - no need for exploration,/exploitation tradeoff&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But can be longer to train.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy search&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We can dfine our policy as the probability distribution of actions (for a given state)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/0*354cfoILK19WFTWa.&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And how good is this policy? Measured with J(&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/0*PfUAJaIGoEsvfbCG.&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We must find &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; to maximize J(&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;). How?&lt;/p&gt;

&lt;p&gt;2 steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Measure the quality of a π (policy) with a policy score function J(θ)&lt;/li&gt;
  &lt;li&gt;Use policy gradient ascent to find the best parameter θ that improves our π.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;the Policy Score function J(θ)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;3 ways (maybe more)&lt;/p&gt;

&lt;p&gt;Calculate the mean of the return from the first time step (G1). This is the cumulative discounted reward for the entire episode.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*tP4l4IrIG3aMLTrMt-1-HA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In a continuous environment, we can use the average value, because we can’t rely on a specific start state. Each state value is now weighted (because some happen more than others) by  the probability of the occurrence of the respected state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*S-XLkrvPuVUqLrFW1hmIMg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Third, we can use the average reward per time step. The idea here is that we want to get the most reward per time step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*3SejRRby6vAnThZ8c2UaQg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy gradient ascent&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;because we want to maximize our Policy score function&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/0*oh-lF13hYWt2Bd6V.&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The solution will be to use the Policy Gradient Theorem. This provides  an analytic expression for the gradient ∇ of J(θ) (performance) with  respect to policy θ that does not involve the differentiation of the  state distribution. (using &lt;a href=&quot;http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/&quot;&gt;likelihood ratio trick&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*iKhO5anOAfc3oqJOM2i_8A.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It gives&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*zjEh737KfmDUzNECjW4e4w.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;R(&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\tau&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.1132em;&quot;&gt;τ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;) is like a scalar value score.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As with the previous section, this is good to watch the video at the same time.&lt;/p&gt;

&lt;p&gt;And now this is the implementation in&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/castorfou/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Doom%20Deathmatch/Doom-deathmatch%20REINFORCE%20Monte%20Carlo%20Policy%20gradients.ipynb&quot;&gt;doom deathmatch notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/castorfou/Deep_reinforcement_learning_Course/raw/master/Policy%20Gradients/Doom%20Deathmatch/assets/doomPG1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;as with Pong, we &lt;a href=&quot;https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/&quot;&gt;stack&lt;/a&gt; frames to understand dynamic with deque.&lt;/p&gt;

&lt;p&gt;Even with GPU growth setup, I run an error after the 1st epoch.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;==========================================
Epoch:  1 / 5000
-----------
Number of training episodes: 15
Total reward: 7.0
Mean Reward of that batch 0.4666666666666667
Average Reward of all training: 0.4666666666666667
Max reward for a batch so far: 7.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ResourceExhaustedError: OOM when allocating tensor with shape[5030,32,24,39] and &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 &lt;span class=&quot;o&quot;&gt;[[]]&lt;/span&gt;
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;current allocation info.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have to reduce batch size (to 1000) to make it work.&lt;/p&gt;

&lt;p&gt;And I can monitor gpu memory consumption with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;watch nvidia-smi&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/watch_nvidia_smi.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;or we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpustat -i 2&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[0] Quadro RTX 4000&lt;/td&gt;
      &lt;td&gt;59’C,  &lt;strong&gt;34 %&lt;/strong&gt;,   39 W&lt;/td&gt;
      &lt;td&gt;7819 /  7982 MB&lt;/td&gt;
      &lt;td&gt;explore(6729M) gdm(162M) explore(388M) explore(282M) explore(86M) explore(89M) explore(3M)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;31921---chapter-6-advantage-actor-critic-a2c-and-asynchronous-advantage-actor-critic-a3c-v1&quot;&gt;(3/19/21) - Chapter 6: Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) V1&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/&quot;&gt;Article&lt;/a&gt;, &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/A2C%20with%20Sonic%20the%20Hedgehog&quot;&gt;Notebook&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=GCfUdkCL7FQ&quot;&gt;Video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;“hybrid method”: &lt;strong&gt;Actor Critic&lt;/strong&gt;. We’ll using two neural networks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;an &lt;strong&gt;Actor&lt;/strong&gt; that controls how our agent behaves (policy-based)&lt;/li&gt;
  &lt;li&gt;a &lt;strong&gt;Critic&lt;/strong&gt; that measures how good the action taken is (value-based)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*e1N-YzQmJt-5KwUkdUvAHg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Actor is using a &lt;strong&gt;policy&lt;/strong&gt; function 
&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi(s, a, \theta)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;mpunct&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;mpunct&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
Critic is using a &lt;strong&gt;value&lt;/strong&gt; function&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;^&lt;/mo&gt;&lt;/mover&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\widehat{q}(s,a,w)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord accent&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.67056em;&quot;&gt;&lt;span style=&quot;top:-3em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;q&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;svg-align&quot; style=&quot;width:calc(100% - 0.16668em);margin-left:0.16668em;top:-3.43056em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;height:0.24em;&quot;&gt;&lt;svg width='100%' height='0.24em' viewBox='0 0 1062 239' preserveAspectRatio='none'&gt;&lt;path d='M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z'/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.19444em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;mpunct&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;mpunct&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02691em;&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
Which means 2 sets of weights to be optimized separately &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\theta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.02778em;&quot;&gt;θ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and w.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*KlX2-kNXRYLAYpdnI8VPiA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can use advantage function to stabilize learning:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*SvSFYWx5-u5zf38baqBgyQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;two-different-strategies-asynchronous-or-synchronous&quot;&gt;Two different strategies: Asynchronous or Synchronous&lt;/h4&gt;

&lt;p&gt;We have two different strategies to implement an Actor Critic agent:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A2C (aka Advantage Actor Critic)&lt;/li&gt;
  &lt;li&gt;A3C (aka Asynchronous Advantage Actor Critic)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here we focus on A2C.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(3/22/21) - Implementation and video&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It is a little bit confusing. I won’t run it. I would have liked a more pregressive approach and to understand all steps Thomas did to get to that final implementation.&lt;/p&gt;

&lt;h2 id=&quot;32421---chapter-7-proximal-policy-optimization-ppo-v1&quot;&gt;(3/24/21) - Chapter 7: Proximal Policy Optimization PPO V1&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e&quot;&gt;Article&lt;/a&gt;, &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/PPO%20with%20Sonic%20the%20Hedgehog&quot;&gt;Notebook&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The central idea of Proximal Policy Optimization is to avoid having too large policy update. (we use a ratio that will tells us the difference between our new and old policy and clip this ratio from 0.8 to 1.2)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clipped Surrogate Objective Function&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/0*Ab-_UqHukYN6syxS&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will penalize changes that lead to a ratio that will away from 1 (in the paper ratio can only vary from 0.8 to 1.2). &lt;strong&gt;By doing that we’ll ensure that not having too large policy update because the new policy can’t be too different from the older one.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;2 implementations are known TRPO (Trust Region Policy Optimization) and PPO clip. TRPO being complex and costly, we focus on PPO:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/0*Dk8XFEOzI7yaSfLE&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the final loss will be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*T0D50EPz-oqGDn55uHv9IA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Now the &lt;a href=&quot;https://github.com/castorfou/Deep_reinforcement_learning_Course/tree/master/PPO%20with%20Sonic%20the%20Hedgehog&quot;&gt;implementation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;By looking at the implementation, I ran into &lt;a href=&quot;https://github.com/DLR-RM/stable-baselines3&quot;&gt;Stable baselines3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is a major update of Stable Baselines based on pytorch. It seems interesting!&lt;/p&gt;

&lt;p&gt;I like this comment from Stable Baselines3 in the &lt;a href=&quot;https://araffin.github.io/post/sb3/&quot;&gt;v1.0 blog post&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Deep reinforcement learning (RL) research has grown rapidly in recent years, yet results are often  &lt;a href=&quot;https://arxiv.org/abs/1709.06560&quot;&gt;difficult to reproduce&lt;/a&gt;. A major challenge is that small implementation details can have a substantial effect on performance – often greater than the  &lt;a href=&quot;https://iclr.cc/virtual_2020/poster_r1etN1rtPB.html&quot;&gt;difference between algorithms&lt;/a&gt;. It is particularly important that implementations used as experimental &lt;em&gt;baselines&lt;/em&gt; are reliable; otherwise, novel algorithms compared to weak baselines lead to inflated estimates of performance improvements.&lt;/p&gt;

  &lt;p&gt;To help with this problem, we present Stable-Baselines3 (SB3), an  open-source framework implementing seven commonly used model-free deep  RL algorithms, relying on the  &lt;a href=&quot;https://github.com/openai/gym&quot;&gt;OpenAI Gym interface&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I will create a new blog entry about Stable Baselines3.&lt;/p&gt;

&lt;p&gt;as for previous notebook, I need to purchase Sonic2-3 to make it worked. Not for now maybe later.&lt;/p&gt;</content><author><name></name></author><category term="reinforcement learning" /><summary type="html">A course by Thomas Simonini</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/RL.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/RL.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Conda and jupyter tips</title><link href="https://castorfou.github.io/guillaume_blog/blog/conda-and-jupyter-tips.html" rel="alternate" type="text/html" title="Conda and jupyter tips" /><published>2021-02-16T00:00:00-06:00</published><updated>2021-02-16T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/conda-and-jupyter-tips</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/conda-and-jupyter-tips.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-16-conda-and-jupyter-tips.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Keeping-track-of-python-environments&quot;&gt;Keeping track of python environments&lt;a class=&quot;anchor-link&quot; href=&quot;#Keeping-track-of-python-environments&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I manage all my python environments with &lt;code&gt;conda&lt;/code&gt; from &lt;code&gt;miniconda&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;manual-way&quot;&gt;manual way&lt;a class=&quot;anchor-link&quot; href=&quot;#manual-way&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;However I don't have a strong process to keep track of my environment specifications. Usually I manually create an &lt;code&gt;env.txt&lt;/code&gt; file under my projects. Keeping all commands I have used to create that environment.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/env&lt;span class=&quot;se&quot;&gt;\ \ &lt;/span&gt;mit_6S191.txt
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;env_name: mit_6S191
libraries: python 3.7, tensorflow 2


Installation commands:
conda create -n mit_6S191 python=3.7
conda activate mit_6S191

conda install tensorflow tensorflow-gpu
conda install -c conda-forge jupyter_contrib_nbextensions
conda install matplotlib numpy opencv
conda install -c pytorch torchvision
conda install nb_conda
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;What happens if I add packages in that environment. Or want to use that environment in another project. I have to remember the link between env name and project name.&lt;/p&gt;
&lt;p&gt;That is not robust.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;yml-way&quot;&gt;yml way&lt;a class=&quot;anchor-link&quot; href=&quot;#yml-way&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Keeping a yml file could be a solution to keep track of environment specifications. It doesn't answer to my last concern though (linking env name and project name)&lt;/p&gt;
&lt;p&gt;But there is a limitation linked with channels.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;conda env &lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; --from-history
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;name: fastai
channels:
  - defaults
dependencies:
  - python=3.8
  - fastai
  - jupyter
  - jupyter_contrib_nbextensions
  - fastbook
prefix: /home/explore/miniconda3/envs/fastai
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In that example, &lt;code&gt;fastai&lt;/code&gt; package should come from &lt;code&gt;fastai&lt;/code&gt; channel but conda doesn't keep that information.&lt;/p&gt;
&lt;p&gt;Using&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda install -n my_env rdkit::rdkit
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;could be an option.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;automate-yml-way&quot;&gt;automate yml way&lt;a class=&quot;anchor-link&quot; href=&quot;#automate-yml-way&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Since conda keeps active environment in env variable &lt;code&gt;CONDA_DEFAULT_ENV&lt;/code&gt;, we can automatically create up-to-date yml file.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONDA_DEFAULT_ENV&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;fastai
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;conda env &lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; --from-history &amp;gt; ~/temp/env_&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONDA_DEFAULT_ENV&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;.yml
&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;ls ~/temp/env_&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONDA_DEFAULT_ENV&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;.yml
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;/home/explore/temp/env_fastai.yml
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;But for it to be usable, I will have to install package using the &lt;code&gt;&amp;lt;channel&amp;gt;::&amp;lt;package&amp;gt;&lt;/code&gt; way.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/create_yml.sh
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;#!/bin/bash
conda env export --from-history &amp;gt; env_`echo $CONDA_DEFAULT_ENV`.yml
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Conda-commands&quot;&gt;Conda commands&lt;a class=&quot;anchor-link&quot; href=&quot;#Conda-commands&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When managing conda environments, I very often fall on this documentation page which is simply great: &lt;a href=&quot;https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html&quot;&gt;https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Next time I visit this page, I will enter entries here to track my common commands.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Jupyter-installation&quot;&gt;Jupyter installation&lt;a class=&quot;anchor-link&quot; href=&quot;#Jupyter-installation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Jupyter-extensions&quot;&gt;Jupyter extensions&lt;a class=&quot;anchor-link&quot; href=&quot;#Jupyter-extensions&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I have already explained how to install jupyter extensions and the one I use.
&lt;a href=&quot;/guillaume_blog/fastai/jupyter/fastbook/2020/09/24/fastai-book.html#update-jupyter-to-include-extensions-(toc,-...&quot;&gt;update jupyter to include extensions&lt;/a&gt;)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;nb_conda&quot;&gt;nb_conda&lt;a class=&quot;anchor-link&quot; href=&quot;#nb_conda&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This is usefull to switch from environment to another without having to stop/restart jupyter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/nb_conda.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="conda" /><category term="jupyter" /><summary type="html"></summary></entry><entry><title type="html">Learning: Collège de France - Représentations parcimonieuses</title><link href="https://castorfou.github.io/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html" rel="alternate" type="text/html" title="Learning: Collège de France - Représentations parcimonieuses" /><published>2021-02-10T00:00:00-06:00</published><updated>2021-02-10T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/college-de-france-representations-parcimonieuses</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html">&lt;p&gt;Un exposé en 8 cours au collège de France de Stéphane Mallat sur les &lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/course-2020-2021.htm&quot;&gt;représentations parcimonieuses - 2021&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cela donne envie d’aller voir ses autres cours:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/course-2017-2018.htm&quot;&gt;2018&lt;/a&gt;: L’apprentissage face à la malédiction de la grande dimension&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/course-2018-2019.htm&quot;&gt;2019&lt;/a&gt;: L’apprentissage par réseaux de neurones profonds&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/course-2019-2020.htm&quot;&gt;2020&lt;/a&gt;: Modèles multi-échelles et réseaux de neurones convolutifs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A peu près 16 vidéos de 1h30 par cours. Et des notes de cours en pdf.&lt;/p&gt;

&lt;h2 id=&quot;21521---le-triangle--régularité-approximation-parcimonie--lecture-1&quot;&gt;2/15/21 - Le triangle « Régularité, Approximation, Parcimonie » (lecture 1)&lt;/h2&gt;

&lt;p&gt;C’est l’introduction du cours. J’apprécie les références historiques et philosphiques partant du rasoir d’Ockam. C’est le principe d’économie ou de parcimonie: le beau, le vrai viendrait du simple.&lt;/p&gt;

&lt;p&gt;La 1ere fois que j’entends une référence précise sur l’opposition entre biais (erreur sur modèle) et variance (erreur sur données ou mesures)&lt;/p&gt;

&lt;p&gt;Et une invitation à consulter une &lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/seminar-2018-02-21-11h15.htm&quot;&gt;méthodologie d’analyse de données&lt;/a&gt; par Pierre Courtiol en utilisant Kaggle. L’idée d’une approche simple linéaire pour bien comprendre quelles étapes successives à emprunter pour améliorer son approche. Me semble assez orthogonal à ce que peut proposer Jeremy Howard: commencer tôt, overfitting n’est pas un probleme, pas de early stopping, etc.&lt;/p&gt;

&lt;h2 id=&quot;21021---approximations-linéaires-et-analyse-de-fourier-lecture-2&quot;&gt;2/10/21 - Approximations linéaires et analyse de Fourier (lecture 2)&lt;/h2&gt;

&lt;p&gt;J’ai commencé par ce cours conseillé par Rémi mon pote enseignant chercheur en math. C’est un peu le grand écart avec des méthodes d’enseignement anglo-saxonnes mais ça fait du bien. C’est finalement plus proche de ce que j’ai connu dans ma formation initiale.&lt;/p&gt;

&lt;p&gt;S.Mallat présente les équivalences (sous certaines conditions) entre&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Régularité&lt;/li&gt;
  &lt;li&gt;Approximation en basse dimension&lt;/li&gt;
  &lt;li&gt;et représentation parcimonieuse&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;dans le cadre des approximations linéaires. Il parle des 2 mondes: traitement du signal et analyse de la donnée. Je suis moins intéressé par le 1er monde, mais j’apprécie la piqure de rappel. Je ne me rappelais pas du tout l’importance de l’analyse de Fourier et la construction des bases de L[0,1] par exemple.&lt;/p&gt;

&lt;p&gt;Et il revient sur les singularités, beaucoup d’informations sont portées par les singularités (par exemple les frontières dans une image)&lt;/p&gt;

&lt;p&gt;Je crois bien que je vais me faire toute la session, et sans doute les autres années.&lt;/p&gt;

&lt;h2 id=&quot;22321---grande-dimension-et-composantes-principales-lecture-3&quot;&gt;2/23/21 - Grande dimension et composantes principales (lecture 3)&lt;/h2&gt;

&lt;p&gt;Dans ce cadre linéaire grande dimension, quelle meilleure base - approche PCA et base Karhunen-Loeve.&lt;/p&gt;

&lt;p&gt;Quid quand on passe en non linéaire.&lt;/p&gt;

&lt;p&gt;Réseau neurone à 1 couche cachée, théoreme de representation universel.&lt;/p&gt;

&lt;p&gt;Retour sur les bases de L²[0,1] qui sont les bases de Fourier en variables complexes.&lt;/p&gt;

&lt;p&gt;Pour un passage en dimension q, on remplace n par (n1, …, nq) et la multiplication n*u par le produit scalaire &amp;lt;n, u&amp;gt;.&lt;/p&gt;

&lt;p&gt;En travaillant sur les équivalences du triangle, il montre pourquoi on est très limité en approximation lineaire quand la dimension augmente.&lt;/p&gt;

&lt;p&gt;En approximation lineaire, il suffit de prendre les 1ers vecteurs (se limiter à une dimension q) (en base de fourier par exemple) pour avoir une assez bonne approximation. Dans des signaux plus perturbés (avec des singularités) on perd plus d’énergie: il faudrait échantilloner plus fin dans ces zones de singularités et si on dispose d’une base orthonormée il s’agirait non plus de prendre les q 1ers vecteurs mais de prendre ceux d’intéret.&lt;/p&gt;

&lt;h2 id=&quot;3221---approximations-non-linéaires-et-réseaux-de-neurones-lecture-4&quot;&gt;3/2/21 - Approximations non linéaires et réseaux de neurones (lecture 4)&lt;/h2&gt;

&lt;p&gt;Le triangle (approximation basse dimensions, représentation parcimonieuse, régularité) d’un point de vue non linéaire.&lt;/p&gt;

&lt;p&gt;Ici plutôt qu’approximer un signal en prenant les M 1ers coefficients de Fourier (basses dimensions), on va prendre M coefficients mais dépendamment de x. C’est ici qu’on introduit la non-linéarité. L’erreur est alors la queue de distribution des coefficients ordonnés. On veut que l’énergie des plus petits coefficients soit négligeable.&lt;/p&gt;

&lt;p&gt;Pas facile d’obtenir cet ordre, on cherche une façon de limiter les coefficients non ordonnés nous donnant une représentation parcimonieuse. En utilisant la nome l&lt;sub&gt;$\alpha$&lt;/sub&gt; avec &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; petit (inférieur à 2 et proche de 0), on introduit cette décroissance mais cette fois-ci sur les coefficients non ordonnés.&lt;/p&gt;

&lt;p&gt;Intéressant d’avoir des normes convexes, et dans ce cas on ne peut prendre que &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;=1. C’est pour ça qu’on voit apparaître partout les normes l&lt;sub&gt;1&lt;/sub&gt; dans les algorithmes d’apprentissage (norme convexe garantissant une forme de sparsité).&lt;/p&gt;

&lt;p&gt;On passe aux réseaux de neurones à 1 couche cachée. Et on va basculer dans les notations de x(u) à f(x)., avec x &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; [0, 1]&lt;sup&gt;d&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/rep_par_lecture4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ici on projette f dans l’espace engendré par ces vecteurs { &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\rho&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ρ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(x.w&lt;sub&gt;m&lt;/sub&gt;+b&lt;sub&gt;m&lt;/sub&gt;) }&lt;sub&gt;n&amp;lt;=M&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;On peut facilement calculer l’erreur quadratique comme l’intégrale sur les x &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; [0, 1]&lt;sup&gt;d&lt;/sup&gt; de la norme l² ( f(x)-f&lt;sub&gt;tilde&lt;/sub&gt;(x) ) et il y a un belle démonstration qui est le &lt;strong&gt;théorème d’approximation universelle&lt;/strong&gt; (démontrée entre 1988 et 1992) qui montre que l’erreur tend vers 0 quand M tend vers l’infini.&lt;/p&gt;

&lt;p&gt;La démonstration avec &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\rho&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ρ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; = e&lt;sup&gt;ia&lt;/sup&gt; revient à une décomposition d’en Fourier. Et pour d’autres non régularité comme reLu ou sigmoid, il s’agit d’un changement de base.&lt;/p&gt;

&lt;p&gt;Et là on arrive à la malédiction de la dimensionnalité car quand d est grand (disons 1M), les coefficients baissent à une faible vitesse. Que faut-il faire pour battre cette malédiction?&lt;/p&gt;

&lt;p&gt;Baron en 1993 introduit une hypothèse de regularité qui permet de borner l’erreur par un terme qui ne dépend pas de la dimension. C’est donc gagné sauf que l’hypothèse de régularité n’est généralement pas valide dans les cas qui nous intéressent.&lt;/p&gt;

&lt;p&gt;Stéphane Mallat, de façon brillante mais est-ce étonnant, explique pourquoi l’approche des mathématiciens est une impasse et pourquoi ce qu’on cherche à faire se ramène à un problème bayésien. Car les problèmes qui nous intéressent (par exemple la classification d’objets, ne va solliciter qu’un minuscule espace (même si de grande dimension) parmi toutes les images possibles). On va donc chercher à caractériser x pour chaque y (classe). (revoir vidéo entre 49’ et 1h03)&lt;/p&gt;

&lt;p&gt;L’enjeu est de caractériser le support qui est beaucoup plus concentré que [0,1]&lt;sup&gt;d&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Donc on va retravailler sur les approximations non linéaires de x, le signal lui-même (et non plus f), et d’essayer de comprendre pourquoi on peut faire beaucoup mieux que la transformée de Fourier et quelle genre de bases vont nous permettre de faire bcp mieux. Une des applications va être la compression, qui va nous amener à étudier la théorie de l’information et la théorie de l’information c’est exactement la théorie probabiliste qui explique ces phénomènes de concentration et les mesure avec l’entropie.&lt;/p&gt;

&lt;p&gt;Introduction des bases d’ondelettes qui vont permettre de représenter les singularités locales. Les ondelettes sont à la fois localisées (paramètre v) et dilatées (paramètre s). Il faudra à partir de ces ondelettes construire des bases orthogonales pour arriver à des approximations basses dimensions (et garder les grands coefficients)&lt;/p&gt;

&lt;p&gt;On introduit la notion de régularité locale exprimée avec lipchitz &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Avec &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &amp;lt;1 pour exprimer les singularités.&lt;/p&gt;

&lt;h2 id=&quot;3921---ondelettes-et-échantillonnage-lecture-5&quot;&gt;3/9/21 - Ondelettes et échantillonnage (lecture 5)&lt;/h2&gt;

&lt;p&gt;On était resté sur une représentation de signaux qui ne présentent pas de régularité uniforme mais qui présentent des singularités que nous voulons capter, ces singularités étant porteuses d’informations importantes (par exemple les contours dans une image). Ces singularités n’étant pas très nombreuses, on peut toujours parler de &lt;strong&gt;régularité locale&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;On va donc utiliser des ondelettes pour décomposer ces signaux, d’où la notion de &lt;strong&gt;représentation parcimonieuse&lt;/strong&gt;, exprimée sur la base d’ondelettes orthonormales. Et enfin en en sélectionnant un petit nombre nous revenons sur nos &lt;strong&gt;approximations en basse dimension&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Le produit scalaire du signal x(u) par l’ondelette &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ψ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\psi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8888799999999999em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;ψ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;sub&gt;v,s&lt;/sub&gt; revient à un produit de convolution de x par l’ondelette conjuguée. Ca veut dire que sur les points de singularités les produits scalaires vont être maximisés.&lt;/p&gt;

&lt;p&gt;Stéphane Mallat passe un long moment pour nous amener à la construction de ces bases d’ondelettes orthonormales. Il part des bases de Haar puis de Shannon et arrive à une construction plus récente par Yves Meyer en 1986.&lt;/p&gt;

&lt;h2 id=&quot;31621---multi-résolutions-lecture-6&quot;&gt;3/16/21 - Multi-résolutions (lecture 6)&lt;/h2&gt;

&lt;p&gt;On a vu la dernière fois qu’on pouvait construire une base d’ondelette le long des indices de dilatations en 2&lt;sup&gt;j&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;On va voir maintenant qu’on peut translater les ondelettes par des facteurs 2&lt;sup&gt;j&lt;/sup&gt;.n.&lt;/p&gt;

&lt;p&gt;Donc quand j est grand, les échelles sont de plus en plus grande. Et j petit va amener un échantillonnage  de plus en plus fin.&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mo fence=&quot;true&quot;&gt;{&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Ψ&lt;/mi&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;msqrt&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msup&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Ψ&lt;/mi&gt;&lt;mrow&gt;&lt;mo fence=&quot;true&quot;&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;msup&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msup&gt;&lt;/mfrac&gt;&lt;mo fence=&quot;true&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo fence=&quot;true&quot;&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;Z&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\left\{ \Psi_{(j,n)}(u)=\frac{1}{\sqrt{2^j}}\Psi \left( \frac{u-2^jn}{2^j} \right) \right\}_{(j, n) \epsilon \Z^2}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:2.676394em;vertical-align:-1.17473em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;minner&quot;&gt;&lt;span class=&quot;minner&quot;&gt;&lt;span class=&quot;mopen delimcenter&quot; style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;delimsizing size3&quot;&gt;{&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;Ψ&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.34480000000000005em;&quot;&gt;&lt;span style=&quot;top:-2.5198em;margin-left:0em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05724em;&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;mpunct mtight&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.3551999999999999em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.32144em;&quot;&gt;&lt;span style=&quot;top:-2.1496679999999997em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord sqrt&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.9603320000000001em;&quot;&gt;&lt;span class=&quot;svg-align&quot; style=&quot;top:-3em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot; style=&quot;padding-left:0.833em;&quot;&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.750664em;&quot;&gt;&lt;span style=&quot;top:-2.9890000000000003em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05724em;&quot;&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-2.920332em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;hide-tail&quot; style=&quot;min-width:0.853em;height:1.08em;&quot;&gt;&lt;svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'&gt;&lt;path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.07966799999999996em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.677em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.93em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;Ψ&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;minner&quot;&gt;&lt;span class=&quot;mopen delimcenter&quot; style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;delimsizing size3&quot;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.5016639999999999em;&quot;&gt;&lt;span style=&quot;top:-2.314em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.750664em;&quot;&gt;&lt;span style=&quot;top:-2.9890000000000003em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05724em;&quot;&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.677em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;−&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.824664em;&quot;&gt;&lt;span style=&quot;top:-3.063em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05724em;&quot;&gt;j&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;n&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.686em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose delimcenter&quot; style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;delimsizing size3&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose delimcenter&quot; style=&quot;top:0em;&quot;&gt;&lt;span class=&quot;delimsizing size3&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:-0.4747300000000001em;&quot;&gt;&lt;span style=&quot;top:-1.7002700000000002em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05724em;&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;mpunct mtight&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;ϵ&lt;/span&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathbb mtight&quot;&gt;Z&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.7463142857142857em;&quot;&gt;&lt;span style=&quot;top:-2.786em;margin-right:0.07142857142857144em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.5em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size3 size1 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.17473em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;sont-elles des bases orthonormales. Ensuite on appliquerait les techniques d’approximations consistant à éliminer les petits coefficients.&lt;/p&gt;

&lt;p&gt;Les multi-résolutions sont des espaces linéaires sur lesquels nous allons projeter ces signaux. On va chercher à réduire les dimensions (par ex d’une image) en projetant sur ces espaces emboîtés. Et conserver le maximum d’information.&lt;/p&gt;

&lt;p&gt;Un produit scalaire avec une fonction translatée peut toujours s’écrire comme un produit de convolution (Stéphane Mallat répète souvent cette propriété)&lt;/p&gt;

&lt;p&gt;Stéphane Mallat fait ensuite le lien avec les algorithmes en bancs de filtre (cascades de filtrage + échantillonnage).&lt;/p&gt;

&lt;p&gt;Dans ces opérations il y a sans arrêt des passages du continu au discret. Par exemple si je prends un signal et que je le projette sur ces espaces je me retrouve avec les coordonnées, qui sont les produits scalaires avec mes &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϕ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\phi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8888799999999999em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϕ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;sub&gt;j,n&lt;/sub&gt; (car base orthogonale), ce qui revient à filtrer et sous échantillonner.&lt;/p&gt;

&lt;h2 id=&quot;32321---bases-orthonormales-dondelettes-lecture-7&quot;&gt;3/23/21 - Bases orthonormales d’ondelettes (lecture 7)&lt;/h2&gt;

&lt;p&gt;On repart sur notre triangle. Depuis 2 cours on est sur l’approximation basse dimension.&lt;/p&gt;

&lt;p&gt;Stéphane Mallat applique le théorème sur des cas particuliers de la base de Haar, puis de la base de Shannon. Et revient sur la construction d’une base orthonormales avec des ondelettes “optimales”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/cdf_coeff_ondelette.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Quand on prend le produit scalaire de notre signal f avec les ondelettes, on obtient des résultats presque nuls lorsque le signal est régulier. Et plus on a de moments nuls avec nos ondelettes, plus la régularité est ignorée (l’approximation par projection sur un espace vectorielle des monômes à l’ordre n).&lt;/p&gt;

&lt;p&gt;On va cascader les projections a&lt;sub&gt;j&lt;/sub&gt; (et les détails d&lt;sub&gt;j&lt;/sub&gt;), et ça va revenir à cascader les filtres (les coefficients et les ondelettes).&lt;/p&gt;

&lt;p&gt;Pour cela on calcule les valeurs des a&lt;sub&gt;j&lt;/sub&gt; et d&lt;sub&gt;j&lt;/sub&gt; en fonction de a&lt;sub&gt;j-1&lt;/sub&gt;. On montre que cela s’obtient en filtrant (respectivement avec les &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\overline{h}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.89444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord overline&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.89444em;&quot;&gt;&lt;span style=&quot;top:-3em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.81444em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;overline-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; et &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mover accent=&quot;true&quot;&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&quot;true&quot;&gt;‾&lt;/mo&gt;&lt;/mover&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\overline{g}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.825em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord overline&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.63056em;&quot;&gt;&lt;span style=&quot;top:-3em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;g&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.55056em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;overline-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.19444em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;) puis en sous-échantillonnant. En cascadant on obtient une série de filtrages, sous-échantillonnages, filtrages, sous-échantillonnages, , etc.&lt;/p&gt;

&lt;p&gt;Les filtrages sont des convolutions. Si h a un support compact, ça va réduire le temps de calcul.(le nombre d’éléments non nuls correspond à la taille du filtre). Le nombre d’opérations pour passer de a&lt;sub&gt;L&lt;/sub&gt; à a&lt;sub&gt;L-1&lt;/sub&gt;, d&lt;sub&gt;L-1&lt;/sub&gt; est N*2m (où N: nombre de coefficients de a&lt;sub&gt;L&lt;/sub&gt; et m est le nombre de moments nuls)&lt;/p&gt;

&lt;p&gt;Le nombre d’opérations est linéaire, et la constante correspond à la taille des filtres.&lt;/p&gt;

&lt;p&gt;On peut inverser cet algorithmes (car base o.n.) et la structure emboîtée va nous donner algorithme de reconstruction. On va sur-échantillonner (augmenter d’un facteur 2 en intercalant des 0) et appliquer les filtres g et h, et sommer pour obtenir le résultat.&lt;/p&gt;

&lt;p&gt;Donc en gardant la base fréquence a&lt;sub&gt;J&lt;/sub&gt; et tous les détails {d&lt;sub&gt;j&lt;/sub&gt;}, on reconstitue a&lt;sub&gt;L&lt;/sub&gt;. (les signaux sur des grilles de plus en plus fines)&lt;/p&gt;

&lt;p&gt;Stéphane Mallat finit sur des exemples en 2 dimensions. En 2 dimensions on aura 3 ondelettes à chaque échelle (1 avec les hautes fréquences dans une direction, 1 avec les hautes fréquences dans l’autre direction, et la dernière avec haute fréquence sur les 2 directions (les coins)).&lt;/p&gt;</content><author><name></name></author><category term="deep learning" /><category term="math" /><summary type="html">Un exposé en 8 cours au collège de France de Stéphane Mallat sur les représentations parcimonieuses - 2021.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/math.jpeg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/math.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>