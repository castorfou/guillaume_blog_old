<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://castorfou.github.io/guillaume_blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://castorfou.github.io/guillaume_blog/" rel="alternate" type="text/html" /><updated>2021-04-08T07:44:16-05:00</updated><id>https://castorfou.github.io/guillaume_blog/feed.xml</id><title type="html">Guillaume’s blog</title><subtitle>Journey for a datascientist</subtitle><entry><title type="html">using SOCKS5 proxy - with git, apt, pip, …</title><link href="https://castorfou.github.io/guillaume_blog/blog/git-avec-proxy-socks.html" rel="alternate" type="text/html" title="using SOCKS5 proxy - with git, apt, pip, …" /><published>2021-04-06T00:00:00-05:00</published><updated>2021-04-06T00:00:00-05:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/git%20avec%20proxy%20socks</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/git-avec-proxy-socks.html">&lt;h2 id=&quot;setup-socks5-server&quot;&gt;setup socks5 server&lt;/h2&gt;

&lt;p&gt;using &lt;a href=&quot;https://community.hetzner.com/tutorials/install-and-configure-danted-proxy-socks5&quot;&gt;dante&lt;/a&gt; server&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;dante-server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Conf file&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;nano /etc/danted.conf

logoutput: stderr
internal: enp3s0 port &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1080
external: enp3s0
socksmethod: none
clientmethod: none
user.privileged: proxy
user.unprivileged: nobody
user.libwrap: nobody
client pass &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        from: 0.0.0.0/0 to: 0.0.0.0/0
        log: error connect disconnect
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
client block &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        from: 0.0.0.0/0 to: 0.0.0.0/0
        log: connect error
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
socks pass &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        from: 0.0.0.0/0 to: 0.0.0.0/0
        log: error connect disconnect
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
socks block &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        from: 0.0.0.0/0 to: 0.0.0.0/0
        log: connect error
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Start and monitor usage&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;service danted restart
&lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; /var/log/syslog
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;git-setup&quot;&gt;Git setup&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; .ssh/config
Host github.com
IdentityFile ~/.ssh/id_rsa_gmail
ProxyCommand /bin/nc &lt;span class=&quot;nt&quot;&gt;-X&lt;/span&gt; 5 &lt;span class=&quot;nt&quot;&gt;-x&lt;/span&gt; 192.168.50.202:1080 %h %p
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;proxychains&quot;&gt;Proxychains&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;installation&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# to be downloaded from apt mirrors:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# libproxychains proxychains&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;dpkg &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; libproxychains3_3.1-7_amd64.deb proxychains_3.1-7_all.deb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;configuration&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;vi /etc/proxychains.conf

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ProxyList]
&lt;span class=&quot;c&quot;&gt;# add proxy here ...&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# meanwile&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# defaults set to &quot;tor&quot;&lt;/span&gt;
socks5          192.168.50.202  1080
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;usage&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;proxychains apt update
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;proxychains apt upgrade

proxychains pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;pycaret 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="git" /><summary type="html">setup socks5 server</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://git-scm.com/images/logos/downloads/Git-Icon-1788C.png" /><media:content medium="image" url="https://git-scm.com/images/logos/downloads/Git-Icon-1788C.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ANITI’s first Reinforcement Learning Virtual School</title><link href="https://castorfou.github.io/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html" rel="alternate" type="text/html" title="ANITI’s first Reinforcement Learning Virtual School" /><published>2021-04-01T00:00:00-05:00</published><updated>2021-04-01T00:00:00-05:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/Aniti%20-%20RLVS%20-%20seminaire%20RL</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html">&lt;p&gt;&lt;img src=&quot;https://d1keuthy5s86c8.cloudfront.net/static/ems/upload/img/72947a097165dcd24a6f700e2f28d690.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://rlvs.aniti.fr/&quot;&gt;https://rlvs.aniti.fr/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Schedule is&lt;/p&gt;

&lt;h2 id=&quot;rlvs-schedule&quot;&gt;RLVS schedule&lt;/h2&gt;

&lt;p&gt;This condensed schedule does not include class breaks and social events. Times are Central European Summer Time (UTC+2).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Schedule&lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;March 25th&lt;/td&gt;
      &lt;td&gt;9:00-9:10&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/opening.html&quot;&gt;Opening remarks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/sebastien-gerchinovitz.html&quot;&gt;S. Gerchinovitz&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;9:10-9:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/rlvs-overview.html&quot;&gt;RLVS Overview&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html&quot;&gt;E. Rachelson&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;9:30-13:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/rl-fundamentals.html&quot;&gt;RL fundamentals&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html&quot;&gt;E. Rachelson&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;14:00-16:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/deep-learning.html&quot;&gt;Introduction to Deep Learning&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/dennis-wilson.html&quot;&gt;D. Wilson&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;16:30-17:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/human-behavioral-agents.html&quot;&gt;Reward Processing Biases in Humans and RL Agents&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/irina-rish.html&quot;&gt;I. Rish&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;17:45-18:45&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/hierarchical.html&quot;&gt;Introduction to Hierarchical Reinforcement Learning&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/doina-precup.html&quot;&gt;D. Precup&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;March 26th&lt;/td&gt;
      &lt;td&gt;10:00-12:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/stochastic-bandits.html&quot;&gt;Stochastic bandits&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/tor-lattimore.html&quot;&gt;T. Lattimore&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;14:00-16:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/mcts.html&quot;&gt;Monte Carlo Tree Search&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/tor-lattimore.html&quot;&gt;T. Lattimore&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;16:30-17:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/clinical.html&quot;&gt;Multi-armed bandits in clinical trials&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/donald-berry.html&quot;&gt;D. A. Berry&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;April 1st&lt;/td&gt;
      &lt;td&gt;9:00-15:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/dqn.html&quot;&gt;Deep Q-Networks and its variants&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/bilal-piot.html&quot;&gt;B. Piot&lt;/a&gt;, &lt;a href=&quot;https://rl-vs.github.io/rlvs2021/corentin-tallec.html&quot;&gt;C. Tallec&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;15:15-16:15&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/regularized-mdps.html&quot;&gt;Regularized MDPs&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/matthieu-geist.html&quot;&gt;M. Geist&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;16:30-17:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/regret-bound.html&quot;&gt;Regret bounds of model-based reinforcement learning&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/mengdi-wang.html&quot;&gt;M. Wang&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;April 2nd&lt;/td&gt;
      &lt;td&gt;9:00-12:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/pg.html&quot;&gt;Policy Gradients and Actor Critic methods&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/olivier-sigaud.html&quot;&gt;O. Sigaud&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;14:00-15:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/pg-pitfalls.html&quot;&gt;Pitfalls in Policy Gradient methods&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/olivier-sigaud.html&quot;&gt;O. Sigaud&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;15:30-17:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/exploration.html&quot;&gt;Exploration in Deep RL&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/matteo-pirotta.html&quot;&gt;M. Pirotta&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;April 8th&lt;/td&gt;
      &lt;td&gt;9:00-11:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/evo-rl.html&quot;&gt;Evolutionary Reinforcement Learning&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/dennis-wilson.html&quot;&gt;D. Wilson&lt;/a&gt;, &lt;a href=&quot;https://rl-vs.github.io/rlvs2021/jean-baptiste-mouret.html&quot;&gt;J.-B. Mouret&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;11:30-12:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/evolving-agents.html&quot;&gt;Evolving Agents that Learn More Like Animals&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/sebastian-risi.html&quot;&gt;S. Risi&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;14:00-16:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/micro-data.html&quot;&gt;Micro-data Policy Search&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/konstantinos-chatzilygeroudis.html&quot;&gt;K. Chatzilygeroudis&lt;/a&gt;, &lt;a href=&quot;https://rl-vs.github.io/rlvs2021/jean-baptiste-mouret.html&quot;&gt;J.-B. Mouret&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;16:30-17:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/efficient-motor.html&quot;&gt;Efficient Motor Skills Learning in Robotics&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/dongheui-lee.html&quot;&gt;D. Lee&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;April 9th&lt;/td&gt;
      &lt;td&gt;9:00-13:00&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/tips-and-tricks.html&quot;&gt;RL tips and tricks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/antonin-raffin.html&quot;&gt;A. Raffin&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;14:30-15:30&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/symbolic.html&quot;&gt;Symbolic representations and reinforcement learning&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/marta-garnelo.html&quot;&gt;M. Garnelo&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;15:45-16:45&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/model-learning.html&quot;&gt;Leveraging model-learning for extreme generalization&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/leslie-kaelbling.html&quot;&gt;L. P. Kaelbling&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;17:00-18:00&lt;/td&gt;
      &lt;td&gt;RLVS wrap-up&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html&quot;&gt;E. Rachelson&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;4121---deep-q-networks-and-its-variants&quot;&gt;(4/1/21) - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416824/?view=&quot;&gt;Deep Q-Networks and its variants&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Speaker is Bilal Piot.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep Q network&lt;/strong&gt; as a solution for a practicable control theory.&lt;/p&gt;

&lt;p&gt;Introduction of ALE (Atari Learning Environment)&lt;/p&gt;

&lt;p&gt;DQN is (almost) end-to-end: from raw observations to actions. Bilal explains the preprocessing part (from 160x210x3 to 84x84 + stacking 4 frames + downsampling to 15 Hz)&lt;/p&gt;

&lt;p&gt;Value Iteration (VI) algorithm: Recurrent algorithm to get Q. $Q_{k+1}=T^*Q$&lt;/p&gt;

&lt;p&gt;But it is not practical in a real-world case. What we can do is use interactions with real world. And estimate $Q^*$ using a regression.&lt;/p&gt;

&lt;p&gt;Would be interesting to have slides. I like the link between regression notations and VI notation.&lt;/p&gt;

&lt;p&gt;From neural Fitted-$Q$ to DQN. Main difference is data collection (in DQN you have updated interactions and it allows exploration, and size of architecture)&lt;/p&gt;

&lt;p&gt;With DQN we have acting part and learning part. Acting is the data collection. (using $\epsilon$-greedy policy)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;hands-on based on DQN tutorial notebook.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;had to  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;export LD_LIBRARY_PATH=/home/explore/miniconda3/envs/aniti/lib/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Nice introduction to JAX and haiku. Haiku is similar modules in pytorch and can turn NN into pure version. Which is useful for Jax.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;overview of the literature&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://kstatic.googleusercontent.com/files/f6b5f285173d4449285a8e812b8385f45c03f7104e1c41370a73e0c8558ff82d6a69e60962dd91c4972c444fd73bc4f98a06b5487eff5a037a37bc42f97cef3b&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4221---from-policy-gradients-to-actor-critic-methods&quot;&gt;(4/2/21) - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416833/?view=&quot;&gt;From Policy Gradients to Actor Critic methods&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Olivier Sigaud is the speaker.&lt;/p&gt;

&lt;p&gt;He has pre-recorded his lecture in videos. I have missed the start so I will have to watch them later.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416836/?view=&quot;&gt;Policy Gradient in pratice&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Don’t become an alchemist ;)&lt;/p&gt;

&lt;p&gt;As stochastic policies, squashed gaussian is interesting because it allows continuous variable + bounds.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416838/?view=#&quot;&gt;Exploration in Deep RL&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;4821---evolutionary-reinforcement-learning&quot;&gt;(4/8/21) - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416851/?view=&quot;&gt;Evolutionary Reinforcement Learning&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;pdf version of the slides are available &lt;a href=&quot;https://rl-vs.github.io/rlvs2021/class-material/evolutionary/light-virtual_school_neat_hyperneat.pdf&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;then &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416848/?view=&quot;&gt;Evolving Agents that Learn More Like Animals&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This morning was more about what we can do when we have infinite calculation power and data.&lt;/p&gt;

&lt;p&gt;Afternoon will be the opposite.&lt;/p&gt;

&lt;h2 id=&quot;4821---micro-data-policy-search&quot;&gt;(4/8/21) - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416841/?view=&quot;&gt;Micro-data Policy Search&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Most policy search algorithms require thousands of training episodes to  find an effective policy, which is often infeasible when experiments  takes time or are expensive (for instance, with physical robot or with  an aerodynamics simulator). This class focuses on the extreme other end  of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word  “big-data”, we refer to this challenge as “micro-data reinforcement  learning”. We will describe two main strategies: (1) leverage prior  knowledge on the policy structure (e.g., dynamic movement primitives),  on the policy parameters (e.g., demonstrations), or on the dynamics  (e.g., simulators), and (2) create data-driven surrogate models of the  expected reward (e.g., Bayesian optimization) or the dynamical model  (e.g., model-based policy search), so that the policy optimizer queries  the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup.&lt;/p&gt;</content><author><name></name></author><category term="reinforcement learning" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/RL.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/RL.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Logbook for April 21</title><link href="https://castorfou.github.io/guillaume_blog/blog/logbook-April.html" rel="alternate" type="text/html" title="Logbook for April 21" /><published>2021-04-01T00:00:00-05:00</published><updated>2021-04-01T00:00:00-05:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/logbook-April</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/logbook-April.html">&lt;h2 id=&quot;week-13---apr-21&quot;&gt;Week 13 - Apr 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Thursday 4/1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html&quot;&gt;Aniti RLVS&lt;/a&gt; - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416824/?view=#&quot;&gt;Deep Q-Networks and its variants&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Collège de France - Algorithmes quantiques : quand la physique quantique défie la thèse de Church-Turing &lt;a href=&quot;https://www.college-de-france.fr/site/frederic-magniez/inaugural-lecture-2020-2021.htm&quot;&gt;Leçon inaugurale&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 4/2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html&quot;&gt;Aniti RLVS&lt;/a&gt; - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416833/?view=&quot;&gt;From Policy Gradients to Actor Critic methods&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html&quot;&gt;Aniti RLVS&lt;/a&gt; - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416836/?view=&quot;&gt;Policy Gradient in pratice&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;git to use socks server to github (to go through local firewall)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html&quot;&gt;Aniti RLVS&lt;/a&gt; - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416838/?view=#&quot;&gt;Exploration in Deep RL&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;week-14---apr-21&quot;&gt;Week 14 - Apr 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 4/7&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.07442&quot;&gt;tabnet&lt;/a&gt;: pytorch (and fastai with Zach Mueller) implementations&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thursday 4/8&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html&quot;&gt;Aniti RLVS&lt;/a&gt; - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416851/?view=&quot;&gt;Evolutionary Reinforcement Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Jupyter notebook turned into &lt;a href=&quot;https://www.blog.pythonlibrary.org/2018/09/25/creating-presentations-with-jupyter-notebook/&quot;&gt;slides with RISE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html&quot;&gt;Aniti RLVS&lt;/a&gt; - &lt;a href=&quot;https://whova.com/embedded/session/rlstc_202011/1416841/?view=&quot;&gt;Micro-data Policy Search&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="logbook" /><summary type="html">Week 13 - Apr 21</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Headless raspberry pi: create a wifi to ethernet bridge</title><link href="https://castorfou.github.io/guillaume_blog/blog/headless-raspberry-pi-bridge-network.html" rel="alternate" type="text/html" title="Headless raspberry pi: create a wifi to ethernet bridge" /><published>2021-03-25T00:00:00-05:00</published><updated>2021-03-25T00:00:00-05:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/headless-raspberry-pi-bridge-network</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/headless-raspberry-pi-bridge-network.html">&lt;p&gt;After my internet provider router stopped unexpectedly yesterday, I had to find a solution with internet access from phones and raspberry pi to broadcast internet to full home devices.&lt;/p&gt;

&lt;h2 id=&quot;headless-raspberry-pi&quot;&gt;Headless raspberry pi&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Installation on SD from ubuntu&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;for a reason, &lt;em&gt;raspberry pi imager&lt;/em&gt; snap doesn’t work (due to a bug linked to QT+wayland).&lt;/p&gt;

&lt;p&gt;I download deb ubuntu version (imager_1.6_amd64.deb) from &lt;a href=&quot;https://www.raspberry.org/software&quot;&gt;https://www.raspberry.org/software&lt;/a&gt; and install with dpkg. (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo dpkg -i imager_1.6_amd64.deb&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rpi-imager&lt;/code&gt;, I can install by selecting the default OS (raspberry Pi OS 32-bit), and SD card as storage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Headless wifi&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As explained in &lt;a href=&quot;https://www.raspberrypi.org/documentation/configuration/wireless/headless.md&quot;&gt;https://www.raspberrypi.org/documentation/configuration/wireless/headless.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Create (touch) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wpa_supplicant.conf&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/boot&lt;/code&gt; of SD card and paste this content:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1
country=FR

network={
 ssid=&quot;AndroidAP&quot;
 psk=&quot;&amp;lt;Password for your wireless LAN&amp;gt;&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Headless ssh&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As explained in &lt;a href=&quot;https://www.raspberrypi.org/documentation/remote-access/ssh/README.md&quot;&gt;https://www.raspberrypi.org/documentation/remote-access/ssh/README.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Create (touch) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/boot&lt;/code&gt; of SD card&lt;/p&gt;

&lt;p&gt;If it is found, SSH is enabled and the file is deleted. The content of  the file does not matter; it could contain text, or nothing at all.&lt;/p&gt;

&lt;h2 id=&quot;test-installation&quot;&gt;Test installation&lt;/h2&gt;

&lt;p&gt;Boot. After a couple of minutes, I have a notification on phone saying a device is connected on my phone hotspot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/home/explore/git/guillaume/blog/images/raspberrypi_hotspot.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And ssh raspberry (default username/password are pi/raspberry)&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ssh &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; pi 192.168.43.179
pi@192.168.43.179&lt;span class=&quot;s1&quot;&gt;'s password: 
Linux raspberrypi 5.4.83-v7+ #1379 SMP Mon Dec 14 13:08:57 GMT 2020 armv7l

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Thu Mar 25 06:23:17 2021

SSH is enabled and the default password for the '&lt;/span&gt;pi&lt;span class=&quot;s1&quot;&gt;' user has not been changed.
This is a security risk - please login as the '&lt;/span&gt;pi&lt;span class=&quot;s1&quot;&gt;' user and type '&lt;/span&gt;passwd&lt;span class=&quot;s1&quot;&gt;' to set a new password.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Headless raspberry is ready to be used.&lt;/p&gt;

&lt;h2 id=&quot;wifi-to-ethernet-bridge&quot;&gt;Wifi to ethernet bridge&lt;/h2&gt;

&lt;p&gt;I will use &lt;a href=&quot;https://willhaley.com/blog/raspberry-pi-wifi-ethernet-bridge/&quot;&gt;https://willhaley.com/blog/raspberry-pi-wifi-ethernet-bridge/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The only  package that is needed is dnsmasq however from a clean install it is a  good idea to make sure everything is up-to-date:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;get up-to-date system&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get upgrade &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;rpi-update dnsmasq &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;rpi-update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Option 1 - Same Subnet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Save this script as a file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bridge.sh&lt;/code&gt; on your Pi.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env bash&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$EUID&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-ne&lt;/span&gt; 0 &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;run as root&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&amp;amp;2 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;1

&lt;span class=&quot;c&quot;&gt;##########################################################&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# You should not need to update anything below this line #&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;##########################################################&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# parprouted  - Proxy ARP IP bridging daemon&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# dhcp-helper - DHCP/BOOTP relay agent&lt;/span&gt;

apt update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; parprouted dhcp-helper

systemctl stop dhcp-helper
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;dhcp-helper

&lt;span class=&quot;c&quot;&gt;# Enable ipv4 forwarding.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt; s/#net.ipv4.ip_forward&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1/net.ipv4.ip_forward&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1/ /etc/sysctl.conf

&lt;span class=&quot;c&quot;&gt;# Service configuration for standard WiFi connection. Connectivity will&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# be lost if the username and password are incorrect.&lt;/span&gt;
systemctl restart wpa_supplicant.service

&lt;span class=&quot;c&quot;&gt;# Enable IP forwarding for wlan0 if it's not already enabled.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'^option ip-forwarding 1$'&lt;/span&gt; /etc/dhcpcd.conf &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;option ip-forwarding 1&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /etc/dhcpcd.conf

&lt;span class=&quot;c&quot;&gt;# Disable dhcpcd control of eth0.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'^denyinterfaces eth0$'&lt;/span&gt; /etc/dhcpcd.conf &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;denyinterfaces eth0&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /etc/dhcpcd.conf

&lt;span class=&quot;c&quot;&gt;# Configure dhcp-helper.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /etc/default/dhcp-helper &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
DHCPHELPER_OPTS=&quot;-b wlan0&quot;
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF

&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Enable avahi reflector if it's not already enabled.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/#enable-reflector=no/enable-reflector=yes/'&lt;/span&gt; /etc/avahi/avahi-daemon.conf
&lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'^enable-reflector=yes$'&lt;/span&gt; /etc/avahi/avahi-daemon.conf &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;something went wrong...&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Manually set 'enable-reflector=yes in /etc/avahi/avahi-daemon.conf'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# I have to admit, I do not understand ARP and IP forwarding enough to explain&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# exactly what is happening here. I am building off the work of others. In short&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# this is a service to forward traffic from WiFi to Ethernet.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;' &amp;gt;/usr/lib/systemd/system/parprouted.service
[Unit]
Description=proxy arp routing service
Documentation=https://raspberrypi.stackexchange.com/q/88954/79866
Requires=sys-subsystem-net-devices-wlan0.device dhcpcd.service
After=sys-subsystem-net-devices-wlan0.device dhcpcd.service

[Service]
Type=forking
# Restart until wlan0 gained carrier
Restart=on-failure
RestartSec=5
TimeoutStartSec=30
# clone the dhcp-allocated IP to eth0 so dhcp-helper will relay for the correct subnet
ExecStartPre=/bin/bash -c '/sbin/ip addr add &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;/sbin/ip &lt;span class=&quot;nt&quot;&gt;-4&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-br&lt;/span&gt; addr show wlan0 | /bin/grep &lt;span class=&quot;nt&quot;&gt;-Po&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;/32 dev eth0'
ExecStartPre=/sbin/ip link set dev eth0 up
ExecStartPre=/sbin/ip link set wlan0 promisc on
ExecStart=-/usr/sbin/parprouted eth0 wlan0
ExecStopPost=/sbin/ip link set wlan0 promisc off
ExecStopPost=/sbin/ip link set dev eth0 down
ExecStopPost=/bin/bash -c '/sbin/ip addr del &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;/sbin/ip &lt;span class=&quot;nt&quot;&gt;-4&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-br&lt;/span&gt; addr show wlan0 | /bin/grep &lt;span class=&quot;nt&quot;&gt;-Po&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;d+&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;/32 dev eth0'

[Install]
WantedBy=wpa_supplicant.service
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF

&lt;/span&gt;systemctl daemon-reload
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;parprouted
systemctl start parprouted dhcp-helper
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Execute the script on your Pi like so.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;bash bridge.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Reboot.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;reboot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Done!&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="raspberry pi" /><summary type="html">After my internet provider router stopped unexpectedly yesterday, I had to find a solution with internet access from phones and raspberry pi to broadcast internet to full home devices.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://my.raspberrypi.org/favicon.ico" /><media:content medium="image" url="https://my.raspberrypi.org/favicon.ico" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Git - How To Contribute To A Project</title><link href="https://castorfou.github.io/guillaume_blog/blog/contribute-to-a-project-with-git.html" rel="alternate" type="text/html" title="Git - How To Contribute To A Project" /><published>2021-03-25T00:00:00-05:00</published><updated>2021-03-25T00:00:00-05:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/contribute%20to%20a%20project%20with%20git</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/contribute-to-a-project-with-git.html">&lt;p&gt;Based on http://qpleple.com/how-to-contribute-to-a-project-on-github/&lt;/p&gt;

&lt;h2 id=&quot;using-clustergit-as-an-example&quot;&gt;Using clustergit as an example&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Fork&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Make your own working copy of the project by forking it: go on the project page (https://github.com/mnagel/clustergit) and click “Fork”. You can access you copy at: https://github.com/castorfou/clustergit&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clone&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Clone your fork git repository on your local computer:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone git@github.com:castorfou/clustergit.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Branch&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch master-to-main
git checkout master-to-main
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is very important, create one branch per patch. And never submit a patch that has been done on the branch &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;master&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt;!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Develop&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here I want to reflect change from Oct/20 where default branch name in github is now main&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/master/main/g'&lt;/span&gt; clustergit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Commit&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git add &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt;
git commit &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;default branch name 'main'&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Push to github&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git push origin master-to-main
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create pull request&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Go on your fork page (https://github.com/castorfou/clustergit), then select &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;master-to-main&lt;/code&gt; in the branch list and click “Pull Request”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Submit patch&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Check the diff, write a message explaining what you have done and why the repository owner should accept your pull request and submit.&lt;/p&gt;</content><author><name></name></author><category term="git" /><summary type="html">Based on http://qpleple.com/how-to-contribute-to-a-project-on-github/</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://git-scm.com/images/logos/downloads/Git-Icon-1788C.png" /><media:content medium="image" url="https://git-scm.com/images/logos/downloads/Git-Icon-1788C.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stable baselines 3 - 1st steps</title><link href="https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html" rel="alternate" type="text/html" title="Stable baselines 3 - 1st steps" /><published>2021-03-24T00:00:00-05:00</published><updated>2021-03-24T00:00:00-05:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-03-24-stable-baselines-3.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;What-is-stable-baselines-3-(sb3)&quot;&gt;What is stable baselines 3 (sb3)&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-stable-baselines-3-(sb3)&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;I have just read about this new release. This is a complete rewrite of stable baselines 2, without any reference to tensorflow, and based on pytorch (&amp;gt;1.4+).&lt;/p&gt;
&lt;p&gt;There is a lot of running implementations of RL algorithms, based on gym.
A very good introduction in this &lt;a href=&quot;https://araffin.github.io/post/sb3/&quot;&gt;blog entry&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://araffin.github.io/post/sb3/&quot;&gt;Stable-Baselines3: Reliable Reinforcement Learning Implementations | Antonin Raffin | Homepage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GitHub repository: &lt;a href=&quot;https://github.com/DLR-RM/stable-baselines3&quot;&gt;https://github.com/DLR-RM/stable-baselines3&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Documentation: &lt;a href=&quot;https://stable-baselines3.readthedocs.io/&quot;&gt;https://stable-baselines3.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RL Baselines3 Zoo: &lt;a href=&quot;https://github.com/DLR-RM/rl-baselines3-zoo&quot;&gt;https://github.com/DLR-RM/rl-baselines3-zoo&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Contrib: &lt;a href=&quot;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&quot;&gt;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RL Tutorial: &lt;a href=&quot;https://github.com/araffin/rl-tutorial-jnrr19&quot;&gt;https://github.com/araffin/rl-tutorial-jnrr19&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;My-installation&quot;&gt;My installation&lt;a class=&quot;anchor-link&quot; href=&quot;#My-installation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Standard installation&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda create --name stablebaselines3 &lt;span class=&quot;nv&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;.7
conda activate stablebaselines3
pip install stable-baselines3&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;extra&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
conda install -c conda-forge jupyter_contrib_nbextensions
conda install nb_conda
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;conda list
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;# packages in environment at /home/explore/miniconda3/envs/stablebaselines3:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main  
_pytorch_select           0.1                       cpu_0  
absl-py                   0.12.0                   pypi_0    pypi
atari-py                  0.2.6                    pypi_0    pypi
attrs                     20.3.0             pyhd3deb0d_0    conda-forge
backcall                  0.2.0              pyh9f0ad1d_0    conda-forge
backports                 1.0                        py_2    conda-forge
backports.functools_lru_cache 1.6.1                      py_0    conda-forge
blas                      1.0                         mkl  
bleach                    3.3.0              pyh44b312d_0    conda-forge
box2d                     2.3.10                   pypi_0    pypi
box2d-py                  2.3.8                    pypi_0    pypi
ca-certificates           2021.1.19            h06a4308_1  
cachetools                4.2.1                    pypi_0    pypi
certifi                   2020.12.5        py37h06a4308_0  
cffi                      1.14.5           py37h261ae71_0  
chardet                   4.0.0                    pypi_0    pypi
cloudpickle               1.6.0                    pypi_0    pypi
cudatoolkit               11.0.221             h6bb024c_0  
cycler                    0.10.0                   pypi_0    pypi
decorator                 4.4.2                      py_0    conda-forge
defusedxml                0.7.1              pyhd8ed1ab_0    conda-forge
entrypoints               0.3             pyhd8ed1ab_1003    conda-forge
fire                      0.4.0              pyh44b312d_0    conda-forge
freetype                  2.10.4               h5ab3b9f_0  
future                    0.18.2                   pypi_0    pypi
google-auth               1.28.0                   pypi_0    pypi
google-auth-oauthlib      0.4.3                    pypi_0    pypi
grpcio                    1.36.1                   pypi_0    pypi
gym                       0.18.0                   pypi_0    pypi
icu                       58.2              hf484d3e_1000    conda-forge
idna                      2.10                     pypi_0    pypi
importlib-metadata        3.7.3            py37h89c1867_0    conda-forge
intel-openmp              2019.4                      243  
ipykernel                 5.5.0            py37h888b3d9_1    conda-forge
ipython                   7.21.0           py37h888b3d9_0    conda-forge
ipython_genutils          0.2.0                      py_1    conda-forge
jedi                      0.18.0           py37h89c1867_2    conda-forge
jinja2                    2.11.3             pyh44b312d_0    conda-forge
jpeg                      9b                   h024ee3a_2  
jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
jupyter_client            6.1.12             pyhd8ed1ab_0    conda-forge
jupyter_contrib_core      0.3.3                      py_2    conda-forge
jupyter_contrib_nbextensions 0.5.1              pyhd8ed1ab_2    conda-forge
jupyter_core              4.7.1            py37h89c1867_0    conda-forge
jupyter_highlight_selected_word 0.2.0           py37h89c1867_1002    conda-forge
jupyter_latex_envs        1.4.6           pyhd8ed1ab_1002    conda-forge
jupyter_nbextensions_configurator 0.4.1            py37h89c1867_2    conda-forge
kiwisolver                1.3.1                    pypi_0    pypi
lcms2                     2.11                 h396b838_0  
ld_impl_linux-64          2.33.1               h53a641e_7  
libffi                    3.3                  he6710b0_2  
libgcc-ng                 9.1.0                hdf63c60_0  
libmklml                  2019.0.5                      0  
libpng                    1.6.37               hbc83047_0  
libsodium                 1.0.18               h36c2ea0_1    conda-forge
libstdcxx-ng              9.1.0                hdf63c60_0  
libtiff                   4.2.0                h85742a9_0  
libuv                     1.40.0               h7b6447c_0  
libwebp-base              1.2.0                h27cfd23_0  
libxml2                   2.9.10               hb55368b_3  
libxslt                   1.1.34               hc22bd24_0  
lxml                      4.6.3            py37h9120a33_0  
lz4-c                     1.9.3                h2531618_0  
markdown                  3.3.4                    pypi_0    pypi
markupsafe                1.1.1            py37hb5d75c8_2    conda-forge
matplotlib                3.3.4                    pypi_0    pypi
mistune                   0.8.4           py37h4abf009_1002    conda-forge
mkl                       2020.2                      256  
mkl-service               2.3.0            py37he8ac12f_0  
mkl_fft                   1.3.0            py37h54f3939_0  
mkl_random                1.1.1            py37h0573a6f_0  
nb_conda                  2.2.1                    py37_0  
nb_conda_kernels          2.3.1            py37h06a4308_0  
nbconvert                 5.6.1            py37hc8dfbb8_1    conda-forge
nbformat                  5.1.2              pyhd8ed1ab_1    conda-forge
ncurses                   6.2                  he6710b0_1  
ninja                     1.10.2           py37hff7bd54_0  
notebook                  5.7.10           py37hc8dfbb8_0    conda-forge
numpy                     1.20.1                   pypi_0    pypi
numpy-base                1.19.2           py37hfa32c7d_0  
oauthlib                  3.1.0                    pypi_0    pypi
olefile                   0.46                     py37_0  
opencv-python             4.5.1.48                 pypi_0    pypi
openssl                   1.1.1j               h27cfd23_0  
packaging                 20.9               pyh44b312d_0    conda-forge
pandas                    1.2.3                    pypi_0    pypi
pandoc                    2.12                 h7f98852_0    conda-forge
pandocfilters             1.4.2                      py_1    conda-forge
parso                     0.8.1              pyhd8ed1ab_0    conda-forge
pexpect                   4.8.0              pyh9f0ad1d_2    conda-forge
pickleshare               0.7.5                   py_1003    conda-forge
pillow                    7.2.0                    pypi_0    pypi
pip                       21.0.1           py37h06a4308_0  
prometheus_client         0.9.0              pyhd3deb0d_0    conda-forge
prompt-toolkit            3.0.18             pyha770c72_0    conda-forge
protobuf                  3.15.6                   pypi_0    pypi
psutil                    5.8.0                    pypi_0    pypi
ptyprocess                0.7.0              pyhd3deb0d_0    conda-forge
pyasn1                    0.4.8                    pypi_0    pypi
pyasn1-modules            0.2.8                    pypi_0    pypi
pycparser                 2.20                       py_2  
pyglet                    1.5.0                    pypi_0    pypi
pygments                  2.8.1              pyhd8ed1ab_0    conda-forge
pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
pyrsistent                0.17.3           py37h4abf009_1    conda-forge
python                    3.7.10               hdb3f193_0  
python-dateutil           2.8.1                      py_0    conda-forge
python_abi                3.7                     1_cp37m    conda-forge
pytorch                   1.7.1           py3.7_cuda11.0.221_cudnn8.0.5_0    pytorch
pytz                      2021.1                   pypi_0    pypi
pyyaml                    5.3.1            py37hb5d75c8_1    conda-forge
pyzmq                     19.0.2           py37hac76be4_2    conda-forge
readline                  8.1                  h27cfd23_0  
requests                  2.25.1                   pypi_0    pypi
requests-oauthlib         1.3.0                    pypi_0    pypi
rsa                       4.7.2                    pypi_0    pypi
scipy                     1.6.1                    pypi_0    pypi
send2trash                1.5.0                      py_0    conda-forge
setuptools                52.0.0           py37h06a4308_0  
six                       1.15.0             pyh9f0ad1d_0    conda-forge
sqlite                    3.35.2               hdfb4753_0  
stable-baselines3         1.0                      pypi_0    pypi
tensorboard               2.4.1                    pypi_0    pypi
tensorboard-plugin-wit    1.8.0                    pypi_0    pypi
termcolor                 1.1.0            py37h06a4308_1  
terminado                 0.9.3            py37h89c1867_0    conda-forge
testpath                  0.4.4                      py_0    conda-forge
tk                        8.6.10               hbc83047_0  
torchaudio                0.7.2                      py37    pytorch
torchvision               0.8.2                py37_cu110    pytorch
tornado                   6.1              py37h4abf009_0    conda-forge
traitlets                 5.0.5                      py_0    conda-forge
typing-extensions         3.7.4.3                       0  
typing_extensions         3.7.4.3                    py_0    conda-forge
urllib3                   1.26.4                   pypi_0    pypi
wcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge
webencodings              0.5.1                      py_1    conda-forge
werkzeug                  1.0.1                    pypi_0    pypi
wheel                     0.36.2             pyhd3eb1b0_0  
xz                        5.2.5                h7b6447c_0  
yaml                      0.2.5                h516909a_0    conda-forge
zeromq                    4.3.4                h2531618_0  
zipp                      3.4.1              pyhd8ed1ab_0    conda-forge
zlib                      1.2.11               h7b6447c_3  
zstd                      1.4.5                h9ceee32_0  
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;SB3-tutorials&quot;&gt;SB3 tutorials&lt;a class=&quot;anchor-link&quot; href=&quot;#SB3-tutorials&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gym&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;stable_baselines3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2C&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;stable_baselines3.common.monitor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Monitor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;stable_baselines3.common.callbacks&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CheckpointCallback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EvalCallback&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Save a checkpoint every 1000 steps&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;checkpoint_callback&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CheckpointCallback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/home/explore/git/guillaume/stable_baselines_3/logs/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                         &lt;span class=&quot;n&quot;&gt;name_prefix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;rl_model&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Evaluate the model periodically&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# and auto-save the best model and evaluations&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Use a monitor wrapper to properly report episode stats&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eval_env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;LunarLander-v2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Use deterministic actions for evaluation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eval_callback&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EvalCallback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_model_save_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/home/explore/git/guillaume/stable_baselines_3/logs/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;log_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/home/explore/git/guillaume/stable_baselines_3/logs/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Train an agent using A2C on LunarLander-v2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;MlpPolicy&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;LunarLander-v2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_timesteps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callback&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval_callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Retrieve and reset the environment&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Query the agent (stochastic action here)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Issues-and-fix&quot;&gt;Issues and fix&lt;a class=&quot;anchor-link&quot; href=&quot;#Issues-and-fix&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;CUDA error: CUBLAS_STATUS_INTERNAL_ERROR&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Downgrade pytorch to 1.7.1&lt;/p&gt;
&lt;p&gt;to avoid &lt;code&gt;RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasCreate(handle)&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install &lt;span class=&quot;nv&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;.7.1
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;RuntimeError: CUDA error: invalid device function&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;nvidia-smi
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Thu Mar 25 09:13:49 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 4000     Off  | 00000000:01:00.0  On |                  N/A |
| N/A   41C    P5    18W /  N/A |   2104MiB /  7982MiB |     32%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1153      G   /usr/lib/xorg/Xorg                162MiB |
|    0   N/A  N/A      1904      G   /usr/lib/xorg/Xorg                268MiB |
|    0   N/A  N/A      2076      G   /usr/bin/gnome-shell              403MiB |
|    0   N/A  N/A      2697      G   ...gAAAAAAAAA --shared-files       54MiB |
|    0   N/A  N/A      7220      G   ...AAAAAAAAA= --shared-files       84MiB |
|    0   N/A  N/A     57454      G   /usr/lib/firefox/firefox            2MiB |
|    0   N/A  N/A     59274      C   ...ablebaselines3/bin/python     1051MiB |
+-----------------------------------------------------------------------------+
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;CUDA version is 11.0 on my workstation.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;nvcc --version
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;conda install &lt;span class=&quot;nv&quot;&gt;pytorch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;.7.1 &lt;span class=&quot;nv&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;.8.2 &lt;span class=&quot;nv&quot;&gt;torchaudio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;.7.2 &lt;span class=&quot;nv&quot;&gt;cudatoolkit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;11&lt;/span&gt;.0 -c pytorch
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Collecting package metadata (current_repodata.json): done
Solving environment: done

# All requested packages already installed.

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Everything seems fine after these updates.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Stable-baselines-3-user-guide&quot;&gt;Stable baselines 3 user guide&lt;a class=&quot;anchor-link&quot; href=&quot;#Stable-baselines-3-user-guide&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is an impressive documentation associated with stable baselines 3.
&lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html&quot;&gt;Quickstart&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Tips-and-tricks&quot;&gt;Tips and tricks&lt;a class=&quot;anchor-link&quot; href=&quot;#Tips-and-tricks&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This &lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html&quot;&gt;page&lt;/a&gt; covers general advice about RL (where to start, which algorithm to choose, how to evaluate an algorithm, …), as well as tips and tricks when using a custom environment or implementing an RL algorithm.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Be familiar with RL, see &lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/rl.html&quot;&gt;resource&lt;/a&gt; page&lt;/li&gt;
&lt;li&gt;read SB3 documentation&lt;/li&gt;
&lt;li&gt;do the &lt;a href=&quot;https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3&quot;&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tune hyperparameters&lt;/strong&gt;
RL zoo is introduced. It contains some hyperparameter optimization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RL evaluation&lt;/strong&gt;
We suggest you reading &lt;a href=&quot;https://arxiv.org/abs/1709.06560&quot;&gt;Deep Reinforcement Learning that Matters&lt;/a&gt; for a good discussion about RL evaluation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;which algorithm to choose&lt;/strong&gt;
1st criteria is discrete vs continuous actions. And 2nd is capacity to parallelize training.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Discrete Actions&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discrete Actions - Single Process&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;DQN&lt;/code&gt; with extensions (double DQN, prioritized replay, …) are the recommended algorithms. We notably provide &lt;code&gt;QR-DQN&lt;/code&gt; in our &lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html#sb3-contrib&quot;&gt;contrib repo&lt;/a&gt;. &lt;code&gt;DQN&lt;/code&gt; is usually slower to train (regarding wall clock time) but is the most sample efficient (because of its replay buffer).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Discrete Actions - Multiprocessed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You should give a try to &lt;code&gt;PPO&lt;/code&gt; or &lt;code&gt;A2C&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Continuous Actions&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Continuous Actions - Single Process&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Current State Of The Art (SOTA) algorithms are &lt;code&gt;SAC&lt;/code&gt;, &lt;code&gt;TD3&lt;/code&gt; and &lt;code&gt;TQC&lt;/code&gt; (available in our &lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html#sb3-contrib&quot;&gt;contrib repo&lt;/a&gt;). Please use the hyperparameters in the &lt;a href=&quot;https://github.com/DLR-RM/rl-baselines3-zoo&quot;&gt;RL zoo&lt;/a&gt; for best results.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Continuous Actions - Multiprocessed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Take a look at &lt;code&gt;PPO&lt;/code&gt; or &lt;code&gt;A2C&lt;/code&gt;. Again, don’t forget to take the hyperparameters from the &lt;a href=&quot;https://github.com/DLR-RM/rl-baselines3-zoo&quot;&gt;RL zoo&lt;/a&gt; for continuous actions problems (cf &lt;em&gt;Bullet&lt;/em&gt; envs).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating a custom env&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;multiple times there are advices about normalizing: observation and action space. A good practice is to rescale your actions to lie in [-1, 1]. This does not limit you as you can easily rescale the action inside the environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tips and tricks to reproduce a RL paper&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html&quot;&gt;Reinforcement Learning Tips and Tricks — Stable Baselines3 1.1.0a1 documentation&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A personal pick (by @araffin) for environments with gradual difficulty in RL with continuous actions:&amp;gt; &amp;gt; 1.  Pendulum (easy to solve)
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;HalfCheetahBullet (medium difficulty with local minima and shaped reward)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;BipedalWalkerHardcore (if it works on that one, then you can have a cookie)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;in RL with discrete actions:&amp;gt; &amp;gt; 1.  CartPole-v1 (easy to be better than random agent, harder to achieve maximal performance)
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;LunarLander&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pong (one of the easiest Atari game)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;other Atari games (e.g. Breakout)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Resource-page&quot;&gt;Resource page&lt;a class=&quot;anchor-link&quot; href=&quot;#Resource-page&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://stable-baselines3.readthedocs.io/en/master/guide/rl.html&quot;&gt;Reinforcement Learning Resources — Stable Baselines3 1.1.0a1 documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stable-Baselines3 assumes that you already understand the basic concepts of Reinforcement Learning (RL).&lt;/p&gt;
&lt;p&gt;However, if you want to learn about RL, there are several good resources to get started:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://spinningup.openai.com/en/latest/&quot;&gt;OpenAI Spinning Up&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&quot;&gt;David Silver’s course&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html&quot;&gt;Lilian Weng’s blog&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/deep-rl-bootcamp/lectures&quot;&gt;Berkeley’s Deep RL Bootcamp&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse/&quot;&gt;Berkeley’s Deep Reinforcement Learning course&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/dennybritz/reinforcement-learning&quot;&gt;More resources&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Examples&quot;&gt;Examples&lt;a class=&quot;anchor-link&quot; href=&quot;#Examples&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I will run these examples in &lt;a href=&quot;https://github.com/castorfou/handson_stablebaselines3/blob/main/01%20-%20hands-on.ipynb&quot;&gt;01 -hands-on.ipynb&lt;/a&gt; from &lt;a href=&quot;https://github.com/castorfou/handson_stablebaselines3&quot;&gt;handson_stablebaselines3&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;DQN lunarlander&lt;/strong&gt;
&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/lunar_module.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;My module is never landing :(&lt;/p&gt;
&lt;p&gt;Note: animated gif created with &lt;a href=&quot;https://github.com/phw/peek&quot;&gt;peek&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;PPO with multiprocessing cartpole&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/cartpole_ppo.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Monitor training using callback&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This could be useful when you want to monitor training, for instance display live learning curves in Tensorboard (or in &lt;a href=&quot;https://github.com/fossasia/visdom&quot;&gt;Visdom&lt;/a&gt;) or save the best agent.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/sb3_learning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Atari game such as pong (A2C with 6 envt) or breakout&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/sb3_pong.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here the list of valid gym atari environments: &lt;a href=&quot;https://gym.openai.com/envs/#atari&quot;&gt;https://gym.openai.com/envs/#atari&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/sb3_breakout.gif&quot; alt=&quot;sb3_breakout.gif&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;pybullet&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a SDK to real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/bulletphysics/bullet3/&quot;&gt;https://github.com/bulletphysics/bullet3/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://camo.githubusercontent.com/0f979f7423e65c1bd31659d153b6b2a59d3fd83896bafb96f7c3f80e3f7f1ee1/68747470733a2f2f707962756c6c65742e6f72672f776f726470726573732f77702d636f6e74656e742f75706c6f6164732f323031392f30332f63726f707065642d707962756c6c65742e706e67&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We need to install it: &lt;code&gt;pip install pybullet&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I don't have rendering capacity when playing with it. Because robotic is far from my need, I will skip on this one&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Hindsight Experience Replay (HER)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;using &lt;a href=&quot;https://github.com/eleurent/highway-env&quot;&gt;Highway-Env&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;installation with &lt;code&gt;pip install highway-env&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;After 1h15m of training, some 1st results:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/sb3_parking.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;And after that some technical stuff such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learning Rate Schedule: start with high value and reduce it as learning goes&lt;/li&gt;
&lt;li&gt;Advanced Saving and Loading: how to easily create a test environment to evaluate an agent periodically, use a policy independently from a model (and how to save it, load it) and save/load a replay buffer.&lt;/li&gt;
&lt;li&gt;Accessing and modifying model parameters: These functions are useful when you need to e.g. evaluate large set of models with same network structure, visualize different layers of the network or modify parameters manually.&lt;/li&gt;
&lt;li&gt;Record a video or make a gif&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Make a GIF of a Trained Agent&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install imageio&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;and this time the lander is getting closer to moon but not at all between flags.
&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/lander_a2c.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="reinforcement learning" /><category term="pytorch" /><category term="sb3" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://stable-baselines3.readthedocs.io/en/master/_static/logo.png" /><media:content medium="image" url="https://stable-baselines3.readthedocs.io/en/master/_static/logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Git - How to find all *unpushed* commits for all projects in a directory?</title><link href="https://castorfou.github.io/guillaume_blog/blog/clustergit.html" rel="alternate" type="text/html" title="Git - How to find all *unpushed* commits for all projects in a directory?" /><published>2021-03-09T00:00:00-06:00</published><updated>2021-03-09T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/clustergit</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/clustergit.html">&lt;p&gt;Very basic question to help keep my repo clean.&lt;/p&gt;

&lt;h2 id=&quot;installation-clustergit&quot;&gt;Installation clustergit&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mnagel/clustergit&quot;&gt;clustergit&lt;/a&gt; seems a good candidate&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/mnagel/clustergit/master/doc/clustergit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/Applications
git clone git@github.com:mnagel/clustergit.git
&lt;span class=&quot;c&quot;&gt;# add export PATH=&quot;$PATH:$HOME/Applications/clustergit&quot; to ~.bashrc&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;usage-clustergit&quot;&gt;Usage clustergit&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;clustergit status&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;clustergit 
Scanning sub directories of &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
./Deep-Reinforcement-Learning-Hands-On  : Changesn &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1/17&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
./Deep_reinforcement_learning_Course    : Changes
./ReinforcementLearning_references      : On branch main, Untracked files
./blog                                  : Untracked files
./d059                                  : On branch main, Changes
./data-scientist-skills                 : Clean
./deeplearning_specialization           : Clean
./fastai                                : Changes
./fastai_experiments                    : Changes
./fastbook                              : Changes
./gan_specialization                    : Clean
./hello_nbdev                           : Clean
./introduction-reinforcement-learning-david-silver: On branch main, Untracked files
./mit_600.2x Introduction to Computational Thinking and Data Science: Clean
./mit_6S191_Intro_to_deep_learning      : On branch main, No Changes
./pytorch_tutorial                      : On branch main, Changes
./squeezebox                            : On branch main, No Changes
Done

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;clustergit status (detailed)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;clustergit &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;...]
&lt;span class=&quot;nt&quot;&gt;----------------&lt;/span&gt; ./squeezebox &lt;span class=&quot;nt&quot;&gt;-----------------&lt;/span&gt;
running  &lt;span class=&quot;nv&quot;&gt;LC_ALL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;C git status
On branch main
Your branch is up to &lt;span class=&quot;nb&quot;&gt;date &lt;/span&gt;with &lt;span class=&quot;s1&quot;&gt;'origin/main'&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

nothing to commit, working tree clean

./squeezebox                            : On branch main, No Changes
&lt;span class=&quot;nt&quot;&gt;----------------&lt;/span&gt; ./squeezebox &lt;span class=&quot;nt&quot;&gt;-----------------&lt;/span&gt;
Done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;clustergit status (less detailed: hide Clean)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;clustergit &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt;
Scanning sub directories of &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
./d059                                  : On branch main, Changes
./fastai                                : Changes
./fastai_experiments                    : Changes
./fastbook                              : Changes
./introduction-reinforcement-learning-david-silver: On branch main, Untracked files
./mit_6S191_Intro_to_deep_learning      : On branch main, No Changes
./pytorch_tutorial                      : On branch main, Changes
./squeezebox                            : On branch main, No Changes
Done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Clean&lt;/strong&gt; vs &lt;strong&gt;On branch main, No Changes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;seems related to branch name. If branch is named master, then clean is displayed.&lt;/p&gt;

&lt;p&gt;(Mar/25 21) I have just changed clustergit to have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt; as default branch name instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;master&lt;/code&gt; (github having set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt; as the new standard)&lt;/p&gt;

&lt;p&gt;Rename everything from master to main&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Git pull, push&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I am not sure I will use it. But allows to recursively launch pull commands to update repos (if no local changes)&lt;/p&gt;

&lt;h2 id=&quot;rename-branches-from-main-to-master&quot;&gt;Rename branches from &lt;em&gt;main&lt;/em&gt; to &lt;em&gt;master&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.github.com/en/github/administering-a-repository/renaming-a-branch&quot;&gt;Renaming a branch&lt;/a&gt; from github website.&lt;/p&gt;

&lt;p&gt;Rename branch main to master from github website&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.github.com/assets/images/help/branches/branches-link.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Update local clones&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; main master
git fetch origin
git branch &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; origin/master master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;rename-branches-from-master-to-main-i-know&quot;&gt;Rename branches from master to &lt;em&gt;main&lt;/em&gt; (I know)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.github.com/en/github/administering-a-repository/renaming-a-branch&quot;&gt;Renaming a branch&lt;/a&gt; from github website.&lt;/p&gt;

&lt;p&gt;Rename branch master to main from github website&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.github.com/assets/images/help/branches/branches-link.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Update local clones&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git branch &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; master main
git fetch origin
git branch &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; origin/main main
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;rabbitvcs&quot;&gt;RabbitVCS&lt;/h2&gt;

&lt;p&gt;From this &lt;a href=&quot;https://www.addictivetips.com/ubuntu-linux-tips/integrate-git-with-gnome-file-manager-on-linux/&quot;&gt;page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Installation&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;rabbitvcs-nautilus
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Result&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.addictivetips.com/wp-content/uploads/2018/10/rvcs-update-e1540364222288.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These overlay icons are not automatically updated (have to hit Ctrl-F5, it is a cache issue?) Which is not a surprise: number of actions are fired based on file modifications, and here status (commited, pushed) is not at all linked to file modifications. The system doesn’t know that overlay icon should be changed because file was not touched.&lt;/p&gt;

&lt;h2 id=&quot;git-nautilus-icons&quot;&gt;git-nautilus-icons&lt;/h2&gt;

&lt;p&gt;Just to check if it works better than RabbitVCS regarding overlay icon cache issue.&lt;/p&gt;

&lt;p&gt;No I didn’t manage to make it work. Back to RabbitVCS.&lt;/p&gt;

&lt;h2 id=&quot;activate-git-with-globalprotect&quot;&gt;Activate git with GlobalProtect&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;move from ssh to https, keeping password&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git remote &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt;
origin  git@github.com:castorfou/guillaume_blog.git &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;fetch&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
origin  git@github.com:castorfou/guillaume_blog.git &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;push&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;move to https://github.com/castorfou/guillaume_blog.git&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git remote set-url origin https://github.com/castorfou/guillaume_blog.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Make Git store the username and password and it will never ask for them.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config &lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt; credential.helper store
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Save the username and password for a session (cache it);&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git config &lt;span class=&quot;nt&quot;&gt;--global&lt;/span&gt; credential.helper cache
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and to activate trace&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ GIT_TRACE_PACKET&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;GIT_TRACE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;GIT_CURL_VERBOSE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 git fetch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we can enrich certificates with Global Protect CA&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/anaconda3/ssl&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo cp &lt;/span&gt;certPG.pem /etc/ssl/certs/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;add-a-ca-certificate-in-ubuntu&quot;&gt;Add a ca-certificate in ubuntu&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/usr/local/share/ca-certificates/&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Create a new folder, i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo mkdir school&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Copy the . crt file into the school folder.&lt;/li&gt;
  &lt;li&gt;Make sure the permissions are OK (755 for the folder, 644 for the file)&lt;/li&gt;
  &lt;li&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo update-ca-certificates&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We should see effects in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/ssl/certs&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/etc/ssl/certs&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ll &lt;span class=&quot;nt&quot;&gt;-tr&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;..]
lrwxrwxrwx 1 root root     86 mars  24 10:02  cert_M_X5C_sase-net-sslfwd-trust-ca.pem -&amp;gt; /usr/local/share/ca-certificates/globalprotect/cert_M_X5C_sase-net-sslfwd-trust-ca.crt
lrwxrwxrwx 1 root root     39 mars  24 10:02  0dc7de9e.0 -&amp;gt; cert_M_X5C_sase-net-sslfwd-trust-ca.pem
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="git" /><summary type="html">Very basic question to help keep my repo clean.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://git-scm.com/images/logos/downloads/Git-Icon-1788C.png" /><media:content medium="image" url="https://git-scm.com/images/logos/downloads/Git-Icon-1788C.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introduction to Reinforcement Learning with David Silver</title><link href="https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html" rel="alternate" type="text/html" title="Introduction to Reinforcement Learning with David Silver" /><published>2021-03-09T00:00:00-06:00</published><updated>2021-03-09T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/Introduction%20to%20Reinforcement%20Learning%20with%20David%20Silver</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html">&lt;p&gt;This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver&quot;&gt;Website with 10 lectures: videos and slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/castorfou/introduction-reinforcement-learning-david-silver&quot;&gt;My repo with slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_sylabus.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3921---lecture-1-introduction-to-reinforcement-learning&quot;&gt;3/9/21 - Lecture 1: Introduction to Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;This introduction is essentially about giving examples of RL to have a good intuition about this field and to provide definitions or context:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Definitions: rewards, actions, agent, environment, state (and history)&lt;/li&gt;
  &lt;li&gt;Major components: policy, value function, model&lt;/li&gt;
  &lt;li&gt;Categorizing RL agents (taxonomy): value based, policy based, actor critic, model free, model based&lt;/li&gt;
  &lt;li&gt;Learning and planning&lt;/li&gt;
  &lt;li&gt;Prediction and control&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And David gives 2 references:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;well known&lt;/a&gt; Introduction to Reinforcement Learning, Sutton and Barto, 1998&lt;/li&gt;
  &lt;li&gt;Algorithms for Reinforcement Learning, Szepesvari. Available &lt;a href=&quot;http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf&quot;&gt;online&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(s): essentially a map from state to action. Can be deterministic &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(s) or stochastic &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(a|s).&lt;/p&gt;

&lt;p&gt;Value function v&lt;sub&gt;$\pi$&lt;/sub&gt;(s): is a prediction of expected future reward.&lt;/p&gt;

&lt;p&gt;Model: it is not the environment itself but useful to predict what the environment will do next. 2 types of models: transitions model and rewards model. Transition model predicts the next state (e.g. based on dynamics). Reward model predicts the next immediate reward.&lt;/p&gt;

&lt;p&gt;A lot of algorithms are model-free and doesn’t require these models. It is a fundamental distinctions in RL.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec1_taxonomy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then David explains 2 fundamental different problems with Learning vs Planning.&lt;/p&gt;

&lt;p&gt;With Learning, environment is unknown, agent interacts directly with the environment and improves its policy.&lt;/p&gt;

&lt;p&gt;With Planning, a model of environment is known, and agent “plays” with this model and improves its policy.&lt;/p&gt;

&lt;p&gt;These 2 problems may be linked where you start to learn from the environment and apply planning then.&lt;/p&gt;

&lt;p&gt;2 examples based on atari games.&lt;/p&gt;

&lt;p&gt;Another topic is exploration vs exploitation then prediction and control.&lt;/p&gt;

&lt;h2 id=&quot;31021---lecture-2-markov-decision-processes&quot;&gt;3/10/21 - Lecture 2: Markov Decision Processes&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Markov decision processes&lt;/em&gt;&lt;/strong&gt; formally describe an environment for reinforcement learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Markov property&lt;/em&gt;&lt;/strong&gt;: the future is independent of the past given the present.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Markov Process&lt;/em&gt;&lt;/strong&gt; (or &lt;strong&gt;&lt;em&gt;Markov Chain&lt;/em&gt;&lt;/strong&gt;) is the tuple (S, P)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_markovchain.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can take sample episodes from this chain. (e.g. C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep)&lt;/p&gt;

&lt;p&gt;We can formalize the transition matrix from s to s’.&lt;/p&gt;

&lt;p&gt;When you add reward you get &lt;strong&gt;&lt;em&gt;Markov reward process&lt;/em&gt;&lt;/strong&gt; (S, P, R, &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;Reward here is a function to map for each state the immediate reward.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is the discounted factor, &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; [0,1]. David explains why we could need such discount.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Return&lt;/em&gt;&lt;/strong&gt; Gt is the total discounted reward at time-step t for a given sample.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_return.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Value function&lt;/em&gt;&lt;/strong&gt; v(s) is really what we care about, it is the long-term value of state s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_valuefunction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Bellman Equation for MRPs&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The value function can be decomposed into two parts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;immediate reward R&lt;sub&gt;t+1&lt;/sub&gt;&lt;/li&gt;
  &lt;li&gt;discounted value of next state &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.v (S&lt;sub&gt;t+1&lt;/sub&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_bellman.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We use that to calculate value function with &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; $\neq$ 0.&lt;/p&gt;

&lt;p&gt;And calculating value function can be seen as the resolution of this linear equation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_bellman_solving.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And now we introduce actions and it gives &lt;strong&gt;&lt;em&gt;Markov Decision Process&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_mdp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we introduce policy&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_policy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then we can define the &lt;em&gt;state-value function&lt;/em&gt; v&lt;sub&gt;$\pi$&lt;/sub&gt;(s,a) for a given policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_statevaluefunction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and &lt;em&gt;action-value function&lt;/em&gt; q&lt;sub&gt;$\pi$&lt;/sub&gt;(s,a) for a given policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_actionvaluefunction.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And impact on Bellman Equation ends like that:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec2_bellman_mdp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;v is giving us how good it is to be in a state. q is giving us how good is it to take an action.&lt;/p&gt;

&lt;p&gt;And then we have the Bellman equation expressed with v and q.&lt;/p&gt;

&lt;p&gt;We don’t care much about a given v&lt;sub&gt;$\pi$&lt;/sub&gt;, we want to get the best policy. And ultimately to get q&lt;sub&gt;*&lt;/sub&gt; which is the &lt;strong&gt;optimal action value function&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/home/explore/git/guillaume/blog/images/deepmind_lec2_optimal_value_function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The optimal value function specifies the best possible performance in the MDP.
A MDP is “solved” when we know the optimal value function q&lt;sub&gt;*&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;What we really care about is &lt;strong&gt;optimal policy&lt;/strong&gt; &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;sub&gt;*&lt;/sub&gt;. There is a partial ordering about policies. And a theorem saying that for any MDP, there exists at least one optimal policy.&lt;/p&gt;

&lt;p&gt;So the optimal value function calculation is similar to what we did earlier when we averaged the value of the next state but now we take the max instead of average.&lt;/p&gt;

&lt;p&gt;So no we can write the &lt;strong&gt;Bellman Optimality Equation&lt;/strong&gt;. Unfortunately this is non-linear.&lt;/p&gt;

&lt;p&gt;There are many approaches such as iterative ones.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Value Iteration&lt;/li&gt;
  &lt;li&gt;Policy Iteration&lt;/li&gt;
  &lt;li&gt;Q-learning&lt;/li&gt;
  &lt;li&gt;Sarsa&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;31221---lecture-3-planning-by-dynamic-programming&quot;&gt;3/12/21 - Lecture 3: Planning by Dynamic Programming&lt;/h2&gt;

&lt;p&gt;Will discuss from the agent side: how to solve these MDP problems.&lt;/p&gt;

&lt;p&gt;David starts with general ideas on dynamic programming. (programming in a sense of policy)&lt;/p&gt;

&lt;p&gt;Value function is an important idea for RL because it sotres valuable information that you can later reuse (it embeds solutions). And Bellman equation gives the recursive decomposition.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Planning by Dynamic Programming&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We assume full knowledge of the MDP. Dynamic programming is used for planning in an MDP. With 2 usages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;prediction: given MDP and policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, we predict the value of this policy v&lt;sub&gt;$\pi$&lt;/sub&gt;.&lt;/li&gt;
  &lt;li&gt;control: given MDP, we get optimal value function v&lt;sub&gt;&amp;amp;ast;&lt;/sub&gt; and optimal policy $\pi$&lt;sub&gt;&amp;amp;ast;&lt;/sub&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And by full MDP it would mean for an atari game to have access to internal code to calculate everything.&lt;/p&gt;

&lt;p&gt;We need the 2 aspects to solve MDP: prediction to value policy, and control to get the best one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy Evaluation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Problem: evaluate a given policy π
Solution: iterative application of Bellman expectation backup&lt;/p&gt;

&lt;p&gt;(Bellman expectation is used in prediction, Bellman optimality is used in control)&lt;/p&gt;

&lt;p&gt;David takes an example with a small grid-world and calculates iteratively (k=0, 1, 2, …) v(s) for a uniform random policy (north, south, east, west with prob 0.25) (left column). And then we follow policy greedily using v function. (right column)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy Iteration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In small grid-world example, just by evaluating the policy and act greedily were sufficient to get the optimal policy. This is not generally the case. In general, need more iterations of  evaluation (iterative policy evaluation) / improvement (greedy policy).
But this process of policy iteration always converges to π∗&lt;/p&gt;

&lt;p&gt;David uses Jack’s Car Rental where it needs 4 steps to get the optimal policy.  And explains why acting greedy improves the policy.       And if improvement stops, Bellman optimality equation is satisfied, we have our optimal policy.&lt;/p&gt;

&lt;p&gt;Some question then about convergence of v&lt;sub&gt;$\pi$&lt;/sub&gt; . Why not update policy at each step of evaluation -&amp;gt; this is value iteration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value Iteration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Problem: find optimal policy π
Solution: iterative application of Bellman optimality backup&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Extensions to dynamic programming&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DP uses full-width backups. It is effective for medium-sized problems. Curse of dimensionality for large problems. Even one backup can be too expensive.&lt;/p&gt;

&lt;p&gt;One solution is to &lt;strong&gt;sample backups&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Advantages:
Model-free: no advance knowledge of MDP required
Breaks the curse of dimensionality through sampling
Cost of backup is constant, independent of n = |S|&lt;/p&gt;

&lt;h2 id=&quot;31521---lecture-4-model-free-prediction&quot;&gt;3/15/21 - Lecture 4: Model-Free Prediction&lt;/h2&gt;

&lt;p&gt;Model-Free: no-one gives us the MDP. And we still want to solve it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Monte-Carlo learning&lt;/strong&gt;: basically methods which goes all the way to the end of trajectory and estimates value by looking at sample returns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Temporal-Difference learning&lt;/strong&gt;: goes one step ahead and estimates after one step&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TD(&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\lambda&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;λ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;)&lt;/strong&gt;: unify both approaches&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We give up the assumption giving how the environment works (which is highly unrealistic for interesting problems). We break it down in 2 pieces (as with previous lecture with planning):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;policy evaluation case (this lecture) - how much reward we get from that policy (in model-free envt)&lt;/li&gt;
  &lt;li&gt;control (next lecture) - find the optimum value function and then optimum policy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Monte-Carlo Reinforcement Learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We go all the way through the episodes and we take sample returns. So the estimated value function can be the average of all returns. You have to terminate to perform this mean.&lt;/p&gt;

&lt;p&gt;It means we use the &lt;em&gt;empirical mean return&lt;/em&gt; in place of &lt;em&gt;expected return&lt;/em&gt;. (by &lt;em&gt;law of large numbers&lt;/em&gt;, this average returns will converge to value function as the number of episodes for that state tends to infinity)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal-Difference Reinforcement Learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TD learns from incomplete episodes, by bootstrapping&lt;/p&gt;

&lt;p&gt;David takes an example from Sutton about predicting time to commute home, comparing MC and TD.&lt;/p&gt;

&lt;p&gt;TD target (R&lt;sub&gt;t+1&lt;/sub&gt;+&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\gamma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05556em;&quot;&gt;γ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;V&lt;sub&gt;t+1&lt;/sub&gt;) is biased estimate of v&lt;sub&gt;$\pi$&lt;/sub&gt;(S&lt;sub&gt;t&lt;/sub&gt;), but has lower variance than the return G&lt;sub&gt;t&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;David compares perf of MC, TD(0), … using Random Walk example and different values of &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;31821---lecture-5-model-free-control&quot;&gt;3/18/21 - Lecture 5: Model-Free Control&lt;/h2&gt;

&lt;p&gt;Distinction between on-policy (learning by doing the job) and off-policy (following someone else behavior)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;on-policy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Monte-Carlo approach, we have 2 issues. First is that we don’t have access to model so we should use Q(s, a) instead of v(s). Second is lack of exploration so we should use &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-greedy policy.&lt;/p&gt;

&lt;p&gt;With GLIE (Greedy in the Limit with Infinite Exploration), we can update Q after each episodes.&lt;/p&gt;

&lt;p&gt;We will now use TD:&lt;/p&gt;

&lt;p&gt;Natural idea: use TD instead of MC in our control loop&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apply TD to Q(S, A)&lt;/li&gt;
  &lt;li&gt;Use &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-greedy policy improvement&lt;/li&gt;
  &lt;li&gt;Update every time-step&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is SARSA update. Every single time-step we update our diagram.&lt;/p&gt;

&lt;p&gt;A generalisation is n-step Sarsa. n=1 is standard Sarsa. n=&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\infty&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∞&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; is MC.&lt;/p&gt;

&lt;p&gt;To get the best of both worlds, we consider Sarsa(&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\lambda&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;λ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;). We have a forward version&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/home/explore/git/guillaume/blog/images/deepmind_lec5_sarsal.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And a backward version which allows online experience. Thanks to eligibility traces.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;off-policy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Why is this important?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Learn from observing humans or other agents&lt;/li&gt;
  &lt;li&gt;Re-use experience generated from old policies π 1 , π 2 , …, π t−1&lt;/li&gt;
  &lt;li&gt;Learn about optimal policy while following exploratory policy&lt;/li&gt;
  &lt;li&gt;Learn about multiple policies while following one policy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can apply it in importance sampling for off-policy. With Monte-Carlo it is however useless due to high variance. It is imperative to to TD.&lt;/p&gt;

&lt;p&gt;We can apply that to Q-learning. We can use greedy slection on target policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; greedy on behaviour policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\mu&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;μ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;33121---lecture-6-value-function-approximation&quot;&gt;3/31/21 - Lecture 6: Value Function Approximation&lt;/h2&gt;

&lt;p&gt;How to scale up value function approach.&lt;/p&gt;</content><author><name></name></author><category term="reinforcement learning" /><category term="deepmind" /><summary type="html">This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/RL.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/RL.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Use of gpg under linux</title><link href="https://castorfou.github.io/guillaume_blog/blog/gpg-linux.html" rel="alternate" type="text/html" title="Use of gpg under linux" /><published>2021-03-03T00:00:00-06:00</published><updated>2021-03-03T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/gpg-linux</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/gpg-linux.html">&lt;p&gt;from &lt;a href=&quot;https://www.fosslinux.com/27018/best-ways-to-encrypt-files-in-linux.htm&quot;&gt;best ways to encrypt files on linux&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gpg&quot;&gt;gpg&lt;/h2&gt;

&lt;h4 id=&quot;setup-the-key&quot;&gt;setup the key&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--gen-key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and enter a strong passphrase.&lt;/p&gt;

&lt;h4 id=&quot;export-public-key&quot;&gt;export public key&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--armor&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; mypubkey.gpg &lt;span class=&quot;nt&quot;&gt;--export&lt;/span&gt; &amp;lt;E-mail that you registered&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;import-from-windows-box&quot;&gt;import from windows box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--import&lt;/span&gt; mypubkey.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;encrypt-files-from-windows-box&quot;&gt;encrypt files from windows box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; test.txt.gpg &lt;span class=&quot;nt&quot;&gt;--encrypt&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--recipient&lt;/span&gt; &amp;lt;Receiver&lt;span class=&quot;s1&quot;&gt;'s E-Mail ID&amp;gt; test.txt
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;decrypt-files-on-linux-box&quot;&gt;decrypt files on linux box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; test.txt &lt;span class=&quot;nt&quot;&gt;--decrypt&lt;/span&gt; test.txt.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;find--gpg--tmpfs&quot;&gt;find + gpg + tmpfs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;encrypt from Windows&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'df_76*.csv'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-exec&lt;/span&gt; gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;.gpg &lt;span class=&quot;nt&quot;&gt;--encrypt&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--recipient&lt;/span&gt; guillaume.ramelet@michelin.com &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;decrypt from Linux&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There should be better ways to do it.&lt;/p&gt;

&lt;p&gt;Here is my process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Before starting: call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mount_decrypt.sh&lt;/code&gt;. It mounts a tmpfs in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secured_data/data&lt;/code&gt;, and decrypt all gpg files to this directory&lt;/li&gt;
  &lt;li&gt;
    &lt;work&gt;
&lt;/work&gt;
  &lt;/li&gt;
  &lt;li&gt;After work is done: call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;umount_decrypt.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpg_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;gpg_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;src_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;gpg_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;%.*&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;gpg decrypt &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$gpg_name&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; -&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$src_name&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$src_name&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--decrypt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$gpg_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mount_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;GPG_DEC_CMD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/gpg_decrypt.sh
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mount &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; tmpfs &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1G tmpfs &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /media/explore/CHACLEF/janus
find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'df_76*.csv.gpg'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-exec&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$GPG_DEC_CMD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;umount_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;umount &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="gpg" /><summary type="html">from best ways to encrypt files on linux</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/secure.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/secure.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Logbook for March 21</title><link href="https://castorfou.github.io/guillaume_blog/blog/logbook-March.html" rel="alternate" type="text/html" title="Logbook for March 21" /><published>2021-03-01T00:00:00-06:00</published><updated>2021-03-01T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/logbook-March</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/logbook-March.html">&lt;h2 id=&quot;week-9---mar-21&quot;&gt;Week 9 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Deep Generative Modeling (lecture 4) - vaes and gans.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; De-biasing Facial Recognition Systems (lab 2): CNN, VAE, DB-VAE&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Approximations non linéaires et réseaux de neurones (lecture 4)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver&quot;&gt;RL Course by David Silver&lt;/a&gt; lecture 1 - intro (22’/88’)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ilp.mit.edu/attend/future-manufacturing-mit?utm_medium=email&amp;amp;utm_campaign=Future%20of%20Manu%2032-3-4%20day%20of&amp;amp;utm_content=Future%20of%20Manu%2032-3-4%20day%20of+CID_523e5a27df7d82b6ebf742ac50bdef62&amp;amp;utm_source=Email%20campaign&amp;amp;utm_term=SEE%20THE%20AGENDA&quot;&gt;Future of Manufacturing@MIT&lt;/a&gt; - interesting landscape about Manufacturing and AI&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt; by Christoph Molnar. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/lime.html#lime&quot;&gt;LIME&lt;/a&gt; reading to understand context of local surrogate models. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/shap.html&quot;&gt;SHAP&lt;/a&gt; chapter using Janus data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 3 v1) on DQN with temporal limitation using LSTM, and experience replay. (replay buffer)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thursday 3/4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt; by Christoph Molnar. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/pdp.html&quot;&gt;PDP&lt;/a&gt; chapter using Janus data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 3/5&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p220-223) - full vs sample backups, trajectory sampling, heuristic search&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p223+) - start of Approximate Solution Methods, why to use NN.&lt;/p&gt;

&lt;h2 id=&quot;week-10---mar-21&quot;&gt;Week 10 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/8&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Deep Reinforcement Learning. Q-learning vs Policy Gradient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/9&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Ondelettes et échantillonnage (lecture 5)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Introduction to Reinforcement Learning (lecture 1)&lt;/p&gt;

&lt;p&gt;Installation of &lt;a href=&quot;/guillaume_blog/blog/clustergit.html&quot;&gt;clustergit&lt;/a&gt; to detect local (=uncommited  or unpushed) changes in repo&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/10&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 4 v1) on four strategies to improve DQN (fixed Q-targets, double DQN, dueling DQN (DDQN), Prioritized Experience Replay (PER))&lt;/p&gt;

&lt;p&gt;t-SNE using Janus data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Markov Decision Processes (lecture 2)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 3/12&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p287-352) - Applications and case studies, end of the book&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Planning by Dynamic Programming (lecture 3)&lt;/p&gt;

&lt;h2 id=&quot;week-11---mar-21&quot;&gt;Week 11 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/15&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Limitations and New Frontiers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Pixels-to-Control Learning (lab 3): Cartpole and Pong&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Model-Free Prediction (lecture 4)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/16&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Multi-résolutions (lecture 6)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/17&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 5 v1)  - Policy Gradient&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thursday 3/18&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 5 v1)  - Policy Gradient notebooks&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Model-Free Control (lecture 5)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 3/19&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 6 v1)  - Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C)&lt;/p&gt;

&lt;p&gt;College de France - l’apprentissage profond par Yann Lecunn 2016 - &lt;a href=&quot;https://www.college-de-france.fr/site/yann-lecun/course-2016-02-12-14h30.htm&quot;&gt;Pourquoi l’apprentissage profond ?&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;week-12---mar-21&quot;&gt;Week 12 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/22&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Evidential Deep Learning and Uncertainty (lecture 7).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (v1 Part 5)  - Advantage Actor Critic (A2C)  - implementation and video&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/23&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Bases orthonormales d’ondelettes (lecture 7)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/24&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; (Chapter 7 v1)  - Proximal Policy Optimization PPO&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/stable-baselines-3.html&quot;&gt;Stable baselines 3&lt;/a&gt; - init and 1st tutorial&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thursday 3/25&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;setup &lt;a href=&quot;/guillaume_blog/blog/headless-raspberry-pi-bridge-network.html&quot;&gt;headless raspberry pi&lt;/a&gt; to bridge wifi (tethering from phone) to ethernet (to wifi-router)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/stable-baselines-3.html&quot;&gt;Stable baselines 3&lt;/a&gt; - finalize init and go through documentation&lt;/p&gt;

&lt;p&gt;Create a &lt;a href=&quot;/guillaume_blog/blog/contribute-to-a-project-with-git.html&quot;&gt;patch for a github&lt;/a&gt; project (by forking and pulling request)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 3/26&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/stable-baselines-3.html&quot;&gt;Stable baselines 3&lt;/a&gt; - Documentation &amp;gt; Examples&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/clustergit.html&quot;&gt;Rename my branches&lt;/a&gt; named &lt;em&gt;master&lt;/em&gt; to &lt;em&gt;main&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;week-13---mar-21&quot;&gt;Week 13 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/29&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Bias and Fairness (lecture 8).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/31&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Parcimonie et compression d’images (lecture 8)&lt;/p&gt;</content><author><name></name></author><category term="logbook" /><summary type="html">Week 9 - Mar 21</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>