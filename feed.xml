<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://castorfou.github.io/guillaume_blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://castorfou.github.io/guillaume_blog/" rel="alternate" type="text/html" /><updated>2021-03-09T04:01:12-06:00</updated><id>https://castorfou.github.io/guillaume_blog/feed.xml</id><title type="html">Guillaume’s blog</title><subtitle>Journey for a datascientist</subtitle><entry><title type="html">Introduction to Reinforcement Learning with David Silver</title><link href="https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html" rel="alternate" type="text/html" title="Introduction to Reinforcement Learning with David Silver" /><published>2021-03-09T00:00:00-06:00</published><updated>2021-03-09T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/Introduction%20to%20Reinforcement%20Learning%20with%20David%20Silver</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html">&lt;p&gt;This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver&quot;&gt;Website with 10 lectures: videos and slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/castorfou/introduction-reinforcement-learning-david-silver&quot;&gt;My repo with slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_sylabus.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3921---lecture-1-introduction-to-reinforcement-learning&quot;&gt;3/9/21 - Lecture 1: Introduction to Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;This introduction is essentially about giving examples of RL to have a good intuition about this field and to provide definitions or context:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Definitions: rewards, actions, agent, environment, state (and history)&lt;/li&gt;
  &lt;li&gt;Major components: policy, value function, model&lt;/li&gt;
  &lt;li&gt;Categorizing RL agents (taxonomy): value based, policy based, actor critic, model free, model based&lt;/li&gt;
  &lt;li&gt;Learning and planning&lt;/li&gt;
  &lt;li&gt;Prediction and control&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And David gives 2 references:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;well known&lt;/a&gt; Introduction to Reinforcement Learning, Sutton and Barto, 1998&lt;/li&gt;
  &lt;li&gt;Algorithms for Reinforcement Learning, Szepesvari. Available &lt;a href=&quot;http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf&quot;&gt;online&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Policy &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(s): essentially a map from state to action. Can be deterministic &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(s) or stochastic &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(a&lt;/td&gt;
      &lt;td&gt;s).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Value function v&lt;sub&gt;$\pi$&lt;/sub&gt;(s): is a prediction of expected future reward.&lt;/p&gt;

&lt;p&gt;Model: it is not the environment itself but useful to predict what the environment will do next. 2 types of models: transitions model and rewards model. Transition model predicts the next state (e.g. based on dynamics). Reward model predicts the next immediate reward.&lt;/p&gt;

&lt;p&gt;A lot of algorithms are model-free and doesn’t require these models. It is a fundamental distinctions in RL.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/deepmind_lec1_taxonomy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then David explains 2 fundamental different problems with Learning vs Planning.&lt;/p&gt;

&lt;p&gt;With Learning, environment is unknown, agent interacts directly with the environment and improves its policy.&lt;/p&gt;

&lt;p&gt;With Planning, a model of environment is known, and agent “plays” with this model and improves its policy.&lt;/p&gt;

&lt;p&gt;These 2 problems may be linked where you start to learn from the environment and apply planning then.&lt;/p&gt;

&lt;p&gt;2 examples based on atari games.&lt;/p&gt;

&lt;p&gt;Another topic is exploration vs exploitation then prediction and control.&lt;/p&gt;</content><author><name></name></author><category term="reinforcement learning" /><category term="deepmind" /><summary type="html">This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/RL.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/RL.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Use of gpg under linux</title><link href="https://castorfou.github.io/guillaume_blog/blog/gpg-linux.html" rel="alternate" type="text/html" title="Use of gpg under linux" /><published>2021-03-03T00:00:00-06:00</published><updated>2021-03-03T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/gpg-linux</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/gpg-linux.html">&lt;p&gt;from &lt;a href=&quot;https://www.fosslinux.com/27018/best-ways-to-encrypt-files-in-linux.htm&quot;&gt;best ways to encrypt files on linux&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gpg&quot;&gt;gpg&lt;/h2&gt;

&lt;h4 id=&quot;setup-the-key&quot;&gt;setup the key&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--gen-key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and enter a strong passphrase.&lt;/p&gt;

&lt;h4 id=&quot;export-public-key&quot;&gt;export public key&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--armor&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; mypubkey.gpg &lt;span class=&quot;nt&quot;&gt;--export&lt;/span&gt; &amp;lt;E-mail that you registered&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;import-from-windows-box&quot;&gt;import from windows box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--import&lt;/span&gt; mypubkey.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;encrypt-files-from-windows-box&quot;&gt;encrypt files from windows box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; test.txt.gpg &lt;span class=&quot;nt&quot;&gt;--encrypt&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--recipient&lt;/span&gt; &amp;lt;Receiver&lt;span class=&quot;s1&quot;&gt;'s E-Mail ID&amp;gt; test.txt
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;decrypt-files-on-linux-box&quot;&gt;decrypt files on linux box&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; test.txt &lt;span class=&quot;nt&quot;&gt;--decrypt&lt;/span&gt; test.txt.gpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;find--gpg--tmpfs&quot;&gt;find + gpg + tmpfs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;encrypt from Windows&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'df_76*.csv'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-exec&lt;/span&gt; gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;.gpg &lt;span class=&quot;nt&quot;&gt;--encrypt&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--recipient&lt;/span&gt; guillaume.ramelet@michelin.com &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;decrypt from Linux&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There should be better ways to do it.&lt;/p&gt;

&lt;p&gt;Here is my process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Before starting: call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mount_decrypt.sh&lt;/code&gt;. It mounts a tmpfs in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secured_data/data&lt;/code&gt;, and decrypt all gpg files to this directory&lt;/li&gt;
  &lt;li&gt;
    &lt;work&gt;
&lt;/work&gt;
  &lt;/li&gt;
  &lt;li&gt;After work is done: call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;umount_decrypt.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpg_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;gpg_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;src_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;gpg_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;%.*&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;gpg decrypt &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$gpg_name&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; -&amp;gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$src_name&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
gpg &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$src_name&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--decrypt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$gpg_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;base&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mount_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;GPG_DEC_CMD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/gpg_decrypt.sh
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mount &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; tmpfs &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1G tmpfs &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /media/explore/CHACLEF/janus
find &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'df_76*.csv.gpg'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-exec&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$GPG_DEC_CMD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;umount_decrypt.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;TARGET_DATA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/explore/git/guillaume/d059/secured_data/data
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;umount &lt;span class=&quot;nv&quot;&gt;$TARGET_DATA&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="gpg" /><summary type="html">from best ways to encrypt files on linux</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/secure.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/secure.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Logbook for March 21</title><link href="https://castorfou.github.io/guillaume_blog/blog/logbook-March.html" rel="alternate" type="text/html" title="Logbook for March 21" /><published>2021-03-01T00:00:00-06:00</published><updated>2021-03-01T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/logbook-March</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/logbook-March.html">&lt;h2 id=&quot;week-9---mar-21&quot;&gt;Week 9 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Deep Generative Modeling (lecture 4) - vaes and gans.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; De-biasing Facial Recognition Systems (lab 2): CNN, VAE, DB-VAE&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Approximations non linéaires et réseaux de neurones (lecture 4)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver&quot;&gt;RL Course by David Silver&lt;/a&gt; lecture 1 - intro (22’/88’)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ilp.mit.edu/attend/future-manufacturing-mit?utm_medium=email&amp;amp;utm_campaign=Future%20of%20Manu%2032-3-4%20day%20of&amp;amp;utm_content=Future%20of%20Manu%2032-3-4%20day%20of+CID_523e5a27df7d82b6ebf742ac50bdef62&amp;amp;utm_source=Email%20campaign&amp;amp;utm_term=SEE%20THE%20AGENDA&quot;&gt;Future of Manufacturing@MIT&lt;/a&gt; - interesting landscape about Manufacturing and AI&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wednesday 3/3&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt; by Christoph Molnar. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/lime.html#lime&quot;&gt;LIME&lt;/a&gt; reading to understand context of local surrogate models. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/shap.html&quot;&gt;SHAP&lt;/a&gt; chapter using Janus data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/an-introduction-to-deep-reinforcement-learning.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt; on DQN with temporal limitation using LSTM, and experience replay. (replay buffer)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thursday 3/4&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/&quot;&gt;Interpretable Machine Learning&lt;/a&gt; by Christoph Molnar. &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/pdp.html&quot;&gt;PDP&lt;/a&gt; chapter using Janus data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 3/5&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p220-223) - full vs sample backups, trajectory sampling, heuristic search&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p223+) - start of Approximate Solution Methods, why to use NN.&lt;/p&gt;

&lt;h2 id=&quot;week-10---mar-21&quot;&gt;Week 10 - Mar 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 3/8&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt; Deep Reinforcement Learning. Q-learning vs Policy Gradient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuesday 3/9&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt; Ondelettes et échantillonnage (lecture 5)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html&quot;&gt;RL Course by David Silver&lt;/a&gt; Introduction to Reinforcement Learning (lecture 1)&lt;/p&gt;</content><author><name></name></author><category term="logbook" /><summary type="html">Week 9 - Mar 21</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Logbook for February 21</title><link href="https://castorfou.github.io/guillaume_blog/blog/logbook-Februrary.html" rel="alternate" type="text/html" title="Logbook for February 21" /><published>2021-02-26T00:00:00-06:00</published><updated>2021-02-26T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/logbook-Februrary</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/logbook-Februrary.html">&lt;p&gt;This is a test. I will try to keep words on a monthly (this page), weekly (per heading), daily basis. Just some short entries with possibly some links to more detailed materials.&lt;/p&gt;

&lt;h2 id=&quot;week-8---feb-21&quot;&gt;Week 8 - Feb 21&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Monday 2/22&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To develop knowledge about RL, here is my learning process on a weekly basis.&lt;/p&gt;

&lt;p&gt;Monday &lt;a href=&quot;/guillaume_blog/blog/learning-MIT-6.S191-2021.html&quot;&gt;MIT 6S191&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tuesday &lt;a href=&quot;/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html&quot;&gt;College de France&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Wednesday &lt;a href=&quot;/guillaume_blog/blog/an-introduction-to-deep-reinforcement-learning.html&quot;&gt;Deep Reinforcement Learning by Thomas Simonini&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Friday &lt;a href=&quot;https://github.com/castorfou/datascience-papers&quot;&gt;RL readings&lt;/a&gt;: papers, books, …&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Friday 2/26&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;blog fastpages - setup automated upgrade (instructions from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_fastpages_docs&lt;/code&gt;) v2.1.42&lt;/p&gt;

&lt;p&gt;blog fastpages - display image preview (update of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;RL - understood differences between Q-learning and Sarsa algorithms in &lt;a href=&quot;/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html&quot;&gt;end of step2 part2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RL - &lt;a href=&quot;/guillaume_blog/blog/reinforcement-learning-readings.html&quot;&gt;Sutton&lt;/a&gt; book (p200-220) - eligibility traces, and start of planning vs learning&lt;/p&gt;</content><author><name></name></author><category term="logbook" /><summary type="html">This is a test. I will try to keep words on a monthly (this page), weekly (per heading), daily basis. Just some short entries with possibly some links to more detailed materials.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/logbook.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Practicing: Deep Reinforcement Learning Course by Thomas Simonini</title><link href="https://castorfou.github.io/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html" rel="alternate" type="text/html" title="Practicing: Deep Reinforcement Learning Course by Thomas Simonini" /><published>2021-02-19T00:00:00-06:00</published><updated>2021-02-19T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/Deep%20Reinforcement%20Learning%20Course%20by%20Thomas%20Simonini</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html">&lt;p&gt;A course by &lt;a href=&quot;https://www.simoninithomas.com/&quot;&gt;Thomas Simonini&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://simoninithomas.github.io/Deep_reinforcement_learning_Course/&quot;&gt;Syllabus (from 2018)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/deep-reinforcement-learning-course/launching-deep-reinforcement-learning-course-v2-0-38fa3c24bcbc&quot;&gt;kind of syllabus (from 2020)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Everything available in &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course&quot;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I appreciate the effort to update examples, and some 2018 implementations became obsolete. Historical Atari VC2600 games are now Starcraft 2 or minecraft, and news series on building AI for video games in Unity and Unreal Engine..&lt;/p&gt;

&lt;h2 id=&quot;21921---step-1---an-introduction-to-deep-reinforcement-learning&quot;&gt;(2/19/21) - &lt;a href=&quot;https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c&quot;&gt;Step 1&lt;/a&gt; - An Introduction to Deep Reinforcement Learning?&lt;/h2&gt;

&lt;p&gt;Previous version from &lt;a href=&quot;https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419&quot;&gt;2018: What is Deep Reinforcement Learning?&lt;/a&gt; is quite interesting. With 3 parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What Reinforcement Learning is, and how rewards are the central idea&lt;/li&gt;
  &lt;li&gt;The three approaches of Reinforcement Learning&lt;/li&gt;
  &lt;li&gt;What the “Deep” in Deep Reinforcement Learning means&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*aKYFRoEmmKkybqJOvLt2JQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rewards, long-term future reward, discount rate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*zrzRTXt8rtWF5fX__kZ-yQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Episodic (starting and ending point) vs Continuous (e.g. stock trading) tasks.&lt;/p&gt;

&lt;p&gt;Way of learning: Monte Carlo (MC: rewards collected at the end of an episode) vs Temporal Difference (TD: estimate rewards at each step)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*LLfj11fivpkKZkwQ8uPi3A.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exploration/Exploitation trade off. Will see later different ways to handle that trade-off.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*APLmZ8CVgu0oY3sQBVYIuw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;three-approaches-to-reinforcement-learning&quot;&gt;Three approaches to Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;These are value-based, policy-based, and model-based.&lt;/p&gt;

&lt;h4 id=&quot;value-based&quot;&gt;Value Based&lt;/h4&gt;

&lt;p&gt;In value-based RL, the goal is to optimize the value function &lt;em&gt;V(s)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The value function is a function that tells us the maximum expected future reward the agent will get at each state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*2_JRk-4O523bcOcSy1u31g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;policy-based&quot;&gt;Policy Based&lt;/h4&gt;

&lt;p&gt;In policy-based RL, we want to directly optimize the policy function &lt;em&gt;π(s)&lt;/em&gt; without using a value function.&lt;/p&gt;

&lt;p&gt;The policy is what defines the agent behavior at a given time.&lt;/p&gt;

&lt;p&gt;We have two types of policy:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deterministic: a policy at a given state will always return the same action.&lt;/li&gt;
  &lt;li&gt;Stochastic: output a distribution probability over actions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*fii7Z01laRGateAJDvloAQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;model-based&quot;&gt;Model Based&lt;/h4&gt;

&lt;p&gt;In model-based RL, we model the environment. This means we create a model of the behavior of the environment. Not addressed in this course.&lt;/p&gt;

&lt;h3 id=&quot;deep-reinforcement-learning&quot;&gt;Deep Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;In Q-learning, we keep a table of actions to take for each state (based on reward). This can be huge.&lt;/p&gt;

&lt;p&gt;Deep Learning allows to approximate this Q function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c&quot;&gt;Updated version&lt;/a&gt; from 2020 (and &lt;a href=&quot;https://www.youtube.com/watch?v=q0BiUn5LiBc&quot;&gt;video&lt;/a&gt; version)&lt;/p&gt;

&lt;p&gt;This is a good starting point, well explained.&lt;/p&gt;

&lt;p&gt;Reinforcement Learning is just a &lt;strong&gt;computational approach of learning from action.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A formal definition&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that &lt;strong&gt;learn from the environment&lt;/strong&gt; by &lt;strong&gt;interacting with it&lt;/strong&gt; through trial and error and &lt;strong&gt;receiving rewards&lt;/strong&gt; (positive or negative) &lt;strong&gt;as unique feedback.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some explanations about &lt;strong&gt;observations&lt;/strong&gt; (partial description) vs &lt;strong&gt;states&lt;/strong&gt; (fully observed envt). Only differs in implementation, all theoretical background stays the same.&lt;/p&gt;

&lt;p&gt;Action space where we can distinguish &lt;strong&gt;discrete&lt;/strong&gt; (e.g. fire, up) actions from &lt;strong&gt;continuous&lt;/strong&gt; (e.g. turn 23deg) ones.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt; part is the same as the one from 2018. With cheese, mouse, maze example.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Episodic&lt;/strong&gt; and &lt;strong&gt;continuous&lt;/strong&gt; tasks part is the same as the one from 2018.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exploration/Exploitation trade-off&lt;/strong&gt; is explained the same way + an additional example taken from &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse-fa18/&quot;&gt;berkley - CS 294-112&lt;/a&gt; - Deep Reinforcement Learning course. I want to learn more about this course!&lt;/p&gt;

&lt;p&gt;About &lt;strong&gt;solving RL problems&lt;/strong&gt;, it is now presented as 2 main approaches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;policy-based&lt;/strong&gt; methods&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;value-based&lt;/strong&gt; methods&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*Vujmmyswrg2wIjmpvSUBfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And bedore to explain that, nice presentation of what is a &lt;strong&gt;policy $\pi$&lt;/strong&gt;. Solving RL problem is to find that optimal policy: directly with policy-based method, indirectly (through value function) with value-based method.&lt;/p&gt;

&lt;p&gt;There is an explanation about different types of policy: &lt;strong&gt;deterministic&lt;/strong&gt; and &lt;strong&gt;stochastic&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;And that we use deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based). Thomas suggests to go further with deep learning with MIT 6.S191, which is the &lt;a href=&quot;https://castorfou.github.io/guillaume_blog/deep%20learning/mit/tensorflow/2021/02/05/learning-MIT-6.S191-2021.html&quot;&gt;one&lt;/a&gt; (version 2021) I follow these days.&lt;/p&gt;

&lt;h2 id=&quot;21921---step-2---part-1---q-learning-lets-create-an-autonomous-taxi&quot;&gt;(2/19/21) - &lt;a href=&quot;https://thomassimonini.medium.com/q-learning-lets-create-an-autonomous-taxi-part-1-2-3e8f5e764358&quot;&gt;Step 2 - part 1&lt;/a&gt; - Q-Learning, let’s create an autonomous Taxi&lt;/h2&gt;

&lt;p&gt;And in &lt;a href=&quot;https://www.youtube.com/watch?v=230bR2DrbdE&amp;amp;feature=emb_logo&quot;&gt;video&lt;/a&gt; (I like to read + watch the video at the same time)&lt;/p&gt;

&lt;p&gt;Here in Step 2 we focus on a value-based method: Q-learning. And what is seen in part 1 and 2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*2yYWVAXJh4FI2lpsL0ajwQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;value-based-method&quot;&gt;Value-based method&lt;/h4&gt;

&lt;p&gt;Remember what we mean in value-based method&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*jfUUaZuHUa1h61oD6O18KA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;you don’t train your policy, you define a simple function such as greedy function to select the best association State-Action, so the best action.&lt;/p&gt;

&lt;h4 id=&quot;bellman-equation&quot;&gt;&lt;strong&gt;Bellman equation&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;each value as the sum of the expected return, &lt;strong&gt;which is a long process.&lt;/strong&gt; This is equivalent &lt;strong&gt;to the sum of immediate reward + the discounted value of the state that follows.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*FMjoVEELvz0oKcIfmcvGPQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;monte-carlo-vs-temporal-difference&quot;&gt;Monte Carlo vs Temporal Difference&lt;/h4&gt;

&lt;p&gt;And then an explanation about 2 types of method to learn a policy or a value-function:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Monte Carlo&lt;/em&gt;: learning at the end of the episode. With &lt;em&gt;Monte Carlo&lt;/em&gt;, we update the value function from a complete episode and so we &lt;strong&gt;use the actual accurate discounted return of this episode.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;TD learning&lt;/em&gt;: learning at each step. With &lt;em&gt;TD learning&lt;/em&gt;, we update the value function from a step, so we replace Gt that we don’t have with &lt;strong&gt;an estimated return called TD target.&lt;/strong&gt; (chich is the immediate reward + the discounted value of the next state)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*c8nfnXRu8n1h78bWPEK5vg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was not clear to me that these methods could be used for policy-based approach. It is now!&lt;/p&gt;

&lt;h2 id=&quot;22421---step-2---part-2---q-learning-lets-create-an-autonomous-taxi&quot;&gt;(2/24/21) - &lt;a href=&quot;https://thomassimonini.medium.com/q-learning-lets-create-an-autonomous-taxi-part-2-2-8cbafa19d7f5&quot;&gt;Step 2 - part 2&lt;/a&gt; - Q-Learning, let’s create an autonomous Taxi&lt;/h2&gt;

&lt;p&gt;But the video is not yet available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is Q-Learning?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Q-Learning is an &lt;strong&gt;off-policy value-based method that uses a TD approach to train its action-value function:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;“Off-policy”&lt;/em&gt;: we’ll talk about that at the end of this chapter.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Value-based method”&lt;/em&gt;: it means that it finds its optimal policy indirectly by training a  value-function or action-value function that will tell us what’s &lt;strong&gt;the value of each state or each state-action pair.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“Uses a TD approach”&lt;/em&gt;: &lt;strong&gt;updates its action-value function at each step.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Q stands for quality (quality of action). After training we’ll get the optimal Q-function.&lt;/p&gt;

&lt;p&gt;When choosing an action, we have to balance between exploration and exploitation with &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; - greedy:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*AYz65tJDERsWTg2DGEJ35g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But at beginning Q table is not trained yet so we have to increase exploitation. It is done with some decreasing &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*1J2lJN48gdjeuoRBqsO_CA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Q-learning algorithm is a 4-step process:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;step1: Q-Table init&lt;/li&gt;
  &lt;li&gt;step2: Choose action (&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; - greedy strategy)&lt;/li&gt;
  &lt;li&gt;step3: Perform action A&lt;sub&gt;t&lt;/sub&gt; and get R&lt;sub&gt;t+1&lt;/sub&gt; and S&lt;sub&gt;t+1&lt;/sub&gt;&lt;/li&gt;
  &lt;li&gt;step4: Update Q(S&lt;sub&gt;t&lt;/sub&gt;, A&lt;sub&gt;t&lt;/sub&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*teZ5KRfvYjMKZnmhaWTUXg.png&quot; alt=&quot;Update Q(S&amp;lt;sub&amp;gt;t&amp;lt;/sub&amp;gt;, A&amp;lt;sub&amp;gt;t&amp;lt;/sub&amp;gt;)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Why it is called &lt;strong&gt;off-policy&lt;/strong&gt;? Because we don’t have the same logic to select action (&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; - greedy) and update Q (greedy).&lt;/p&gt;

&lt;p&gt;With &lt;em&gt;On-policy:&lt;/em&gt; we use the &lt;strong&gt;same policy for acting and updating.&lt;/strong&gt; Sarsa is such an algorithm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*gVl6V-wbX_hOoNQATx081Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nice and simple manual example with mouse, cheese in a maze. We run Q-learning and make all calculation by hands.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/500/1*GMuThIF7aNj-V_d6hTRN8A.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;todo&gt; &lt;/todo&gt;

&lt;div class=&quot;alert alert-info&quot;&gt;implement with numpy+gym this algorithm should be a nice exercise.&lt;/div&gt;

&lt;p&gt;There is an exercise to implement a taxi, within this &lt;a href=&quot;https://colab.research.google.com/gist/simoninithomas/466c81aa1c2a07dd14793240c6d033c5/q-learning-with-taxi-v3.ipynb#scrollTo=20tSdDbxxK_H&quot;&gt;notebook&lt;/a&gt; at colab google. Taxi V3 is an env from opengym.&lt;/p&gt;

&lt;h2 id=&quot;3321---back-to-2018---deep-q-learning-with-doom&quot;&gt;(3/3/21) - back to 2018 - Deep Q-learning with Doom&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8&quot;&gt;Article&lt;/a&gt;, &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb&quot;&gt;Notebook&lt;/a&gt;, &lt;a href=&quot;https://youtu.be/gCJyVX98KJ4&quot;&gt;Video&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’ll create an agent that learns to play Doom. Doom is a big  environment with a gigantic state space (millions of different states).  Creating and updating a Q-table for that environment would not be  efficient at all.&lt;/p&gt;

&lt;p&gt;The best idea in this case is to create a &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/&quot;&gt;neural network&lt;/a&gt; that will approximate, given a state, the different Q-values for each action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-media-1.freecodecamp.org/images/1*LglEewHrVsuEGpBun8_KTg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Addresses pb of temporal limitation: get multiple frames to have sense of motion.&lt;/p&gt;

&lt;p&gt;Video is nice because it goes from start and follows closely all steps.&lt;/p&gt;

&lt;p&gt;I wil try to implement in my own by creating an environment and running under a clone of Deep_reinforcement_learning_Course &lt;a href=&quot;https://github.com/simoninithomas/Deep_reinforcement_learning_Course&quot;&gt;Thomas’s repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here at &lt;a href=&quot;https://github.com/castorfou/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb&quot;&gt;Deep Q learning with Doom.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I had to switch to tensorflow-gpu 1.13. Manage some cuda memory issue. But then was able to run it.&lt;/p&gt;

&lt;p&gt;However as Thomas says, I should do it step by step on my own.&lt;/p&gt;</content><author><name></name></author><category term="reinforcement learning" /><summary type="html">A course by Thomas Simonini</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/RL.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/RL.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Conda and jupyter tips</title><link href="https://castorfou.github.io/guillaume_blog/blog/conda-and-jupyter-tips.html" rel="alternate" type="text/html" title="Conda and jupyter tips" /><published>2021-02-16T00:00:00-06:00</published><updated>2021-02-16T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/conda-and-jupyter-tips</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/conda-and-jupyter-tips.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-16-conda-and-jupyter-tips.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Keeping-track-of-python-environments&quot;&gt;Keeping track of python environments&lt;a class=&quot;anchor-link&quot; href=&quot;#Keeping-track-of-python-environments&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I manage all my python environments with &lt;code&gt;conda&lt;/code&gt; from &lt;code&gt;miniconda&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;manual-way&quot;&gt;manual way&lt;a class=&quot;anchor-link&quot; href=&quot;#manual-way&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;However I don't have a strong process to keep track of my environment specifications. Usually I manually create an &lt;code&gt;env.txt&lt;/code&gt; file under my projects. Keeping all commands I have used to create that environment.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/env&lt;span class=&quot;se&quot;&gt;\ \ &lt;/span&gt;mit_6S191.txt
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;env_name: mit_6S191
libraries: python 3.7, tensorflow 2


Installation commands:
conda create -n mit_6S191 python=3.7
conda activate mit_6S191

conda install tensorflow tensorflow-gpu
conda install -c conda-forge jupyter_contrib_nbextensions
conda install matplotlib numpy opencv
conda install -c pytorch torchvision
conda install nb_conda
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;What happens if I add packages in that environment. Or want to use that environment in another project. I have to remember the link between env name and project name.&lt;/p&gt;
&lt;p&gt;That is not robust.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;yml-way&quot;&gt;yml way&lt;a class=&quot;anchor-link&quot; href=&quot;#yml-way&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Keeping a yml file could be a solution to keep track of environment specifications. It doesn't answer to my last concern though (linking env name and project name)&lt;/p&gt;
&lt;p&gt;But there is a limitation linked with channels.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;conda env &lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; --from-history
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;name: fastai
channels:
  - defaults
dependencies:
  - python=3.8
  - fastai
  - jupyter
  - jupyter_contrib_nbextensions
  - fastbook
prefix: /home/explore/miniconda3/envs/fastai
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In that example, &lt;code&gt;fastai&lt;/code&gt; package should come from &lt;code&gt;fastai&lt;/code&gt; channel but conda doesn't keep that information.&lt;/p&gt;
&lt;p&gt;Using&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conda install -n my_env rdkit::rdkit
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;could be an option.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;automate-yml-way&quot;&gt;automate yml way&lt;a class=&quot;anchor-link&quot; href=&quot;#automate-yml-way&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Since conda keeps active environment in env variable &lt;code&gt;CONDA_DEFAULT_ENV&lt;/code&gt;, we can automatically create up-to-date yml file.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONDA_DEFAULT_ENV&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;fastai
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;conda env &lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; --from-history &amp;gt; ~/temp/env_&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONDA_DEFAULT_ENV&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;.yml
&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;ls ~/temp/env_&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CONDA_DEFAULT_ENV&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;.yml
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;/home/explore/temp/env_fastai.yml
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;But for it to be usable, I will have to install package using the &lt;code&gt;&amp;lt;channel&amp;gt;::&amp;lt;package&amp;gt;&lt;/code&gt; way.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/create_yml.sh
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;#!/bin/bash
conda env export --from-history &amp;gt; env_`echo $CONDA_DEFAULT_ENV`.yml
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Conda-commands&quot;&gt;Conda commands&lt;a class=&quot;anchor-link&quot; href=&quot;#Conda-commands&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When managing conda environments, I very often fall on this documentation page which is simply great: &lt;a href=&quot;https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html&quot;&gt;https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Next time I visit this page, I will enter entries here to track my common commands.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Jupyter-installation&quot;&gt;Jupyter installation&lt;a class=&quot;anchor-link&quot; href=&quot;#Jupyter-installation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Jupyter-extensions&quot;&gt;Jupyter extensions&lt;a class=&quot;anchor-link&quot; href=&quot;#Jupyter-extensions&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I have already explained how to install jupyter extensions and the one I use.
&lt;a href=&quot;/guillaume_blog/fastai/jupyter/fastbook/2020/09/24/fastai-book.html#update-jupyter-to-include-extensions-(toc,-...&quot;&gt;update jupyter to include extensions&lt;/a&gt;)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;nb_conda&quot;&gt;nb_conda&lt;a class=&quot;anchor-link&quot; href=&quot;#nb_conda&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This is usefull to switch from environment to another without having to stop/restart jupyter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/guillaume_blog/images/copied_from_nb/../images/nb_conda.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="conda" /><category term="jupyter" /><summary type="html"></summary></entry><entry><title type="html">Learning: Collège de France - Représentations parcimonieuses</title><link href="https://castorfou.github.io/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html" rel="alternate" type="text/html" title="Learning: Collège de France - Représentations parcimonieuses" /><published>2021-02-10T00:00:00-06:00</published><updated>2021-02-10T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/college-de-france-representations-parcimonieuses</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html">&lt;p&gt;Un cours au collège de France de Stéphane Mallat sur les &lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/course-2020-2021.htm&quot;&gt;représentations parcimonieuses - 2021&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cela donne envie d’aller voir ses autres cours:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/course-2017-2018.htm&quot;&gt;2018&lt;/a&gt;: L’apprentissage face à la malédiction de la grande dimension&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/course-2018-2019.htm&quot;&gt;2019&lt;/a&gt;: L’apprentissage par réseaux de neurones profonds&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/course-2019-2020.htm&quot;&gt;2020&lt;/a&gt;: Modèles multi-échelles et réseaux de neurones convolutifs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A peu près 16 vidéos de 1h30 par cours. Et des notes de cours en pdf.&lt;/p&gt;

&lt;h2 id=&quot;21521---le-triangle--régularité-approximation-parcimonie--lecture-1&quot;&gt;2/15/21 - Le triangle « Régularité, Approximation, Parcimonie » (lecture 1)&lt;/h2&gt;

&lt;p&gt;C’est l’introduction du cours. J’apprécie les références historiques et philosphiques partant du rasoir d’Ockam. C’est le principe d’économie ou de parcimonie: le beau, le vrai viendrait du simple.&lt;/p&gt;

&lt;p&gt;La 1ere fois que j’entends une référence précise sur l’opposition entre biais (erreur sur modèle) et variance (erreur sur données ou mesures)&lt;/p&gt;

&lt;p&gt;Et une invitation à consulter une &lt;a href=&quot;https://www.college-de-france.fr/site/stephane-mallat/seminar-2018-02-21-11h15.htm&quot;&gt;méthodologie d’analyse de données&lt;/a&gt; par Pierre Courtiol en utilisant Kaggle. L’idée d’une approche simple linéaire pour bien comprendre quelles étapes successives à emprunter pour améliorer son approche. Me semble assez orthogonal à ce que peut proposer Jeremy Howard: commencer tôt, overfitting n’est pas un probleme, pas de early stopping, etc.&lt;/p&gt;

&lt;h2 id=&quot;21021---approximations-linéaires-et-analyse-de-fourier-lecture-2&quot;&gt;2/10/21 - Approximations linéaires et analyse de Fourier (lecture 2)&lt;/h2&gt;

&lt;p&gt;J’ai commencé par ce cours conseillé par Rémi mon pote enseignant chercheur en math. C’est un peu le grand écart avec des méthodes d’enseignement anglo-saxonnes mais ça fait du bien. C’est finalement plus proche de ce que j’ai connu dans ma formation initiale.&lt;/p&gt;

&lt;p&gt;S.Mallat présente les équivalences (sous certaines conditions) entre&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Régularité&lt;/li&gt;
  &lt;li&gt;Approximation en basse dimension&lt;/li&gt;
  &lt;li&gt;et représentation parcimonieuse&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;dans le cadre des approximations linéaires. Il parle des 2 mondes: traitement du signal et analyse de la donnée. Je suis moins intéressé par le 1er monde, mais j’apprécie la piqure de rappel. Je ne me rappelais pas du tout l’importance de l’analyse de Fourier et la construction des bases de L[0,1] par exemple.&lt;/p&gt;

&lt;p&gt;Et il revient sur les singularités, beaucoup d’informations sont portées par les singularités (par exemple les frontières dans une image)&lt;/p&gt;

&lt;p&gt;Je crois bien que je vais me faire toute la session, et sans doute les autres années.&lt;/p&gt;

&lt;h2 id=&quot;22321---grande-dimension-et-composantes-principales-lecture-3&quot;&gt;2/23/21 - Grande dimension et composantes principales (lecture 3)&lt;/h2&gt;

&lt;p&gt;Dans ce cadre linéaire grande dimension, quelle meilleure base - approche PCA et base Karhunen-Loeve.&lt;/p&gt;

&lt;p&gt;Quid quand on passe en non linéaire.&lt;/p&gt;

&lt;p&gt;Réseau neurone à 1 couche cachée, théoreme de representation universel.&lt;/p&gt;

&lt;p&gt;Retour sur les bases de L²[0,1] qui sont les bases de Fourier en variables complexes.&lt;/p&gt;

&lt;p&gt;Pour un passage en dimension q, on remplace n par (n1, …, nq) et la multiplication n*u par le produit scalaire &amp;lt;n, u&amp;gt;.&lt;/p&gt;

&lt;p&gt;En travaillant sur les équivalences du triangle, il montre pourquoi on est très limité en approximation lineaire quand la dimension augmente.&lt;/p&gt;

&lt;p&gt;En approximation lineaire, il suffit de prendre les 1ers vecteurs (se limiter à une dimension q) (en base de fourier par exemple) pour avoir une assez bonne approximation. Dans des signaux plus perturbés (avec des singularités) on perd plus d’énergie: il faudrait échantilloner plus fin dans ces zones de singularités et si on dispose d’une base orthonormée il s’agirait non plus de prendre les q 1ers vecteurs mais de prendre ceux d’intéret.&lt;/p&gt;

&lt;h2 id=&quot;3221---approximations-non-linéaires-et-réseaux-de-neurones-lecture-4&quot;&gt;3/2/21 - Approximations non linéaires et réseaux de neurones (lecture 4)&lt;/h2&gt;

&lt;p&gt;Le triangle (approximation basse dimensions, représentation parcimonieuse, régularité) d’un point de vue non linéaire.&lt;/p&gt;

&lt;p&gt;Ici plutôt qu’approximer un signal en prenant les M 1ers coefficients de Fourier (basses dimensions), on va prendre M coefficients mais dépendamment de x. C’est ici qu’on introduit la non-linéarité. L’erreur est alors la queue de distribution des coefficients ordonnés. On veut que l’énergie des plus petits coefficients soit négligeable.&lt;/p&gt;

&lt;p&gt;Pas facile d’obtenir cet ordre, on cherche une façon de limiter les coefficients non ordonnés nous donnant une représentation parcimonieuse. En utilisant la nome l&lt;sub&gt;$\alpha$&lt;/sub&gt; avec &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; petit (inférieur à 2 et proche de 0), on introduit cette décroissance mais cette fois-ci sur les coefficients non ordonnés.&lt;/p&gt;

&lt;p&gt;Intéressant d’avoir des normes convexes, et dans ce cas on ne peut prendre que &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;=1. C’est pour ça qu’on voit apparaître partout les normes l&lt;sub&gt;1&lt;/sub&gt; dans les algorithmes d’apprentissage (norme convexe garantissant une forme de sparsité).&lt;/p&gt;

&lt;p&gt;On passe aux réseaux de neurones à 1 couche cachée. Et on va basculer dans les notations de x(u) à f(x)., avec x &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; [0, 1]&lt;sup&gt;d&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/rep_par_lecture4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ici on projette f dans l’espace engendré par ces vecteurs { &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\rho&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ρ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(x.w&lt;sub&gt;m&lt;/sub&gt;+b&lt;sub&gt;m&lt;/sub&gt;) }&lt;sub&gt;n&amp;lt;=M&lt;/sub&gt;.&lt;/p&gt;

&lt;p&gt;On peut facilement calculer l’erreur quadratique comme l’intégrale sur les x &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; [0, 1]&lt;sup&gt;d&lt;/sup&gt; de la norme l² ( f(x)-f&lt;sub&gt;tilde&lt;/sub&gt;(x) ) et il y a un belle démonstration qui est le &lt;strong&gt;théorème d’approximation universelle&lt;/strong&gt; (démontrée entre 1988 et 1992) qui montre que l’erreur tend vers 0 quand M tend vers l’infini.&lt;/p&gt;

&lt;p&gt;La démonstration avec &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ρ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\rho&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ρ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; = e&lt;sup&gt;ia&lt;/sup&gt; revient à une décomposition d’en Fourier. Et pour d’autres non régularité comme reLu ou sigmoid, il s’agit d’un changement de base.&lt;/p&gt;

&lt;p&gt;Et là on arrive à la malédiction de la dimensionnalité car quand d est grand (disons 1M), les coefficients baissent à une faible vitesse. Que faut-il faire pour battre cette malédiction?&lt;/p&gt;

&lt;p&gt;Baron en 1993 introduit une hypothèse de regularité qui permet de borner l’erreur par un terme qui ne dépend pas de la dimension. C’est donc gagné sauf que l’hypothèse de régularité n’est généralement pas valide dans les cas qui nous intéressent.&lt;/p&gt;

&lt;p&gt;Stéphane Mallat, de façon brillante mais est-ce étonnant, explique pourquoi l’approche des mathématiciens est une impasse et pourquoi ce qu’on cherche à faire se ramène à un problème bayésien. Car les problèmes qui nous intéressent (par exemple la classification d’objets, ne va solliciter qu’un minuscule espace (même si de grande dimension) parmi toutes les images possibles). On va donc chercher à caractériser x pour chaque y (classe). (revoir vidéo entre 49’ et 1h03)&lt;/p&gt;

&lt;p&gt;L’enjeu est de caractériser le support qui est beaucoup plus concentré que [0,1]&lt;sup&gt;d&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Donc on va retravailler sur les approximations non linéaires de x, le signal lui-même (et non plus f), et d’essayer de comprendre pourquoi on peut faire beaucoup mieux que la transformée de Fourier et quelle genre de bases vont nous permettre de faire bcp mieux. Une des applications va être la compression, qui va nous amener à étudier la théorie de l’information et la théorie de l’information c’est exactement la théorie probabiliste qui explique ces phénomènes de concentration et les mesure avec l’entropie.&lt;/p&gt;

&lt;p&gt;Introduction des bases d’ondelettes qui vont permettre de représenter les singularités locales. Les ondelettes sont à la fois localisées (paramètre v) et dilatées (paramètre s). Il faudra à partir de ces ondelettes construire des bases orthogonales pour arriver à des approximations basses dimensions (et garder les grands coefficients)&lt;/p&gt;

&lt;p&gt;On introduit la notion de régularité locale exprimée avec lipchitz &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Avec &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\alpha&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.0037em;&quot;&gt;α&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &amp;lt;1 pour exprimer les singularités.&lt;/p&gt;

&lt;h2 id=&quot;3921---ondelettes-et-échantillonnage-lecture-5&quot;&gt;3/9/21 - Ondelettes et échantillonnage (lecture 5)&lt;/h2&gt;

&lt;p&gt;On était resté sur une représentation de signaux qui ne présentent pas de régularité uniforme mais qui présentent des singularités que nous voulons capter, ces singularités étant porteuses d’informations importantes (par exemple les contours dans une image). Ces singularités n’étant pas très nombreuses, on peut toujours parler de &lt;strong&gt;régularité locale&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;On va donc utiliser des ondelettes pour décomposer ces signaux, d’où la notion de &lt;strong&gt;représentation parcimonieuse&lt;/strong&gt;, exprimée sur la base d’ondelettes orthonormales. Et enfin en en sélectionnant un petit nombre nous revenons sur nos &lt;strong&gt;approximations en basse dimension&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Le produit scalaire du signal x(u) par l’ondelette &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ψ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\psi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8888799999999999em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;ψ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;sub&gt;v,s&lt;/sub&gt; revient à un produit de convolution de x par l’ondelette conjuguée. Ca veut dire que sur les points de singularités les produits scalaires vont être maximisés.&lt;/p&gt;

&lt;p&gt;Stéphane Mallat passe un long moment pour nous amener à la construction de ces bases d’ondelettes orthonormales. Il part des bases de Haar puis de Shannon et arrive à une construction plus récente par Yves Meyer en 1986.&lt;/p&gt;</content><author><name></name></author><category term="deep learning" /><category term="math" /><summary type="html">Un cours au collège de France de Stéphane Mallat sur les représentations parcimonieuses - 2021.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/math.jpeg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/math.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Learning: MIT 6.S191 Introduction to Deep Learning - 2021</title><link href="https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021.html" rel="alternate" type="text/html" title="Learning: MIT 6.S191 Introduction to Deep Learning - 2021" /><published>2021-02-05T00:00:00-06:00</published><updated>2021-02-05T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021.html">&lt;p&gt;From &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;http://introtodeeplearning.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I keep all content (lectures, notebooks) in &lt;a href=&quot;https://github.com/castorfou/mit_6s191&quot;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is done with google contribution, and therefore all examples are in tensorflow. I will try to adapt notebooks in PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;2521---intro-to-deep-learning---lecture-1&quot;&gt;2/5/21 - Intro to Deep Learning - lecture 1&lt;/h2&gt;

&lt;p&gt;Lecturer: Alexander Amini&lt;/p&gt;

&lt;p&gt;Intro is just jaw-dropping!&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtu.be/5tvmMX8r_OM?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;t=40&quot;&gt;2020 intro&lt;/a&gt; was top.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtu.be/5tvmMX8r_OM?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;t=149&quot;&gt;2021 intro&lt;/a&gt; is just awesome.&lt;/p&gt;

&lt;p&gt;It is a standard overview of simple deep learning concepts: Perceptron, multi-perceptron, dense layers, loss, gradient-descent, backprop, SGD, regularization, dropout, early stoppping&lt;/p&gt;

&lt;h2 id=&quot;21521---deep-sequence-modeling---lecture-2&quot;&gt;2/15/21 - Deep Sequence Modeling - lecture 2&lt;/h2&gt;

&lt;p&gt;New lecturer: Ava Soleimany&lt;/p&gt;

&lt;p&gt;Nice introduction to sequence modeling with Many-to-One, One-to-Many, Many-to-Many.&lt;/p&gt;

&lt;p&gt;RNN and implementation in TensorFlow. And NLP examples: next word problem. (and NLP concepts such as Vocabulary, Indexing, Embedding)&lt;/p&gt;

&lt;p&gt;And what we need for sequence modeling:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;handle variable-length sequences&lt;/li&gt;
  &lt;li&gt;track long-term dependencies&lt;/li&gt;
  &lt;li&gt;maintain information about order&lt;/li&gt;
  &lt;li&gt;share parameters across the sequence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Backpropagation through time and problem of exploding/vanishing gradients.&lt;/p&gt;

&lt;p&gt;Against exploding: gradient clipping. Against vanishing: 3 ways explained - activation functions, weight init, network arch.&lt;/p&gt;

&lt;p&gt;Gated cell: to control what information is passed through. Ex: LSTM Long Short Term Memory. They support something closed to Forget Store Update Output. Ava explains graphically which part of LSTM cells is providing which function.&lt;/p&gt;

&lt;p&gt;And then examples: Music generation (to generate 4th movement of last symphony from Schubert!), sentiment classification, machine translation (with Attention mechanisms which provide learnable memory access to solve Not long memory), trajectory prediction, environmental modeling.&lt;/p&gt;

&lt;h2 id=&quot;21621---intro-to-tensorflow--music-generation---software-lab-1&quot;&gt;2/16/21 - Intro to TensorFlow;  Music Generation - software lab 1&lt;/h2&gt;

&lt;p&gt;As an exercise I have completed labs in TensorFlow and adapted them in &lt;a href=&quot;https://github.com/castorfou/mit_6s191/blob/main/introtodeeplearning/lab1/Part1_TensorFlow_transposed%20to%20PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With LSTM, I ran into this error: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Which is solved by calling &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.config.experimental.set_memory_growth&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;gpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_physical_devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'GPU'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Currently, memory growth needs to be the same across GPUs
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gpu&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experimental&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_memory_growth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logical_gpus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;experimental&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_logical_devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'GPU'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Physical GPUs,&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logical_gpus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Logical GPUs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;RuntimeError&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Memory growth must be set before GPUs have been initialized
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Music lab is nice to play with. I am not sure I would be able to convert to PyTorch. It would require time!&lt;/p&gt;

&lt;h2 id=&quot;22221---deep-computer-vision---lecture-3&quot;&gt;2/22/21 - Deep Computer Vision - lecture 3&lt;/h2&gt;

&lt;p&gt;I have never been a big fan of computer vision.&lt;/p&gt;

&lt;p&gt;I like the idea developed by Alexander Amini about &lt;strong&gt;hierarchy of features&lt;/strong&gt;. (low level: edges, spots; mid level: eyes, noses)&lt;/p&gt;

&lt;p&gt;And how he explains limitation of FC layers for visual detection, and introduction of spatial structure (feature extraction with convolutions)&lt;/p&gt;

&lt;p&gt;Some nice examples of hand-engineered convolution filters for different needs: sharpen, edge detect, strong edge detect.&lt;/p&gt;

&lt;p&gt;Then classic explanations of CNN with convolution, max pooling.&lt;/p&gt;

&lt;p&gt;I like the way classification problems are broken down between feature learning (convolution+relu, pooling, repeated several times) and classification (flatten, FC, softmax) which is a task learning part.&lt;/p&gt;

&lt;p&gt;The second part (task learning part) can be anything: classification, object detection, segmentation, probabilistic control, …&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec3_cnn_architectures.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nice explanation of R-CNN to learn region proposals.&lt;/p&gt;

&lt;p&gt;Introduction to Software lab2: de-biaising facial recognition systems.&lt;/p&gt;

&lt;h2 id=&quot;3121---deep-generative-modeling---lecture-4&quot;&gt;3/1/21 - Deep Generative Modeling - lecture 4&lt;/h2&gt;

&lt;p&gt;From pattern discovered from data (underlying structure of the data), generate examples following these patterns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Autoencoder&lt;/strong&gt;: foundational generative model which builds up latent variable representation by self-encoding the input. To train such network, we create a decoder to go from latent variable to generated output, and then compare input to generated output.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec4_autoencoders.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variational autoencoder (vae)&lt;/strong&gt;: with vae we try to encode inputs as distributions defined by mean &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\mu&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;μ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; and variance &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;σ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. And we want to achieve continuity and completeness:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;continuity: points that are close in latent space –&amp;gt; similar content after decoding&lt;/li&gt;
  &lt;li&gt;completeness: sampling from latent space –&amp;gt; ‘meaningful’ content after decoding&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Regularization is pushing to get these properties.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec4_vae_regularization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the learning process is about minimizing reconstruction loss + a regularization term:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec4_vae_loss.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ava is then explaining the smart trick to allow backpropagation to happen. Indeed by introducing stochastic term in the sampling layer, we are breaking the backpropagation logic.&lt;/p&gt;

&lt;p&gt;We are moving z from a normal distribution to &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;μ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\mu&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.625em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;μ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;+&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\sigma&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;σ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; where &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\epsilon&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; follow a normal distribution of mean 0, std 1.&lt;/p&gt;

&lt;p&gt;Explanation then of space disentanglement via &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\beta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.8888799999999999em;vertical-align:-0.19444em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05278em;&quot;&gt;β&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;-VAEs. It allows latent variables to be independent.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec4_vae_beta.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then some introduction about &lt;em&gt;*GANs&lt;/em&gt; (Generative Adversarial Network) which are a way to make a generative model by having 2 neural networks (generator and discriminator) compete with each other.&lt;/p&gt;

&lt;p&gt;And share some recent advances on GAN such as StyleGAN(2), conditional GAN, CycleGAN. CycleGAN is famous for turning horses in zebras, but it can be used to transform speech as well (used in the synthesis of Obama’s voice)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec4_generative_summary.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1321---de-biasing-facial-recognition-systems---software-lab-2&quot;&gt;1/3/21 - De-biasing Facial Recognition Systems - Software Lab 2&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/castorfou/mit_6s191/blob/main/introtodeeplearning/lab2/Part1_MNIST.ipynb&quot;&gt;Part 1 MNIST&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;starts with FC layers. With some overfitting but a good accuracy of 96%.&lt;/p&gt;

&lt;p&gt;then move to a CNN architecture. I ran into &lt;a href=&quot;https://github.com/tensorflow/tensorflow/issues/24828&quot;&gt;gpu issues&lt;/a&gt;.  Accuracy is now 99%.&lt;/p&gt;

&lt;p&gt;I didn’t manage to make the last part working. (using tape.gradient)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/castorfou/mit_6s191/blob/main/introtodeeplearning/lab2/Part2_Debiasing.ipynb&quot;&gt;Part 2 Debiasing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Fit a CNN model to classify faces based on celebA dataset. And see the bias effect by predicting on Fitzpatrick scale skin type classification system.&lt;/p&gt;

&lt;p&gt;Use VAE to learn latent structure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/3s4S6Gc/vae.jpg&quot; alt=&quot;The concept of a VAE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To then debias using DB-VAE model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab2/img/DB-VAE.png&quot; alt=&quot;DB-VAE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a lack of progressive unit tests to validate each step. Cannot go to the end.&lt;/p&gt;

&lt;p&gt;Would be interested to see how to apply to non computer vision problems.&lt;/p&gt;

&lt;h2 id=&quot;3821---deep-reinforcement-learning---lecture-5&quot;&gt;3/8/21 - Deep Reinforcement Learning - lecture 5&lt;/h2&gt;

&lt;p&gt;Q-function captures the expected total future reward an agent in state &lt;em&gt;s&lt;/em&gt; can receive by executing a certain action &lt;em&gt;a&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Distinction between &lt;strong&gt;Value Learning&lt;/strong&gt; (learn Q function) and &lt;strong&gt;Policy Learning&lt;/strong&gt; (find directly &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\pi&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;π&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;(s)).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec5_pg_dqn.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Value Learning or DQN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec5_dqn_summary.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec5_dqn_downsides.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The key thing is about handling of continuous actions.&lt;/p&gt;

&lt;p&gt;Let’s see how to do it with policy learning:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy learning or Policy Gradient (PG)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec5_pg_key_idea.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/mit_6S191_lec5_pg_training.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alexanders ends the lecture by discussing about Deepmind progress:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;alphaGo - 2016: with a pretrain in supervised mode then standard DRL&lt;/li&gt;
  &lt;li&gt;alphaGo Zero - 2017: standard DRL without pretraining&lt;/li&gt;
  &lt;li&gt;alphaZero - 2018: standard DRL without pretraining and applied to several games (Go, Chess, Shogi)&lt;/li&gt;
  &lt;li&gt;MuZero - 2020: learns the rules of the game by itself, create unknown dynamics&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="deep learning" /><category term="MIT" /><category term="tensorflow" /><summary type="html">From http://introtodeeplearning.com/</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/DL.jpg" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/DL.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reinforcement learning readings</title><link href="https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-readings.html" rel="alternate" type="text/html" title="Reinforcement learning readings" /><published>2021-01-26T00:00:00-06:00</published><updated>2021-01-26T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-readings</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-readings.html">&lt;h2 id=&quot;12621---reinforcement-learning-for-real-world-robotics&quot;&gt;1/26/21 - Reinforcement learning for real-world robotics&lt;/h2&gt;

&lt;p&gt;from &lt;a href=&quot;https://www.youtube.com/watch?v=Obek04C8L5E&amp;amp;feature=youtu.be&quot;&gt;https://www.youtube.com/watch?v=Obek04C8L5E&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;at 26’ idea that you can tackle over-optimism models by using ensemble models.
See paper at &lt;a href=&quot;https://github.com/castorfou/datascience-papers/blob/main/RL/2018%20Model-Ensemble%20Trust-Region%20Policy%20Optimization/model_ensemble_trust_region_policy_optimization.pdf&quot;&gt;2018 Model-Ensemble Trust-Region Policy Optimization&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;12621---reinforcement-learning-algorithms--an-intuitive-overview&quot;&gt;1/26/21 - Reinforcement Learning algorithms — an intuitive overview&lt;/h2&gt;

&lt;p&gt;from &lt;a href=&quot;https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc&quot;&gt;https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/RL_taxonomy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;give an overview of various RL models. Model-based vs model-free.&lt;/p&gt;

&lt;p&gt;And papers and codes.&lt;/p&gt;

&lt;h2 id=&quot;12621---reinforcement-learning-partie-1--introduction-in-french&quot;&gt;1/26/21 - Reinforcement learning, partie 1 : introduction (in French)&lt;/h2&gt;

&lt;p&gt;There is a reference to an introduction paper:
from Sutton, Richard S., and Andrew G. Barto &lt;a href=&quot;https://github.com/castorfou/datascience-papers/blob/main/RL/2015%20Reinforcement%20Learning%20an%20introduction%20-%20Sutton%2C%20Richard%20S.%2C%20and%20Andrew%20G.%20Barto/SuttonBartoIPRLBook2ndEd.pdf&quot;&gt;« Reinforcement learning : an introduction. » (2011)&lt;/a&gt;. (I have an updated version from 2015)&lt;/p&gt;

&lt;p&gt;There is a reference to a blog article &lt;a href=&quot;https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287&quot;&gt;[2] Steeve Huang. “Introduction to Various Reinforcement Learning Algorithms. Part I” (Q-Learning, SARSA, DQN, DDPG)”. (2018)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And the paper for OpenAI Gym &lt;a href=&quot;https://github.com/castorfou/datascience-papers/blob/main/RL/2016%20OpenAI%20Gym/1606.01540.pdf&quot;&gt;[3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba. “OpenAI Gym”. (2016)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;12721---reinforcement-learning--an-introduction---i-tabular-solution-methods&quot;&gt;1/27/21 - Reinforcement learning : an introduction - I tabular solution methods&lt;/h2&gt;

&lt;p&gt;as a ref. from Reinforcement learning, partie 1 : introduction (in French)&lt;/p&gt;

&lt;p&gt;I like this summary about RL&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Reinforcement learning is a computational approach to understanding and automating goal-directed learning and decision-making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without relying on exemplary supervision or complete models of the environment. In our opinion, reinforcement learning is the first field to seriously address the computational issues that arise when learning from interaction with an environment in order to achieve long-term goals.
Reinforcement learning uses a formal framework defining the interaction between a learning agent and its environment in terms of states, actions, and rewards. This framework is intended to be a simple way of representing essential features of the artificial intelligence problem. These features include a sense of cause and effect, a sense of uncertainty and nondeterminism, and the existence of explicit goals.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There is some history about RL. Bellman equation and dynamic programming are at the beginning of RL.&lt;/p&gt;

&lt;p&gt;I read about &lt;a href=&quot;http://felix.proba.jussieu.fr/pageperso/pham/Tangente.pdf&quot;&gt;HJB equation&lt;/a&gt; from Huyên PHAM (from a French Math magazine). It is funny to see why dynamic programming has been named that way, and how to deal with management.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The class of methods for solving optimal control problems by solving this equation came to be known as dynamic programming (Bellman, 1957a). Bellman (1957b) also introduced the discrete stochastic version of the optimal control problem known as Markovian decision processes (MDPs), and
Ronald Howard (1960) devised the policy iteration method for MDPs. All of these are essential elements underlying the theory and algorithms of modern reinforcement learning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;All the vocabulary around RL is coming from dynamic programming and MDP.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;&gt;Markov decision process - Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;../images/wikipedia_mdp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Interesting to read that the famous cart pole experiment (learning to balance a pole hinged to a movable cart) came from Michie and Chambers in 1968, 53 years ago! (and derived from tic-tac-toe experiment)&lt;/p&gt;

&lt;p&gt;I don’t understand the subtlety behind the move from “learning with a teacher” to “learning with a critic” following the modified Least-Mean-Square (LMS) algorithm; Widrow and Hoff (1973)&lt;/p&gt;

&lt;p&gt;And some explanations about temporal-difference. I have just understood that a convergence effort happened (in 1989) by Chris Watkin who brought together temporal-difference and optimal control by developing Q-learning.&lt;/p&gt;

&lt;p&gt;After this introduction, here is the content:&lt;/p&gt;

&lt;p&gt;1st part is about &lt;strong&gt;finite markov decision processes&lt;/strong&gt;—and its main ideas including Bellman equations and value functions.&lt;/p&gt;

&lt;p&gt;2nd part is about describing three fundamental classes of methods for &lt;strong&gt;solving finite Markov decision problems: dynamic programming, Monte Carlo methods, and temporal-difference learning&lt;/strong&gt;. Each class of methods has its strengths and weaknesses. &lt;strong&gt;Dynamic programming&lt;/strong&gt; methods are well developed mathematically, but require a complete and accurate model of the environment. &lt;strong&gt;Monte Carlo&lt;/strong&gt; methods don’t require a model and are conceptually simple, but are not suited for step-by-step incremental computation. Finally, &lt;strong&gt;temporal-difference&lt;/strong&gt; methods require no model and are fully incremental, but are more complex to analyze.&lt;/p&gt;

&lt;p&gt;3rd part is about combining these methods to offer a &lt;strong&gt;complete and unified solution to the tabular reinforcement learning problem&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We can think of terms agent, environment, and action as engineers’ terms controller, controlled system (or plant), and control signal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/agent–environment_interaction.png&quot; alt=&quot;The agent–environment interaction in reinforcement learning.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Explanation about agent vs environment. Often not the same as physical boundaries of a robot: this boundary represents the limit of the agent’s absolute control, not of its knowledge. Many different agents can be operated at once.&lt;/p&gt;

&lt;p&gt;The agent’s goal is to maximize the total mount of reward it receives.&lt;/p&gt;

&lt;p&gt;I should re-read the full chapter3 because a lot of concepts coming from MDP is exposed, and their links to RL. At the end I should be able to answer most of end-of-chapter exercises. Have clearer view about how to define what are my agents/environment in my case; how to define actions (low-level definition (e.g. V in level1 electrical grid vs high level decision)); everything related to q* and Q-learning.&lt;/p&gt;

&lt;h4 id=&quot;dynamic-programming-dp-chap4---103-126&quot;&gt;dynamic programming (DP) (chap4 - 103-126)&lt;/h4&gt;

&lt;p&gt;What is key here is to have an exact way to describe your environment. Which is not always feasible. And we need computer power to go through all states, compute value function. There is a balance between policy evaluation and policy improvement but this is not crystal clear to me. And I don’t understand asynchronous DP. I haven’t developed enough intuitions behind DP, and I am unable to answer exercises. I understand though that reinforcement learning can solve some problems by approximating part of it (evaluation, environment, …)&lt;/p&gt;

&lt;h4 id=&quot;monte-carlo-mc-methods-chap5---127-156&quot;&gt;monte carlo (MC) methods (chap5 - 127-156)&lt;/h4&gt;

&lt;p&gt;first-visit vs every-visit methods. First-visit has been widely studied. Blackjack example. Explanation of Monte Carlo ES (exploratory starts); and how to avoid this unlikely assumption thanks to on-policy or off-policy methods (on-policy estimate the value of a policy while using it for control. In off-policy methods these two functions are separated (behavior and target)).&lt;/p&gt;

&lt;p&gt;One issue with MC methods is to ensure sufficient exploration. One approach is to start with a random state-action pair, could work with simulated episodes but unlikely to learn from real experience.&lt;/p&gt;

&lt;p&gt;MC methods do not bootstrap (i.e. they don’t update their value estimates based on other value estimates) (TODO learn more about bootstrapping)&lt;/p&gt;

&lt;h4 id=&quot;temporal-difference-td-learning-chap6---157-180&quot;&gt;temporal-difference (TD) learning (chap6 - 157-180)&lt;/h4&gt;

&lt;p&gt;TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap, or said differently they learn a guess from a guess).&lt;/p&gt;

&lt;p&gt;If you consider optimization as a 2 phases approach: prediction problem (ie policy evaluation) and control problem (ie optimal policy), DP, TD, MC differences are at the prediction problem. On control problem they use variations of generalized policy iteration (GPI).&lt;/p&gt;

&lt;p&gt;TD methods combine the sampling of Monte Carlo with the bootstrapping of DP.&lt;/p&gt;

&lt;p&gt;Example based on Driving Home. In TD you update prediction at each step, not waiting for the final return as in MC.&lt;/p&gt;

&lt;h4 id=&quot;eligibility-traces-chap7---181-208&quot;&gt;eligibility traces (chap7 - 181-208)&lt;/h4&gt;

&lt;p&gt;TD(&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\lambda&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.69444em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;λ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;) is a way to integrate MC and TD.&lt;/p&gt;

&lt;p&gt;If one wants to use TD methods because of their other advantages, but the task is at least partially non-Markov, then the use of an eligibility trace method is indicated. Eligibility traces are the first line of defense against both long-delayed rewards and non-Markov tasks.&lt;/p&gt;

&lt;p&gt;I am not sure to understand the effect of bootstrap.&lt;/p&gt;

&lt;h4 id=&quot;planning-and-learning-with-tabular-methods-chap8---209-220-236&quot;&gt;Planning and Learning with Tabular Methods (chap8 - 209-220-236)&lt;/h4&gt;

&lt;p&gt;planning = require a model (dynamic programming, heuristic search)&lt;/p&gt;

&lt;p&gt;learning = can be used without a model (MC, TD)&lt;/p&gt;

&lt;p&gt;The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment.&lt;/p&gt;

&lt;h2 id=&quot;21821---from-a-deep-reinforcement-learning-based-multi-criteria-decision-support-system-for-optimizing-textile-chemical-process&quot;&gt;2/18/21 - from &lt;a href=&quot;https://www.researchgate.net/publication/346426718_A_Deep_Reinforcement_Learning_Based_Multi-Criteria_Decision_Support_System_for_Optimizing_Textile_Chemical_Process&quot;&gt;A Deep Reinforcement Learning Based Multi-Criteria Decision Support System for Optimizing Textile Chemical Process&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This is a more practical paper and should help to figure out what could be our own implementation.&lt;/p&gt;

&lt;p&gt;Overall MDP (markov decision process) structure is quite interesting with 3 blocks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RF (random forest) models (one per objective)&lt;/li&gt;
  &lt;li&gt;AHP (analytic hierarchy process) which is a MCDM (Multiple criteria decision-making) method&lt;/li&gt;
  &lt;li&gt;DQN which is the reinforcement learning part to approximate the Q function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/mdp_structure.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;there are interesting references.&lt;/p&gt;

&lt;p&gt;[2] K. Suzuki, ARTIFICIAL NEURAL NETWORKS - INDUSTRIAL AND CONTROL ENGINEERING APPLICATIONS. 2011.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is nearly impossible to upgrade the textile chemical manufacturing processes directly by only following the cases from other industries without considering the detailed characteristics of this sector and specific investigations in the applicable advanced technologies. To this end, the construction of accurate models for simulating manufacturing processes using intelligent techniques is rather necessary[2]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[4]A. Ghosh, P. Mal, and A. Majumdar, Advanced Optimization and Decision-Making Techniques in Textile Manufacturing.2019.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[..] Therefore, production decision-makers cannot effectively control the processes in order to obtain desired product functionalities [4]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[53] T.L. Saaty, “What is the analytic hierarchy process?” Mathematical models for decision support, Springer, 1988, pp.109 121.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The AHP is a MCDM method introduced by Saaty [53]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[54]R. S. Sutton and A. G. Barto, Introduction to reinforcement learning, vol. 135. MIT press Cambridge, 1998.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Markov property indicates that the state transitions are only dependent on the current state and current action is taken, but independent to all prior states and actions[54].&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[66] Z. Chourabi, F.Khedher, A. Babay and M. Cheikhrouhou, “Multi-criteria decision making in workforce choice using AHP, WSM and WPM”, J.Text.Inst., 2018&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;However, it is worth remarking that certain features of this framework may hinder the massive promotion and application of it. The AHP has been successfully implemented in MCDM problems [41], [66]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;21821---the-complete-reinforcement-learning-dictionary&quot;&gt;2/18/21 - &lt;a href=&quot;https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e&quot;&gt;The Complete Reinforcement Learning Dictionary&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;recommandations:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;If you’re looking for a quick, 10-minutes crash course into RL with code examples, checkout my &lt;em&gt;Qrash Course&lt;/em&gt; series: &lt;a href=&quot;https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&quot;&gt;Introduction to RL and Q-Learning&lt;/a&gt; and &lt;a href=&quot;https://medium.com/@shakedzy/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c&quot;&gt;Policy Gradients and Actor-Critics&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;I you’re into something deeper, and would like to learn and code several  different RL algorithms and gain more intuition, I can recommend &lt;a href=&quot;https://medium.com/freecodecamp/an-introduction-to-reinforcement-learning-4339519de419&quot;&gt;this series&lt;/a&gt; by &lt;a href=&quot;https://medium.com/@thomassimonini&quot;&gt;Thomas Simonini&lt;/a&gt; and &lt;a href=&quot;https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0&quot;&gt;this series&lt;/a&gt; by &lt;a href=&quot;https://medium.com/@awjuliani&quot;&gt;Arthur Juliani&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;If you’re ready to master RL, I will direct you to the “bible” of Reinforcement Learning — &lt;em&gt;“Reinforcement Learning, an introduction”&lt;/em&gt; by Richard Sutton and Andrew Barto. The second edition (from 2018) is available for free (legally) as a &lt;a href=&quot;https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf&quot;&gt;PDF file&lt;/a&gt;.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3521---reinforcement-learning--an-introduction---ii-approximate-solution-methods&quot;&gt;3/5/21 - Reinforcement learning : an introduction - II Approximate Solution Methods&lt;/h2&gt;

&lt;p&gt;This is the 2nd part of the book.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;On-policy Approximation of Action Values&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As mentioned in introduction of part II, what is developed in part I (our estimates of value functions are represented as a table with one entry for each state or for each state–action pair) is instructive, but of course it is limited to tasks with small numbers of states and actions.&lt;/p&gt;

&lt;p&gt;How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset?&lt;/p&gt;

&lt;p&gt;This is a generalization issue (or &lt;em&gt;function approximation&lt;/em&gt;) one could consider as an instance of supervised learning, where we use the s-&amp;gt;v of each backup as a training example, and then interpret the approximate function produced as an estimated value function.&lt;/p&gt;

&lt;p&gt;Bertsekas and Tsitsiklis (1996) present the state of the art in function approximation in reinforcement learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy approximation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Actor-Critic: The policy structure is known as the actor, because it is used to select actions, and the
estimated value function is known as the critic, because it criticizes the actions made by the actor.&lt;/p&gt;</content><author><name></name></author><category term="reinforcement learning" /><summary type="html">1/26/21 - Reinforcement learning for real-world robotics</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://castorfou.github.io/guillaume_blog/images/RL.png" /><media:content medium="image" url="https://castorfou.github.io/guillaume_blog/images/RL.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Aristotle and Deep learning</title><link href="https://castorfou.github.io/guillaume_blog/blog/Aristotle-and-deep-learning.html" rel="alternate" type="text/html" title="Aristotle and Deep learning" /><published>2021-01-21T00:00:00-06:00</published><updated>2021-01-21T00:00:00-06:00</updated><id>https://castorfou.github.io/guillaume_blog/blog/Aristotle-and-deep-learning</id><content type="html" xml:base="https://castorfou.github.io/guillaume_blog/blog/Aristotle-and-deep-learning.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-21-Aristotle-and-deep-learning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;By reading some references in recent paper, I have started to read “artificial intelligence structures and strategies for complex problem solving” by &lt;a href=&quot;https://www.cs.unm.edu/~luger/&quot;&gt;George Luger&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://images-na.ssl-images-amazon.com/images/I/51nHNl+xfjL._SX258_BO1,204,203,200_.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This is a massive book from 2005 in its 6th edition. I don’t think it has been updated since that.
And the author starts a writing of AI history.&lt;/p&gt;
&lt;p&gt;I have been intrigued by the use of the opening sentence from Aristotle in the Metaphysics: “All men by nature desire to know…”. I remembered that sentence (without knowing it was from Aristotle), and I jumped to The Metaphysics &lt;em&gt; Aristotle’s [wikipedia page](&lt;a href=&quot;https://fr.wikipedia.org/wiki/M%C3%A9taphysique&quot;&gt;https://fr.wikipedia.org/wiki/M%C3%A9taphysique&lt;/a&gt;&lt;/em&gt;%28Aristote%29) (the French one). There is a nice presentation of The MetaPhysics and some extracts that I have found quite interesting. One of them following “All men by nature desire to know” is detailing what is art and science; and for art: one need to be able to recognize similar cases and be able to generalize to an (more) universal rule.&lt;/p&gt;
&lt;p&gt;I cannot not see a link with what is happening in what we do on a daily basis in AI and deep learning. I had been surprised by Jeremy Howard’s curriculum (if I am not wrong he has a major) in Philosophy, and I better understand why he is so good in what he does.&lt;/p&gt;
&lt;p&gt;Should have studied Philosophy and ancient Greek!&lt;/p&gt;
&lt;p&gt;Would love to know your thoughts about that. (and if anyone can ask Jeremy’s without directly @ him)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here is a more detailed analysis of Aristotle thought: (again from wikipedia, not my own ;))&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;By nature, all animals are sentient; but sensation is not yet sufficient to produce knowledge: indeed, remarks Aristotle, sensation engenders memory or not. But animals endowed with memory are the most intelligent and the best able to learn. However, man “lives on art and reasoning.” To learn, you have to feel, remember, but man has the capacity to draw experience from these simple images and from a multitude of experimental notions emerges a single judgment that is universal in all similar cases: it is what constitutes art: “Science and art arise for men through experience” 10. Art therefore presupposes: the ability to recognize similar cases and the ability to apply a universal rule to these cases.

Of experience and art, which is more perfect? In practical life, experience seems superior to art, because it is knowledge of the particular, of the individual: sensations, the foundation of knowledge of the particular, are not science and do not teach us the why ( διότι). Art, for its part, knows the universal and goes beyond individual things, it is to art that knowledge and the faculty of understanding belong: men of art know the why and the cause. The wisest are wise not by practical skill, but by theory (λόγος) and knowledge of the causes. This explains the superiority of the architect over the maneuver.

The sign of this knowledge is that it can be taught; now men of art can teach. However, among the arts some relate to the necessities of life and others come from “leisure” which is knowledge sought for itself, as in mathematics. And through these appears the highest knowledge, wisdom, which has for its object the first causes and the first principles of what-is; therefore the theoretical sciences are superior to the practical sciences.

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From observations (rows of data) we can recognize similar cases (patterns or embeddings) and identify universal rules (models?)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>